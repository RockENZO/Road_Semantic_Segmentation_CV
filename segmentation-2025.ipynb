{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVgOlzfMdiHe"
   },
   "source": [
    "# **Semantic Segmentation Competition (30%)**\n",
    "\n",
    "For this competition, we will use a small autonomous driving dataset. The dataset contains 150 training images and 50 testing images.\n",
    "\n",
    "We provide baseline code that includes the following features:\n",
    "\n",
    "*    Loading the dataset using PyTorch.\n",
    "*    Defining a simple convolutional neural network for semantic segmentation.\n",
    "*    How to use existing loss function for the model learning.\n",
    "*    Train the network on the training data.\n",
    "*    Test the trained network on the testing data.\n",
    "\n",
    "The following changes could be considered:\n",
    "-------\n",
    "1. Data augmentation\n",
    "2. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, and Drop-out.\n",
    "3. Architectural changes: Batch Normalization, Residual layers, etc.\n",
    "4. Use of a new loss function.\n",
    "\n",
    "Your code should be modified from the provided baseline. A pdf report of a maximum of two pages is required to explain the changes you made from the baseline, why you chose those changes, and the improvements they achieved.\n",
    "\n",
    "Marking Rules:\n",
    "-------\n",
    "We will mark the competition based on the final test accuracy on testing images and your report.\n",
    "\n",
    "Final mark (out of 50) = acc_mark + efficiency mark + report mark\n",
    "###Acc_mark 10:\n",
    "\n",
    "We will rank all the submission results based on their test accuracy. Zero improvement over the baseline yields 0 marks. Maximum improvement over the baseline will yield 10 marks. There will be a sliding scale applied in between.\n",
    "\n",
    "###Efficiency mark 10:\n",
    "\n",
    "Efficiency considers not only the accuracy, but the computational cost of running the model (flops: https://en.wikipedia.org/wiki/FLOPS). Efficiency for our purposes is defined to be the ratio of accuracy (in %) to Gflops. Please report the computational cost for your final model and include the efficiency calculation in your report. Maximum improvement over the baseline will yield 10 marks. Zero improvement over the baseline yields zero marks, with a sliding scale in between.\n",
    "\n",
    "###Report mark 30:\n",
    "Your report should comprise:\n",
    "1. An introduction showing your understanding of the task and of the baseline model: [10 marks]\n",
    "\n",
    "2. A description of how you have modified aspects of the system to improve performance. [10 marks]\n",
    "\n",
    "A recommended way to present a summary of this is via an \"ablation study\" table, eg:\n",
    "\n",
    "|Method1|Method2|Method3|Accuracy|\n",
    "|---|---|---|---|\n",
    "|N|N|N|60%|\n",
    "|Y|N|N|65%|\n",
    "|Y|Y|N|77%|\n",
    "|Y|Y|Y|82%|\n",
    "\n",
    "3. Explanation of the methods for reducing the computational cost and/or improve the trade-off between accuracy and cost: [5 marks]\n",
    "\n",
    "4. Limitations/Conclusions: [5 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0dtUDczT_fB"
   },
   "source": [
    "# 1. Download data and set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:17.172498Z",
     "iopub.status.busy": "2025-06-08T06:08:17.172265Z",
     "iopub.status.idle": "2025-06-08T06:08:17.176428Z",
     "shell.execute_reply": "2025-06-08T06:08:17.175805Z",
     "shell.execute_reply.started": "2025-06-08T06:08:17.172466Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719681,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "XCPZsWI_9s-Y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "### Subject: Computer VisionSemantic Segmentation\n",
    "### Year: 2025\n",
    "### Student Name: Ge Wang, XYZ\n",
    "### Student ID: a1880714, a654321\n",
    "### Comptetion Name:  Competition\n",
    "### Final Results:\n",
    "### ACC:         GFLOPs:\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:17.178190Z",
     "iopub.status.busy": "2025-06-08T06:08:17.177856Z",
     "iopub.status.idle": "2025-06-08T06:08:17.190702Z",
     "shell.execute_reply": "2025-06-08T06:08:17.190152Z",
     "shell.execute_reply.started": "2025-06-08T06:08:17.178173Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719692,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "xHPzdgeP67Xu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#1.1 Download the dataset.\n",
    "# dowanload and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:17.191600Z",
     "iopub.status.busy": "2025-06-08T06:08:17.191348Z",
     "iopub.status.idle": "2025-06-08T06:08:26.338767Z",
     "shell.execute_reply": "2025-06-08T06:08:26.337985Z",
     "shell.execute_reply.started": "2025-06-08T06:08:17.191556Z"
    },
    "executionInfo": {
     "elapsed": 8909,
     "status": "ok",
     "timestamp": 1747833728595,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "106OzI7yUPON",
    "outputId": "84e98f0a-f948-4a7b-cd0d-aaa89e5d5570",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "#1.2 Set configs\n",
    "#Use Colab or install PyTorch 1.9 on your local machine to run the code.\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tf\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#--------Data path----------\n",
    "# Use your data path to replace the following path if you use Google drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Dataset path. Ensure that the file path correspond to the path you have here. It is expected that you unzip the data folders before running the notebook.\n",
    "# dataFolder = '/content/drive/MyDrive/Datasets/seg_data'\n",
    "# dataFolder = './seg_data' # local path if you run the code on your local machine.\n",
    "dataFolder = '/kaggle/input/seg-data/seg_data'  #kaggle input path\n",
    "\n",
    "# To access Google Colab GPU; Go To: Edit >>> Notebook Settings >>> Hardware Accelarator: Select GPU.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device: {}'.format(device))\n",
    "\n",
    "#---------Config----------\n",
    "learning_rate = 3e-4 #Tips: design a strategy to adjust the learning rate\n",
    "# learning_rate = 1e-3 # can be adjusted\n",
    "width = 864 # image width and height\n",
    "height = 256 #\n",
    "# batchSize = 4 #can be adjusted\n",
    "batchSize = 2 # can be adjusted\n",
    "# batchSize = 8 \n",
    "epochs = 180 #can be adjusted\n",
    "\n",
    "# if not os.path.exists(dataFolder):\n",
    "#    print('Data Path Error! Pls check your data path')\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING! The device is CPU NOT GPU! Pls avoid using CPU for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM0C-LdtUThm"
   },
   "source": [
    "# 2. Define a dataloader to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.339892Z",
     "iopub.status.busy": "2025-06-08T06:08:26.339575Z",
     "iopub.status.idle": "2025-06-08T06:08:26.360144Z",
     "shell.execute_reply": "2025-06-08T06:08:26.359628Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.339874Z"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1747833728629,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "_5dna2XkVHIq",
    "outputId": "ef7355a5-5c76-4bd4-8188-eb783007b805",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load info for 150 images\n"
     ]
    }
   ],
   "source": [
    "#The class to load images and labels\n",
    "class ExpDataSet(Dataset):\n",
    "    def __init__(self, dataFolder):\n",
    "        self.image_path = os.listdir(os.path.join(dataFolder, \"training/image\"))\n",
    "        self.label_path = os.listdir(os.path.join(dataFolder, \"training/image\"))#Image name only\n",
    "        print('load info for {} images'.format(len(self.image_path)))\n",
    "        assert len(self.image_path) == 150\n",
    "        for idx in range(0, len(self.image_path)):\n",
    "            assert self.image_path[idx] == self.label_path[idx] #same\n",
    "            self.image_path[idx] = os.path.join(dataFolder, \"training/image\", self.image_path[idx])\n",
    "            self.label_path[idx] = os.path.join(dataFolder, \"training/label\", self.label_path[idx])\n",
    "        # --------------------Transformation functions----------------\n",
    "        #-------------Tips: data augmentation can be used (for example flip, resize)-------------\n",
    "        self.transformImg = tf.Compose([tf.ToPILImage(), \n",
    "                                        # tf.RandomRotation(5), # random rotation data augmentation\n",
    "                                        # tf.GaussianBlur(kernel_size=3), # Gaussian blur data augmentation\n",
    "                                        # tf.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),#color jitter data augmentation\n",
    "                                        tf.Resize((height, width)), \n",
    "                                        tf.ToTensor(),\n",
    "                                        tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                        ])\n",
    "        self.transformLabel = tf.Compose([tf.ToPILImage(), tf.Resize((height, width), tf.InterpolationMode.NEAREST)])\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_path[idx])[:, :, 0:3]\n",
    "        label = cv2.imread(self.label_path[idx], 0)\n",
    "        img = self.transformImg(img)  #3*H*W\n",
    "        label = self.transformLabel(label)\n",
    "        label = torch.tensor(np.array(label))  #H*W\n",
    "        return img, label\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "#Get the predefined dataloader\n",
    "exp_data = ExpDataSet(dataFolder)\n",
    "train_loader = DataLoader(exp_data, batch_size=batchSize, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwfMTxpbVVSt"
   },
   "source": [
    "# 3. Define a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.362313Z",
     "iopub.status.busy": "2025-06-08T06:08:26.362097Z",
     "iopub.status.idle": "2025-06-08T06:08:26.782158Z",
     "shell.execute_reply": "2025-06-08T06:08:26.781621Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.362297Z"
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1747833729539,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "aX40GeSbVfOf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Define the semantic segmentation network. Tips: a new network can be used\n",
    "# class SegNetwork(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                  n_class=19,\n",
    "#                  use_residual=False,  #optional residual connection\n",
    "#                  use_batch_norm=False,  #optional batch normalization\n",
    "#                  ):\n",
    "#         super(SegNetwork, self).__init__()\n",
    "#         self.use_residual = use_residual  #optional residual connection\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         #stage 1\n",
    "#         self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn1_1 = nn.BatchNorm2d(64, momentum=0.1)\n",
    "#         self.relu1_1 = nn.ReLU(inplace=True)\n",
    "#         self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn1_2 = nn.BatchNorm2d(64)\n",
    "#         self.relu1_2 = nn.ReLU(inplace=True)\n",
    "#         self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  #1/2\n",
    "\n",
    "#         # Residual block for stage 1\n",
    "#         self.res_block1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, 3, padding=1),\n",
    "#             nn.BatchNorm2d(64, momentum=0.1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         #stage 2\n",
    "#         self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn2_1 = nn.BatchNorm2d(128)\n",
    "#         self.relu2_1 = nn.ReLU(inplace=True)\n",
    "#         self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn2_2 = nn.BatchNorm2d(128)\n",
    "#         self.relu2_2 = nn.ReLU(inplace=True)\n",
    "#         self.drop2_1 = nn.Dropout2d(p=0.1) #optional dropout\n",
    "#         self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  #1/4\n",
    "#         # Residual block for stage 2\n",
    "#         self.res_block2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, 1),  # 1x1 conv to match channels\n",
    "#             nn.BatchNorm2d(128, momentum=0.1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         #stage 3\n",
    "#         self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "#         if use_batch_norm:    \n",
    "#             self.bn3_1 = nn.BatchNorm2d(256)\n",
    "#         self.relu3_1 = nn.ReLU(inplace=True)\n",
    "#         self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn3_2 = nn.BatchNorm2d(256)\n",
    "#         self.relu3_2 = nn.ReLU(inplace=True)\n",
    "#         self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  #1/8\n",
    "#         # Residual block for stage 3\n",
    "#         self.res_block3 = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 1),  # 1x1 conv to match channels\n",
    "#             nn.BatchNorm2d(256, momentum=0.1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         #stage 4\n",
    "#         self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn4_1 = nn.BatchNorm2d(512)\n",
    "#         self.relu4_1 = nn.ReLU(inplace=True)\n",
    "#         self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "#         if use_batch_norm:\n",
    "#             self.bn4_2 = nn.BatchNorm2d(512)\n",
    "#         self.relu4_2 = nn.ReLU(inplace=True)\n",
    "#         self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  #1/16\n",
    "#         # Residual block for stage 4\n",
    "#         self.res_block4 = nn.Sequential(\n",
    "#             nn.Conv2d(256, 512, 1),  # 1x1 conv to match channels\n",
    "#             nn.BatchNorm2d(512, momentum=0.1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         #stage 5\n",
    "#         # Bottleneck: Input 512 channels, Output 512 channels\n",
    "#         self.conv5_1 = nn.Conv2d(512, 512, 3, padding=2, dilation=2)  # Dilated conv\n",
    "#         self.bn5_1 = nn.BatchNorm2d(512, momentum=0.1) if use_batch_norm else None\n",
    "#         self.relu5_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # Upsampling path with skip connections\n",
    "#         self.upsample_conv1 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "#         self.bn_up1 = nn.BatchNorm2d(256, momentum=0.1) if use_batch_norm else None\n",
    "#         self.relu_up1 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         self.upsample_conv2 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "#         self.bn_up2 = nn.BatchNorm2d(128, momentum=0.1) if use_batch_norm else None\n",
    "#         self.relu_up2 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         self.upsample_conv3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "#         self.bn_up3 = nn.BatchNorm2d(64, momentum=0.1) if use_batch_norm else None\n",
    "#         self.relu_up3 = nn.ReLU(inplace=True)\n",
    "\n",
    "#         # Final convolution to produce segmentation map\n",
    "#         self.final_conv = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         inp_shape = x.shape[2:]  # (256, 864)\n",
    "#         # Stage 1\n",
    "#         shortcut1 = x\n",
    "#         x = self.relu(self.conv1_1(x))\n",
    "#         x = self.relu(self.conv1_2(x))\n",
    "#         if self.use_residual and self.res_block1 is not None:\n",
    "#             x = x + self.res_block1(shortcut1)\n",
    "#         skip1 = x\n",
    "#         x = self.pool1(x)\n",
    "\n",
    "#         # Stage 2\n",
    "#         shortcut2 = x\n",
    "#         x = self.relu(self.bn2_1(self.conv2_1(x)))\n",
    "#         x = self.relu(self.bn2_2(self.conv2_2(x)))\n",
    "#         if self.use_residual and self.res_block2 is not None:\n",
    "#             x = x + self.res_block2(shortcut2)\n",
    "#         skip2 = x\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         # Stage 3\n",
    "#         shortcut3 = x\n",
    "#         x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
    "#         x = self.relu(self.bn3_2(self.conv3_2(x)))\n",
    "#         if self.use_residual and self.res_block3 is not None:\n",
    "#             x = x + self.res_block3(shortcut3)\n",
    "#         skip3 = x\n",
    "#         x = self.pool3(x)\n",
    "\n",
    "#         # Stage 4\n",
    "#         shortcut4 = x\n",
    "#         x = self.relu(self.bn4_1(self.conv4_1(x)))\n",
    "#         x = self.relu(self.bn4_2(self.conv4_2(x)))\n",
    "#         if self.use_residual and self.res_block4 is not None:\n",
    "#             x = x + self.res_block4(shortcut4)\n",
    "#         x = self.pool4(x)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x = self.conv5_1(x)\n",
    "#         if self.bn5_1 is not None:\n",
    "#             x = self.bn5_1(x)\n",
    "#         x = self.relu5_1(x)\n",
    "\n",
    "#         # Upsampling path\n",
    "#         x = F.interpolate(x, size=skip3.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "#         x = self.upsample_conv1(x)\n",
    "#         if self.bn_up1 is not None:\n",
    "#             x = self.bn_up1(x)\n",
    "#         x = self.relu_up1(x)\n",
    "#         x = x + skip3\n",
    "\n",
    "#         x = F.interpolate(x, size=skip2.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "#         x = self.upsample_conv2(x)\n",
    "#         if self.bn_up2 is not None:\n",
    "#             x = self.bn_up2(x)\n",
    "#         x = self.relu_up2(x)\n",
    "#         x = x + skip2\n",
    "\n",
    "#         x = F.interpolate(x, size=skip1.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "#         x = self.upsample_conv3(x)\n",
    "#         if self.bn_up3 is not None:\n",
    "#             x = self.bn_up3(x)\n",
    "#         x = self.relu_up3(x)\n",
    "#         x = x + skip1\n",
    "\n",
    "#         # Final convolution\n",
    "#         x = self.final_conv(x)\n",
    "#         x = F.interpolate(x, size=inp_shape, mode=\"bilinear\", align_corners=True)\n",
    "#         return x  # [batch_size, 19, 256, 864]\n",
    "\n",
    "# #Get the predefined network\n",
    "# segNet = SegNetwork(n_class=19,\n",
    "#                     use_residual=True,  #optional residual connection\n",
    "#                     use_batch_norm=True  #optional batch normalization\n",
    "#                     ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If using MobileNetV3 backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# Load DeepLabV3 with MobileNetV3 backbone\n",
    "segNet = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(\n",
    "    weights=None,  # or 'DEFAULT' for pretrained on COCO\n",
    "    num_classes=19  \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsuuIN00Wb-W"
   },
   "source": [
    "# 4. Define a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.783119Z",
     "iopub.status.busy": "2025-06-08T06:08:26.782900Z",
     "iopub.status.idle": "2025-06-08T06:08:26.792406Z",
     "shell.execute_reply": "2025-06-08T06:08:26.791880Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.783102Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "ubLuJmG7XXcV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.softmax(inputs, dim=1)\n",
    "        inputs = inputs.argmax(dim=1)\n",
    "        inputs = inputs.contiguous().view(-1)\n",
    "        targets = targets.contiguous().view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, ignore_index=255):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logpt = -self.ce(inputs, targets)\n",
    "        pt = torch.exp(logpt)\n",
    "        loss = ((1 - pt) ** self.gamma) * -logpt\n",
    "        return loss.mean()\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, smooth=1.0):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=255, weight=torch.tensor([1.0]*19).to(device))  # Adjust weights\n",
    "        self.dice = DiceLoss(smooth=smooth)\n",
    "        self.alpha = alpha\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        dice_loss = self.dice(inputs, targets)\n",
    "        return self.alpha * ce_loss + (1 - self.alpha * dice_loss)\n",
    "\n",
    "\n",
    "# Foe Cross Entropy Loss:\n",
    "# criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "# For Dice Loss:\n",
    "# criterion = DiceLoss()\n",
    "# For Focal Loss:\n",
    "# criterion = FocalLoss(gamma=1.5, weight=torch.tensor([1.0, 2.0, ...]).to(device))\n",
    "# for Hybrid Loss:\n",
    "criterion = HybridLoss(alpha=0.7)\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "# optimizer = torch.optim.Adam(params=segNet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.AdamW(params=segNet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "# for SGD optimizer:\n",
    "# optimizer = torch.optim.SGD(params=segNet.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) #Adding a learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzUEUtY6aXMG"
   },
   "source": [
    "# 5. The function used to compare the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.793480Z",
     "iopub.status.busy": "2025-06-08T06:08:26.793243Z",
     "iopub.status.idle": "2025-06-08T06:08:26.810455Z",
     "shell.execute_reply": "2025-06-08T06:08:26.809930Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.793464Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "2rFg1PNNa3Ch",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#------------------Modification of this function is ***NOT*** allowed---------------\n",
    "def cal_acc(pred_folder, gt_folder, classes=19):\n",
    "    class AverageMeter(object):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "        def reset(self):\n",
    "            self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "        def update(self, val, n=1):\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count\n",
    "    def intersectionAndUnion(output, target, K, ignore_index=255):\n",
    "        assert (output.ndim in [1, 2, 3])\n",
    "        assert output.shape == target.shape\n",
    "        output = output.reshape(output.size).copy()\n",
    "        target = target.reshape(target.size)\n",
    "        output[np.where(target == ignore_index)[0]] = ignore_index\n",
    "        intersection = output[np.where(output == target)[0]]\n",
    "        area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))\n",
    "        area_output, _ = np.histogram(output, bins=np.arange(K + 1))\n",
    "        area_target, _ = np.histogram(target, bins=np.arange(K + 1))\n",
    "        area_union = area_output + area_target - area_intersection\n",
    "        return area_intersection, area_union, area_target\n",
    "    data_list = os.listdir(gt_folder)\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    target_meter = AverageMeter()\n",
    "    for i, image_name in enumerate(data_list):\n",
    "        pred = cv2.imread(os.path.join(pred_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        target = cv2.imread(os.path.join(gt_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        intersection, union, target = intersectionAndUnion(pred, target, classes)\n",
    "        intersection_meter.update(intersection)\n",
    "        union_meter.update(union)\n",
    "        target_meter.update(target)\n",
    "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    mIoU = np.mean(iou_class)\n",
    "    print('Eval result: mIoU {:.4f}.'.format(mIoU))\n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2dRJ8TgboP9"
   },
   "source": [
    "# 6. Define functions to get and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.811637Z",
     "iopub.status.busy": "2025-06-08T06:08:26.811142Z",
     "iopub.status.idle": "2025-06-08T06:08:26.829058Z",
     "shell.execute_reply": "2025-06-08T06:08:26.828387Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.811618Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747833729541,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "dBcRAC2AhLzS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def make_folder(dir_name):#make a folder\n",
    "#     if not os.path.exists(dir_name):\n",
    "#         os.makedirs(dir_name)\n",
    "\n",
    "# def move_folders(grey_temp, color_temp, grey_rs, color_rs):#move folders\n",
    "#     if os.path.exists(grey_temp):\n",
    "#         make_folder(grey_rs)\n",
    "#         for file in os.listdir(grey_temp):\n",
    "#             shutil.move(os.path.join(grey_temp, file), os.path.join(grey_rs, file))\n",
    "#         if os.path.exists(grey_temp):\n",
    "#             shutil.rmtree(grey_temp)\n",
    "#     if os.path.exists(color_temp):\n",
    "#         make_folder(color_rs)\n",
    "#         for file in os.listdir(color_temp):\n",
    "#             shutil.move(os.path.join(color_temp, file), os.path.join(color_rs, file))\n",
    "#         if os.path.exists(color_temp):\n",
    "#             shutil.rmtree(color_temp)\n",
    "\n",
    "# def colorize(gray, palette):#visualize predictions results\n",
    "#     color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n",
    "#     color.putpalette(palette)\n",
    "#     return color\n",
    "\n",
    "# #-------Perform evaluation for a network and save prediction results--------\n",
    "# def get_predictions(segNet, dataFolder, device):#params: a network, data path, device\n",
    "#     gray_folder, color_folder = './temp_grey', './temp_color'\n",
    "#     listImages, gt_folder = os.listdir(os.path.join(dataFolder, \"testing/image\")), os.path.join(dataFolder, \"testing/label\")\n",
    "#     colors_path  = os.path.join(dataFolder, \"colors.txt\") #colors for visualizing greyscale images\n",
    "#     print('Begin testing')\n",
    "#     make_folder(gray_folder)\n",
    "#     make_folder(color_folder)\n",
    "#     colors = np.loadtxt(colors_path).astype('uint8')\n",
    "#     #Tips: muti-scale testing can be used\n",
    "#     transformTest = tf.Compose([tf.ToPILImage(), tf.ToTensor(),\n",
    "#                                 tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "#     for idx in range(0, len(listImages)):\n",
    "#         img = cv2.imread(os.path.join(dataFolder, \"testing/image\", listImages[idx]))[:, :, 0:3]\n",
    "#         img = transformTest(img).unsqueeze(0)  #1*3*H*W\n",
    "#         prediction = segNet(img.to(device))\n",
    "#         prediction = prediction[0].cpu().detach().numpy()\n",
    "#         prediction = np.argmax(prediction, axis=0)\n",
    "#         gray = np.uint8(prediction)\n",
    "#         color = colorize(gray, colors)\n",
    "#         gray_path = os.path.join(gray_folder, listImages[idx])\n",
    "#         color_path = os.path.join(color_folder, listImages[idx])\n",
    "#         cv2.imwrite(gray_path, gray)\n",
    "#         color.save(color_path)\n",
    "#     return gray_folder, color_folder #return folders (paths) which contain grey and color predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions when using MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(dir_name):#make a folder\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def move_folders(grey_temp, color_temp, grey_rs, color_rs):#move folders\n",
    "    if os.path.exists(grey_temp):\n",
    "        make_folder(grey_rs)\n",
    "        for file in os.listdir(grey_temp):\n",
    "            shutil.move(os.path.join(grey_temp, file), os.path.join(grey_rs, file))\n",
    "        if os.path.exists(grey_temp):\n",
    "            shutil.rmtree(grey_temp)\n",
    "    if os.path.exists(color_temp):\n",
    "        make_folder(color_rs)\n",
    "        for file in os.listdir(color_temp):\n",
    "            shutil.move(os.path.join(color_temp, file), os.path.join(color_rs, file))\n",
    "        if os.path.exists(color_temp):\n",
    "            shutil.rmtree(color_temp)\n",
    "\n",
    "def colorize(gray, palette):#visualize predictions results\n",
    "    color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n",
    "    color.putpalette(palette)\n",
    "    return color\n",
    "\n",
    "#-------Perform evaluation for a network and save prediction results--------\n",
    "def get_predictions(segNet, dataFolder, device):#params: a network, data path, device\n",
    "    gray_folder, color_folder = './temp_grey', './temp_color'\n",
    "    listImages, gt_folder = os.listdir(os.path.join(dataFolder, \"testing/image\")), os.path.join(dataFolder, \"testing/label\")\n",
    "    colors_path  = os.path.join(dataFolder, \"colors.txt\") #colors for visualizing greyscale images\n",
    "    print('Begin testing')\n",
    "    make_folder(gray_folder)\n",
    "    make_folder(color_folder)\n",
    "    colors = np.loadtxt(colors_path).astype('uint8')\n",
    "    #Tips: muti-scale testing can be used\n",
    "    transformTest = tf.Compose([tf.ToPILImage(), tf.ToTensor(),\n",
    "                                tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    for idx in range(0, len(listImages)):\n",
    "        img = cv2.imread(os.path.join(dataFolder, \"testing/image\", listImages[idx]))[:, :, 0:3]\n",
    "        img = transformTest(img).unsqueeze(0)  #1*3*H*W\n",
    "        output = segNet(img.to(device))\n",
    "        prediction = output['out'][0].cpu().detach().numpy()\n",
    "        prediction = np.argmax(prediction, axis=0)\n",
    "        gray = np.uint8(prediction)\n",
    "        color = colorize(gray, colors)\n",
    "        gray_path = os.path.join(gray_folder, listImages[idx])\n",
    "        color_path = os.path.join(color_folder, listImages[idx])\n",
    "        cv2.imwrite(gray_path, gray)\n",
    "        color.save(color_path)\n",
    "    return gray_folder, color_folder #return folders (paths) which contain grey and color predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqnC6C_YkqRO"
   },
   "source": [
    "# 7. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T06:08:26.829904Z",
     "iopub.status.busy": "2025-06-08T06:08:26.829731Z",
     "iopub.status.idle": "2025-06-08T07:41:00.682126Z",
     "shell.execute_reply": "2025-06-08T07:41:00.681320Z",
     "shell.execute_reply.started": "2025-06-08T06:08:26.829890Z"
    },
    "executionInfo": {
     "elapsed": 2117560,
     "status": "ok",
     "timestamp": 1747835847097,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "JvzZ3HxFkt_c",
    "outputId": "b893b385-5257-4cd4-83e1-612bf3f3acff",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss=3.114506721496582\n",
      "epoch 0 iter 1 loss=2.5222742557525635\n",
      "epoch 0 iter 2 loss=2.324031352996826\n",
      "epoch 0 iter 3 loss=2.265740394592285\n",
      "epoch 0 iter 4 loss=2.9498250484466553\n",
      "epoch 0 iter 5 loss=2.2172892093658447\n",
      "epoch 0 iter 6 loss=2.3464770317077637\n",
      "epoch 0 iter 7 loss=2.0761032104492188\n",
      "epoch 0 iter 8 loss=2.300729513168335\n",
      "epoch 0 iter 9 loss=2.2782065868377686\n",
      "epoch 0 iter 10 loss=2.088123083114624\n",
      "epoch 0 iter 11 loss=2.0340964794158936\n",
      "epoch 0 iter 12 loss=1.5776695013046265\n",
      "epoch 0 iter 13 loss=1.758310079574585\n",
      "epoch 0 iter 14 loss=2.2154977321624756\n",
      "epoch 0 iter 15 loss=1.60309636592865\n",
      "epoch 0 iter 16 loss=1.628210186958313\n",
      "epoch 0 iter 17 loss=2.5002598762512207\n",
      "epoch 0 iter 18 loss=1.4892181158065796\n",
      "epoch 0 iter 19 loss=1.6233763694763184\n",
      "epoch 0 iter 20 loss=1.5020848512649536\n",
      "epoch 0 iter 21 loss=1.0323553085327148\n",
      "epoch 0 iter 22 loss=1.8917276859283447\n",
      "epoch 0 iter 23 loss=1.4486116170883179\n",
      "epoch 0 iter 24 loss=1.9124151468276978\n",
      "epoch 0 iter 25 loss=1.6645772457122803\n",
      "epoch 0 iter 26 loss=1.539975881576538\n",
      "epoch 0 iter 27 loss=1.2743699550628662\n",
      "epoch 0 iter 28 loss=1.905250072479248\n",
      "epoch 0 iter 29 loss=1.6154677867889404\n",
      "epoch 0 iter 30 loss=1.3507148027420044\n",
      "epoch 0 iter 31 loss=1.1694709062576294\n",
      "epoch 0 iter 32 loss=1.9096229076385498\n",
      "epoch 0 iter 33 loss=0.9802787899971008\n",
      "epoch 0 iter 34 loss=1.4155254364013672\n",
      "epoch 0 iter 35 loss=0.9765634536743164\n",
      "epoch 0 iter 36 loss=1.1021393537521362\n",
      "epoch 0 iter 37 loss=0.9745271801948547\n",
      "epoch 0 iter 38 loss=1.681645154953003\n",
      "epoch 0 iter 39 loss=0.8869345188140869\n",
      "epoch 0 iter 40 loss=0.9750098586082458\n",
      "epoch 0 iter 41 loss=1.0314326286315918\n",
      "epoch 0 iter 42 loss=1.0182266235351562\n",
      "epoch 0 iter 43 loss=1.509225606918335\n",
      "epoch 0 iter 44 loss=1.0528855323791504\n",
      "epoch 0 iter 45 loss=1.0306414365768433\n",
      "epoch 0 iter 46 loss=1.2106128931045532\n",
      "epoch 0 iter 47 loss=2.3284895420074463\n",
      "epoch 0 iter 48 loss=0.935636043548584\n",
      "epoch 0 iter 49 loss=0.7849836349487305\n",
      "epoch 0 iter 50 loss=1.4099215269088745\n",
      "epoch 0 iter 51 loss=0.929936408996582\n",
      "epoch 0 iter 52 loss=0.8474807143211365\n",
      "epoch 0 iter 53 loss=1.3300148248672485\n",
      "epoch 0 iter 54 loss=0.7771445512771606\n",
      "epoch 0 iter 55 loss=1.0071178674697876\n",
      "epoch 0 iter 56 loss=1.5785256624221802\n",
      "epoch 0 iter 57 loss=0.80368971824646\n",
      "epoch 0 iter 58 loss=1.5595945119857788\n",
      "epoch 0 iter 59 loss=0.8417865037918091\n",
      "epoch 0 iter 60 loss=1.0436275005340576\n",
      "epoch 0 iter 61 loss=1.6719210147857666\n",
      "epoch 0 iter 62 loss=1.1305540800094604\n",
      "epoch 0 iter 63 loss=1.6662068367004395\n",
      "epoch 0 iter 64 loss=1.2231229543685913\n",
      "epoch 0 iter 65 loss=0.9527224898338318\n",
      "epoch 0 iter 66 loss=1.2275099754333496\n",
      "epoch 0 iter 67 loss=1.3648035526275635\n",
      "epoch 0 iter 68 loss=0.965300440788269\n",
      "epoch 0 iter 69 loss=1.0726368427276611\n",
      "epoch 0 iter 70 loss=0.8173048496246338\n",
      "epoch 0 iter 71 loss=0.9777334928512573\n",
      "epoch 0 iter 72 loss=1.1054773330688477\n",
      "epoch 0 iter 73 loss=1.1516170501708984\n",
      "epoch 0 iter 74 loss=1.2312803268432617\n",
      "epoch 1 iter 0 loss=1.1438242197036743\n",
      "epoch 1 iter 1 loss=0.7725525498390198\n",
      "epoch 1 iter 2 loss=0.972054123878479\n",
      "epoch 1 iter 3 loss=0.8742237687110901\n",
      "epoch 1 iter 4 loss=0.7710140943527222\n",
      "epoch 1 iter 5 loss=1.1856011152267456\n",
      "epoch 1 iter 6 loss=0.6930731534957886\n",
      "epoch 1 iter 7 loss=0.7624959945678711\n",
      "epoch 1 iter 8 loss=0.7726271152496338\n",
      "epoch 1 iter 9 loss=0.8115273118019104\n",
      "epoch 1 iter 10 loss=1.54486882686615\n",
      "epoch 1 iter 11 loss=0.7777482867240906\n",
      "epoch 1 iter 12 loss=0.7116822004318237\n",
      "epoch 1 iter 13 loss=1.4458202123641968\n",
      "epoch 1 iter 14 loss=1.4103598594665527\n",
      "epoch 1 iter 15 loss=0.6377235651016235\n",
      "epoch 1 iter 16 loss=1.0572000741958618\n",
      "epoch 1 iter 17 loss=0.9218816757202148\n",
      "epoch 1 iter 18 loss=1.0513375997543335\n",
      "epoch 1 iter 19 loss=1.226185917854309\n",
      "epoch 1 iter 20 loss=0.7177322506904602\n",
      "epoch 1 iter 21 loss=0.9513164758682251\n",
      "epoch 1 iter 22 loss=1.2275623083114624\n",
      "epoch 1 iter 23 loss=0.839535653591156\n",
      "epoch 1 iter 24 loss=1.0395722389221191\n",
      "epoch 1 iter 25 loss=0.6151476502418518\n",
      "epoch 1 iter 26 loss=1.038849949836731\n",
      "epoch 1 iter 27 loss=1.5063226222991943\n",
      "epoch 1 iter 28 loss=0.7286491394042969\n",
      "epoch 1 iter 29 loss=0.746494710445404\n",
      "epoch 1 iter 30 loss=1.2076681852340698\n",
      "epoch 1 iter 31 loss=0.917306125164032\n",
      "epoch 1 iter 32 loss=0.748605489730835\n",
      "epoch 1 iter 33 loss=0.967713475227356\n",
      "epoch 1 iter 34 loss=0.7106677889823914\n",
      "epoch 1 iter 35 loss=1.0050513744354248\n",
      "epoch 1 iter 36 loss=1.3787143230438232\n",
      "epoch 1 iter 37 loss=0.9818801283836365\n",
      "epoch 1 iter 38 loss=0.8858639001846313\n",
      "epoch 1 iter 39 loss=0.8575897812843323\n",
      "epoch 1 iter 40 loss=0.8656450510025024\n",
      "epoch 1 iter 41 loss=0.9683158993721008\n",
      "epoch 1 iter 42 loss=0.8586956858634949\n",
      "epoch 1 iter 43 loss=1.111758828163147\n",
      "epoch 1 iter 44 loss=0.6393294930458069\n",
      "epoch 1 iter 45 loss=0.6193910837173462\n",
      "epoch 1 iter 46 loss=0.8864871859550476\n",
      "epoch 1 iter 47 loss=1.0140045881271362\n",
      "epoch 1 iter 48 loss=0.9134538769721985\n",
      "epoch 1 iter 49 loss=1.109215259552002\n",
      "epoch 1 iter 50 loss=0.7346081733703613\n",
      "epoch 1 iter 51 loss=1.1189771890640259\n",
      "epoch 1 iter 52 loss=0.7535836100578308\n",
      "epoch 1 iter 53 loss=0.8542465567588806\n",
      "epoch 1 iter 54 loss=0.6604295969009399\n",
      "epoch 1 iter 55 loss=1.7790887355804443\n",
      "epoch 1 iter 56 loss=1.7896736860275269\n",
      "epoch 1 iter 57 loss=0.804015576839447\n",
      "epoch 1 iter 58 loss=1.4209678173065186\n",
      "epoch 1 iter 59 loss=0.7471557855606079\n",
      "epoch 1 iter 60 loss=0.7388117909431458\n",
      "epoch 1 iter 61 loss=0.6492125391960144\n",
      "epoch 1 iter 62 loss=1.0523388385772705\n",
      "epoch 1 iter 63 loss=1.1520668268203735\n",
      "epoch 1 iter 64 loss=0.7997286319732666\n",
      "epoch 1 iter 65 loss=0.9105785489082336\n",
      "epoch 1 iter 66 loss=0.7299462556838989\n",
      "epoch 1 iter 67 loss=1.3566908836364746\n",
      "epoch 1 iter 68 loss=0.7107400298118591\n",
      "epoch 1 iter 69 loss=0.9893794655799866\n",
      "epoch 1 iter 70 loss=0.9529144167900085\n",
      "epoch 1 iter 71 loss=0.7389684915542603\n",
      "epoch 1 iter 72 loss=0.7206461429595947\n",
      "epoch 1 iter 73 loss=0.9277198314666748\n",
      "epoch 1 iter 74 loss=0.9459390640258789\n",
      "epoch 2 iter 0 loss=0.7296925187110901\n",
      "epoch 2 iter 1 loss=0.7561395168304443\n",
      "epoch 2 iter 2 loss=0.7135847210884094\n",
      "epoch 2 iter 3 loss=0.7455582618713379\n",
      "epoch 2 iter 4 loss=0.7043138146400452\n",
      "epoch 2 iter 5 loss=0.6321436166763306\n",
      "epoch 2 iter 6 loss=1.3170794248580933\n",
      "epoch 2 iter 7 loss=0.9985234141349792\n",
      "epoch 2 iter 8 loss=1.7021598815917969\n",
      "epoch 2 iter 9 loss=1.3468672037124634\n",
      "epoch 2 iter 10 loss=0.8645238876342773\n",
      "epoch 2 iter 11 loss=0.8162915706634521\n",
      "epoch 2 iter 12 loss=0.7930906414985657\n",
      "epoch 2 iter 13 loss=1.6424185037612915\n",
      "epoch 2 iter 14 loss=0.798431396484375\n",
      "epoch 2 iter 15 loss=0.7669853568077087\n",
      "epoch 2 iter 16 loss=0.7537031769752502\n",
      "epoch 2 iter 17 loss=0.7995428442955017\n",
      "epoch 2 iter 18 loss=0.7216731905937195\n",
      "epoch 2 iter 19 loss=1.1039539575576782\n",
      "epoch 2 iter 20 loss=0.5630567073822021\n",
      "epoch 2 iter 21 loss=0.8816645741462708\n",
      "epoch 2 iter 22 loss=0.8585619330406189\n",
      "epoch 2 iter 23 loss=0.7063232064247131\n",
      "epoch 2 iter 24 loss=0.8056665062904358\n",
      "epoch 2 iter 25 loss=1.2239031791687012\n",
      "epoch 2 iter 26 loss=1.1599563360214233\n",
      "epoch 2 iter 27 loss=0.6593338847160339\n",
      "epoch 2 iter 28 loss=1.0495132207870483\n",
      "epoch 2 iter 29 loss=0.8041827082633972\n",
      "epoch 2 iter 30 loss=0.5628592371940613\n",
      "epoch 2 iter 31 loss=0.709385335445404\n",
      "epoch 2 iter 32 loss=0.9013883471488953\n",
      "epoch 2 iter 33 loss=1.05572509765625\n",
      "epoch 2 iter 34 loss=0.9131391644477844\n",
      "epoch 2 iter 35 loss=0.7826431393623352\n",
      "epoch 2 iter 36 loss=0.6861943006515503\n",
      "epoch 2 iter 37 loss=0.5651731491088867\n",
      "epoch 2 iter 38 loss=0.5739491581916809\n",
      "epoch 2 iter 39 loss=0.6445085406303406\n",
      "epoch 2 iter 40 loss=1.0232644081115723\n",
      "epoch 2 iter 41 loss=0.8191348314285278\n",
      "epoch 2 iter 42 loss=0.9228962659835815\n",
      "epoch 2 iter 43 loss=1.4444913864135742\n",
      "epoch 2 iter 44 loss=0.7192004919052124\n",
      "epoch 2 iter 45 loss=0.7682645320892334\n",
      "epoch 2 iter 46 loss=0.7504311203956604\n",
      "epoch 2 iter 47 loss=0.6592172384262085\n",
      "epoch 2 iter 48 loss=0.6283864378929138\n",
      "epoch 2 iter 49 loss=0.6346704363822937\n",
      "epoch 2 iter 50 loss=0.5839706063270569\n",
      "epoch 2 iter 51 loss=0.5913543105125427\n",
      "epoch 2 iter 52 loss=0.799689769744873\n",
      "epoch 2 iter 53 loss=0.7447409629821777\n",
      "epoch 2 iter 54 loss=0.9915557503700256\n",
      "epoch 2 iter 55 loss=0.6666086316108704\n",
      "epoch 2 iter 56 loss=1.6573642492294312\n",
      "epoch 2 iter 57 loss=1.067881464958191\n",
      "epoch 2 iter 58 loss=0.5535692572593689\n",
      "epoch 2 iter 59 loss=1.225687026977539\n",
      "epoch 2 iter 60 loss=1.1814663410186768\n",
      "epoch 2 iter 61 loss=0.7034663558006287\n",
      "epoch 2 iter 62 loss=0.7258853912353516\n",
      "epoch 2 iter 63 loss=0.7102247476577759\n",
      "epoch 2 iter 64 loss=0.6858109831809998\n",
      "epoch 2 iter 65 loss=0.8919073939323425\n",
      "epoch 2 iter 66 loss=1.0416332483291626\n",
      "epoch 2 iter 67 loss=0.8592534065246582\n",
      "epoch 2 iter 68 loss=0.6506384015083313\n",
      "epoch 2 iter 69 loss=0.7151284217834473\n",
      "epoch 2 iter 70 loss=0.9041001796722412\n",
      "epoch 2 iter 71 loss=0.8678117990493774\n",
      "epoch 2 iter 72 loss=0.5629689693450928\n",
      "epoch 2 iter 73 loss=0.8537463545799255\n",
      "epoch 2 iter 74 loss=0.928480327129364\n",
      "epoch 3 iter 0 loss=1.1368768215179443\n",
      "epoch 3 iter 1 loss=0.7805014252662659\n",
      "epoch 3 iter 2 loss=0.6230663061141968\n",
      "epoch 3 iter 3 loss=0.941050112247467\n",
      "epoch 3 iter 4 loss=0.7799469828605652\n",
      "epoch 3 iter 5 loss=0.774773120880127\n",
      "epoch 3 iter 6 loss=0.8265557289123535\n",
      "epoch 3 iter 7 loss=0.6498266458511353\n",
      "epoch 3 iter 8 loss=0.6695093512535095\n",
      "epoch 3 iter 9 loss=0.7716089487075806\n",
      "epoch 3 iter 10 loss=0.3938682973384857\n",
      "epoch 3 iter 11 loss=0.5701445937156677\n",
      "epoch 3 iter 12 loss=0.6019427180290222\n",
      "epoch 3 iter 13 loss=1.0585365295410156\n",
      "epoch 3 iter 14 loss=0.5155095458030701\n",
      "epoch 3 iter 15 loss=0.7134308815002441\n",
      "epoch 3 iter 16 loss=0.37271934747695923\n",
      "epoch 3 iter 17 loss=0.722176194190979\n",
      "epoch 3 iter 18 loss=0.6776477694511414\n",
      "epoch 3 iter 19 loss=0.9493452310562134\n",
      "epoch 3 iter 20 loss=0.7090837359428406\n",
      "epoch 3 iter 21 loss=1.3676958084106445\n",
      "epoch 3 iter 22 loss=1.1686497926712036\n",
      "epoch 3 iter 23 loss=0.8383938074111938\n",
      "epoch 3 iter 24 loss=0.9243647456169128\n",
      "epoch 3 iter 25 loss=0.6685917377471924\n",
      "epoch 3 iter 26 loss=0.6415722370147705\n",
      "epoch 3 iter 27 loss=0.8334213495254517\n",
      "epoch 3 iter 28 loss=0.7816511392593384\n",
      "epoch 3 iter 29 loss=0.7125424742698669\n",
      "epoch 3 iter 30 loss=0.5476430058479309\n",
      "epoch 3 iter 31 loss=0.9752452373504639\n",
      "epoch 3 iter 32 loss=0.8391620516777039\n",
      "epoch 3 iter 33 loss=1.4588983058929443\n",
      "epoch 3 iter 34 loss=0.7740498185157776\n",
      "epoch 3 iter 35 loss=0.9326073527336121\n",
      "epoch 3 iter 36 loss=0.5990551710128784\n",
      "epoch 3 iter 37 loss=0.6912668347358704\n",
      "epoch 3 iter 38 loss=0.6832848191261292\n",
      "epoch 3 iter 39 loss=0.9401471614837646\n",
      "epoch 3 iter 40 loss=0.8250548839569092\n",
      "epoch 3 iter 41 loss=0.7540618181228638\n",
      "epoch 3 iter 42 loss=0.8269029259681702\n",
      "epoch 3 iter 43 loss=0.48610955476760864\n",
      "epoch 3 iter 44 loss=0.5508522987365723\n",
      "epoch 3 iter 45 loss=0.9226833581924438\n",
      "epoch 3 iter 46 loss=0.5426624417304993\n",
      "epoch 3 iter 47 loss=0.6502538323402405\n",
      "epoch 3 iter 48 loss=0.55082106590271\n",
      "epoch 3 iter 49 loss=0.9309167265892029\n",
      "epoch 3 iter 50 loss=0.482756108045578\n",
      "epoch 3 iter 51 loss=0.4429304897785187\n",
      "epoch 3 iter 52 loss=0.8877670168876648\n",
      "epoch 3 iter 53 loss=0.8666588068008423\n",
      "epoch 3 iter 54 loss=0.9731552600860596\n",
      "epoch 3 iter 55 loss=0.4925118088722229\n",
      "epoch 3 iter 56 loss=1.00931715965271\n",
      "epoch 3 iter 57 loss=0.476849764585495\n",
      "epoch 3 iter 58 loss=0.7342245578765869\n",
      "epoch 3 iter 59 loss=0.8243855237960815\n",
      "epoch 3 iter 60 loss=0.48643943667411804\n",
      "epoch 3 iter 61 loss=1.0852807760238647\n",
      "epoch 3 iter 62 loss=0.5378140807151794\n",
      "epoch 3 iter 63 loss=0.9495683908462524\n",
      "epoch 3 iter 64 loss=0.7867997884750366\n",
      "epoch 3 iter 65 loss=0.5513650178909302\n",
      "epoch 3 iter 66 loss=0.958397388458252\n",
      "epoch 3 iter 67 loss=0.5899601578712463\n",
      "epoch 3 iter 68 loss=0.8552095293998718\n",
      "epoch 3 iter 69 loss=0.9235032200813293\n",
      "epoch 3 iter 70 loss=0.9157513380050659\n",
      "epoch 3 iter 71 loss=0.6490646004676819\n",
      "epoch 3 iter 72 loss=0.6584289073944092\n",
      "epoch 3 iter 73 loss=0.7664192318916321\n",
      "epoch 3 iter 74 loss=0.7300991415977478\n",
      "epoch 4 iter 0 loss=0.5746748447418213\n",
      "epoch 4 iter 1 loss=0.6218852996826172\n",
      "epoch 4 iter 2 loss=0.6172348856925964\n",
      "epoch 4 iter 3 loss=0.49671879410743713\n",
      "epoch 4 iter 4 loss=0.5053616166114807\n",
      "epoch 4 iter 5 loss=0.5683913826942444\n",
      "epoch 4 iter 6 loss=0.8120435476303101\n",
      "epoch 4 iter 7 loss=1.0091630220413208\n",
      "epoch 4 iter 8 loss=0.8034805655479431\n",
      "epoch 4 iter 9 loss=0.49119502305984497\n",
      "epoch 4 iter 10 loss=0.4515792429447174\n",
      "epoch 4 iter 11 loss=0.698482096195221\n",
      "epoch 4 iter 12 loss=0.6014863848686218\n",
      "epoch 4 iter 13 loss=0.7353255748748779\n",
      "epoch 4 iter 14 loss=0.4544592797756195\n",
      "epoch 4 iter 15 loss=0.7810962796211243\n",
      "epoch 4 iter 16 loss=0.6023724675178528\n",
      "epoch 4 iter 17 loss=0.7230696678161621\n",
      "epoch 4 iter 18 loss=0.714153528213501\n",
      "epoch 4 iter 19 loss=1.1846678256988525\n",
      "epoch 4 iter 20 loss=0.6642348170280457\n",
      "epoch 4 iter 21 loss=0.8750461935997009\n",
      "epoch 4 iter 22 loss=0.5128917694091797\n",
      "epoch 4 iter 23 loss=0.6434149146080017\n",
      "epoch 4 iter 24 loss=0.9400853514671326\n",
      "epoch 4 iter 25 loss=0.6169242858886719\n",
      "epoch 4 iter 26 loss=0.671002984046936\n",
      "epoch 4 iter 27 loss=0.4833526909351349\n",
      "epoch 4 iter 28 loss=0.7198224067687988\n",
      "epoch 4 iter 29 loss=1.0870108604431152\n",
      "epoch 4 iter 30 loss=0.5135403871536255\n",
      "epoch 4 iter 31 loss=0.595626711845398\n",
      "epoch 4 iter 32 loss=1.0852428674697876\n",
      "epoch 4 iter 33 loss=0.47722187638282776\n",
      "epoch 4 iter 34 loss=0.62297523021698\n",
      "epoch 4 iter 35 loss=1.1054474115371704\n",
      "epoch 4 iter 36 loss=0.6557776927947998\n",
      "epoch 4 iter 37 loss=0.4973331689834595\n",
      "epoch 4 iter 38 loss=0.8428608179092407\n",
      "epoch 4 iter 39 loss=1.0021133422851562\n",
      "epoch 4 iter 40 loss=0.7030200362205505\n",
      "epoch 4 iter 41 loss=0.5735207200050354\n",
      "epoch 4 iter 42 loss=0.4586013853549957\n",
      "epoch 4 iter 43 loss=0.6150739789009094\n",
      "epoch 4 iter 44 loss=0.8134763836860657\n",
      "epoch 4 iter 45 loss=0.7340354323387146\n",
      "epoch 4 iter 46 loss=0.4365302324295044\n",
      "epoch 4 iter 47 loss=0.5709290504455566\n",
      "epoch 4 iter 48 loss=0.5347018241882324\n",
      "epoch 4 iter 49 loss=0.9789749979972839\n",
      "epoch 4 iter 50 loss=0.7606526017189026\n",
      "epoch 4 iter 51 loss=0.6162322759628296\n",
      "epoch 4 iter 52 loss=0.8093069791793823\n",
      "epoch 4 iter 53 loss=0.6749760508537292\n",
      "epoch 4 iter 54 loss=0.5981706976890564\n",
      "epoch 4 iter 55 loss=0.498654842376709\n",
      "epoch 4 iter 56 loss=0.6014569401741028\n",
      "epoch 4 iter 57 loss=0.40592581033706665\n",
      "epoch 4 iter 58 loss=0.6856666803359985\n",
      "epoch 4 iter 59 loss=1.2632938623428345\n",
      "epoch 4 iter 60 loss=1.2119195461273193\n",
      "epoch 4 iter 61 loss=0.4208358824253082\n",
      "epoch 4 iter 62 loss=0.8886738419532776\n",
      "epoch 4 iter 63 loss=0.9471047520637512\n",
      "epoch 4 iter 64 loss=1.3293366432189941\n",
      "epoch 4 iter 65 loss=0.6212578415870667\n",
      "epoch 4 iter 66 loss=0.5900982618331909\n",
      "epoch 4 iter 67 loss=0.698651909828186\n",
      "epoch 4 iter 68 loss=0.7486140131950378\n",
      "epoch 4 iter 69 loss=0.709208607673645\n",
      "epoch 4 iter 70 loss=0.5777105093002319\n",
      "epoch 4 iter 71 loss=1.4398069381713867\n",
      "epoch 4 iter 72 loss=0.7750163078308105\n",
      "epoch 4 iter 73 loss=0.650637686252594\n",
      "epoch 4 iter 74 loss=1.0808191299438477\n",
      "epoch 5 iter 0 loss=0.5041282176971436\n",
      "epoch 5 iter 1 loss=1.2778407335281372\n",
      "epoch 5 iter 2 loss=0.5773988962173462\n",
      "epoch 5 iter 3 loss=0.9190873503684998\n",
      "epoch 5 iter 4 loss=0.6963255405426025\n",
      "epoch 5 iter 5 loss=0.880653977394104\n",
      "epoch 5 iter 6 loss=0.7356314063072205\n",
      "epoch 5 iter 7 loss=0.8701930046081543\n",
      "epoch 5 iter 8 loss=0.9554018378257751\n",
      "epoch 5 iter 9 loss=0.7401697635650635\n",
      "epoch 5 iter 10 loss=0.7590482234954834\n",
      "epoch 5 iter 11 loss=0.5082173943519592\n",
      "epoch 5 iter 12 loss=0.7768608927726746\n",
      "epoch 5 iter 13 loss=0.5532872676849365\n",
      "epoch 5 iter 14 loss=0.8864783644676208\n",
      "epoch 5 iter 15 loss=0.5163107514381409\n",
      "epoch 5 iter 16 loss=0.5025627613067627\n",
      "epoch 5 iter 17 loss=0.5753895044326782\n",
      "epoch 5 iter 18 loss=0.7362737655639648\n",
      "epoch 5 iter 19 loss=0.5159804224967957\n",
      "epoch 5 iter 20 loss=0.5012756586074829\n",
      "epoch 5 iter 21 loss=0.7899202704429626\n",
      "epoch 5 iter 22 loss=0.6522702574729919\n",
      "epoch 5 iter 23 loss=0.7796397805213928\n",
      "epoch 5 iter 24 loss=0.8030096292495728\n",
      "epoch 5 iter 25 loss=0.953180730342865\n",
      "epoch 5 iter 26 loss=0.5693231225013733\n",
      "epoch 5 iter 27 loss=0.9482722282409668\n",
      "epoch 5 iter 28 loss=0.4648332893848419\n",
      "epoch 5 iter 29 loss=0.6739251017570496\n",
      "epoch 5 iter 30 loss=0.7181193232536316\n",
      "epoch 5 iter 31 loss=0.5952938199043274\n",
      "epoch 5 iter 32 loss=0.8031625151634216\n",
      "epoch 5 iter 33 loss=0.48643845319747925\n",
      "epoch 5 iter 34 loss=0.5635030269622803\n",
      "epoch 5 iter 35 loss=0.5063130855560303\n",
      "epoch 5 iter 36 loss=0.7489204406738281\n",
      "epoch 5 iter 37 loss=0.5072444677352905\n",
      "epoch 5 iter 38 loss=1.1121174097061157\n",
      "epoch 5 iter 39 loss=0.49154549837112427\n",
      "epoch 5 iter 40 loss=1.358222484588623\n",
      "epoch 5 iter 41 loss=0.596540093421936\n",
      "epoch 5 iter 42 loss=0.6130996346473694\n",
      "epoch 5 iter 43 loss=0.5443304181098938\n",
      "epoch 5 iter 44 loss=0.6373607516288757\n",
      "epoch 5 iter 45 loss=0.6701698303222656\n",
      "epoch 5 iter 46 loss=0.45753148198127747\n",
      "epoch 5 iter 47 loss=0.4627631902694702\n",
      "epoch 5 iter 48 loss=0.5951931476593018\n",
      "epoch 5 iter 49 loss=0.7234412431716919\n",
      "epoch 5 iter 50 loss=0.6306062340736389\n",
      "epoch 5 iter 51 loss=0.9652500152587891\n",
      "epoch 5 iter 52 loss=0.7313544750213623\n",
      "epoch 5 iter 53 loss=1.0996265411376953\n",
      "epoch 5 iter 54 loss=0.6887365579605103\n",
      "epoch 5 iter 55 loss=0.3978869318962097\n",
      "epoch 5 iter 56 loss=0.6288275122642517\n",
      "epoch 5 iter 57 loss=0.7408773899078369\n",
      "epoch 5 iter 58 loss=0.5892541408538818\n",
      "epoch 5 iter 59 loss=0.5966607332229614\n",
      "epoch 5 iter 60 loss=0.6999152898788452\n",
      "epoch 5 iter 61 loss=0.5811503529548645\n",
      "epoch 5 iter 62 loss=0.4860231876373291\n",
      "epoch 5 iter 63 loss=0.5076756477355957\n",
      "epoch 5 iter 64 loss=1.1489272117614746\n",
      "epoch 5 iter 65 loss=0.4990272521972656\n",
      "epoch 5 iter 66 loss=0.9920719861984253\n",
      "epoch 5 iter 67 loss=0.6114888787269592\n",
      "epoch 5 iter 68 loss=0.4890596568584442\n",
      "epoch 5 iter 69 loss=0.875514566898346\n",
      "epoch 5 iter 70 loss=0.6220704317092896\n",
      "epoch 5 iter 71 loss=0.3054834306240082\n",
      "epoch 5 iter 72 loss=0.4945027530193329\n",
      "epoch 5 iter 73 loss=0.6083255410194397\n",
      "epoch 5 iter 74 loss=0.691211462020874\n",
      "epoch 6 iter 0 loss=0.4100904166698456\n",
      "epoch 6 iter 1 loss=1.4714192152023315\n",
      "epoch 6 iter 2 loss=0.4531787037849426\n",
      "epoch 6 iter 3 loss=0.8897361159324646\n",
      "epoch 6 iter 4 loss=1.056227684020996\n",
      "epoch 6 iter 5 loss=0.5595874786376953\n",
      "epoch 6 iter 6 loss=0.6540395021438599\n",
      "epoch 6 iter 7 loss=0.46261388063430786\n",
      "epoch 6 iter 8 loss=0.5489724278450012\n",
      "epoch 6 iter 9 loss=0.5750093460083008\n",
      "epoch 6 iter 10 loss=0.6748749613761902\n",
      "epoch 6 iter 11 loss=0.5539409518241882\n",
      "epoch 6 iter 12 loss=0.6027402281761169\n",
      "epoch 6 iter 13 loss=0.9256563782691956\n",
      "epoch 6 iter 14 loss=0.6014609336853027\n",
      "epoch 6 iter 15 loss=0.6341619491577148\n",
      "epoch 6 iter 16 loss=0.7644073367118835\n",
      "epoch 6 iter 17 loss=0.8271791338920593\n",
      "epoch 6 iter 18 loss=0.45390036702156067\n",
      "epoch 6 iter 19 loss=0.5598642826080322\n",
      "epoch 6 iter 20 loss=0.6462523937225342\n",
      "epoch 6 iter 21 loss=0.41112443804740906\n",
      "epoch 6 iter 22 loss=0.6678885221481323\n",
      "epoch 6 iter 23 loss=0.5493042469024658\n",
      "epoch 6 iter 24 loss=0.8749186396598816\n",
      "epoch 6 iter 25 loss=0.5598758459091187\n",
      "epoch 6 iter 26 loss=0.5462684035301208\n",
      "epoch 6 iter 27 loss=1.1179651021957397\n",
      "epoch 6 iter 28 loss=0.6044743657112122\n",
      "epoch 6 iter 29 loss=0.3874721825122833\n",
      "epoch 6 iter 30 loss=0.8816084265708923\n",
      "epoch 6 iter 31 loss=0.603405237197876\n",
      "epoch 6 iter 32 loss=0.6778765916824341\n",
      "epoch 6 iter 33 loss=0.6223363280296326\n",
      "epoch 6 iter 34 loss=0.5059769153594971\n",
      "epoch 6 iter 35 loss=0.8035746812820435\n",
      "epoch 6 iter 36 loss=0.615364670753479\n",
      "epoch 6 iter 37 loss=0.5733364224433899\n",
      "epoch 6 iter 38 loss=0.4177177846431732\n",
      "epoch 6 iter 39 loss=0.6543667912483215\n",
      "epoch 6 iter 40 loss=0.6628456115722656\n",
      "epoch 6 iter 41 loss=0.8146633505821228\n",
      "epoch 6 iter 42 loss=0.7611619830131531\n",
      "epoch 6 iter 43 loss=0.3783494532108307\n",
      "epoch 6 iter 44 loss=0.766170084476471\n",
      "epoch 6 iter 45 loss=0.43368250131607056\n",
      "epoch 6 iter 46 loss=0.48871132731437683\n",
      "epoch 6 iter 47 loss=0.45559003949165344\n",
      "epoch 6 iter 48 loss=0.3935069143772125\n",
      "epoch 6 iter 49 loss=0.6850764155387878\n",
      "epoch 6 iter 50 loss=0.9493991732597351\n",
      "epoch 6 iter 51 loss=0.3401276171207428\n",
      "epoch 6 iter 52 loss=0.938924252986908\n",
      "epoch 6 iter 53 loss=0.30755743384361267\n",
      "epoch 6 iter 54 loss=0.5627524852752686\n",
      "epoch 6 iter 55 loss=0.8201308846473694\n",
      "epoch 6 iter 56 loss=0.6130186915397644\n",
      "epoch 6 iter 57 loss=0.3752257227897644\n",
      "epoch 6 iter 58 loss=0.6038079261779785\n",
      "epoch 6 iter 59 loss=0.4965895712375641\n",
      "epoch 6 iter 60 loss=0.4794350564479828\n",
      "epoch 6 iter 61 loss=0.6504672169685364\n",
      "epoch 6 iter 62 loss=0.47026392817497253\n",
      "epoch 6 iter 63 loss=0.4865454435348511\n",
      "epoch 6 iter 64 loss=0.6463598608970642\n",
      "epoch 6 iter 65 loss=0.5626519322395325\n",
      "epoch 6 iter 66 loss=0.548279881477356\n",
      "epoch 6 iter 67 loss=0.45867079496383667\n",
      "epoch 6 iter 68 loss=0.4755305349826813\n",
      "epoch 6 iter 69 loss=1.1007412672042847\n",
      "epoch 6 iter 70 loss=0.7128651738166809\n",
      "epoch 6 iter 71 loss=0.6479379534721375\n",
      "epoch 6 iter 72 loss=0.7199428081512451\n",
      "epoch 6 iter 73 loss=0.4037785530090332\n",
      "epoch 6 iter 74 loss=0.6562919020652771\n",
      "epoch 7 iter 0 loss=0.5897804498672485\n",
      "epoch 7 iter 1 loss=0.40981191396713257\n",
      "epoch 7 iter 2 loss=0.5874328017234802\n",
      "epoch 7 iter 3 loss=0.43953368067741394\n",
      "epoch 7 iter 4 loss=0.3610527217388153\n",
      "epoch 7 iter 5 loss=0.4616714119911194\n",
      "epoch 7 iter 6 loss=0.5268007516860962\n",
      "epoch 7 iter 7 loss=1.3184998035430908\n",
      "epoch 7 iter 8 loss=0.673263669013977\n",
      "epoch 7 iter 9 loss=0.31875041127204895\n",
      "epoch 7 iter 10 loss=0.7905489206314087\n",
      "epoch 7 iter 11 loss=0.9926983118057251\n",
      "epoch 7 iter 12 loss=0.8671647310256958\n",
      "epoch 7 iter 13 loss=0.4535919427871704\n",
      "epoch 7 iter 14 loss=0.6225327253341675\n",
      "epoch 7 iter 15 loss=0.6220229864120483\n",
      "epoch 7 iter 16 loss=0.49602511525154114\n",
      "epoch 7 iter 17 loss=0.6718782186508179\n",
      "epoch 7 iter 18 loss=0.5046871900558472\n",
      "epoch 7 iter 19 loss=0.4464930593967438\n",
      "epoch 7 iter 20 loss=0.626161515712738\n",
      "epoch 7 iter 21 loss=0.6406500339508057\n",
      "epoch 7 iter 22 loss=0.5228698253631592\n",
      "epoch 7 iter 23 loss=0.39979320764541626\n",
      "epoch 7 iter 24 loss=0.4633142054080963\n",
      "epoch 7 iter 25 loss=0.408808171749115\n",
      "epoch 7 iter 26 loss=1.271505355834961\n",
      "epoch 7 iter 27 loss=0.32531672716140747\n",
      "epoch 7 iter 28 loss=0.4039191007614136\n",
      "epoch 7 iter 29 loss=0.8493719100952148\n",
      "epoch 7 iter 30 loss=0.6601743698120117\n",
      "epoch 7 iter 31 loss=1.3405615091323853\n",
      "epoch 7 iter 32 loss=0.30103394389152527\n",
      "epoch 7 iter 33 loss=0.41783520579338074\n",
      "epoch 7 iter 34 loss=0.9327043890953064\n",
      "epoch 7 iter 35 loss=0.6243941187858582\n",
      "epoch 7 iter 36 loss=0.8389486074447632\n",
      "epoch 7 iter 37 loss=0.9272116422653198\n",
      "epoch 7 iter 38 loss=0.5791266560554504\n",
      "epoch 7 iter 39 loss=0.811323881149292\n",
      "epoch 7 iter 40 loss=0.5588377118110657\n",
      "epoch 7 iter 41 loss=0.7152343392372131\n",
      "epoch 7 iter 42 loss=0.8255829215049744\n",
      "epoch 7 iter 43 loss=0.44278982281684875\n",
      "epoch 7 iter 44 loss=0.6017061471939087\n",
      "epoch 7 iter 45 loss=0.7687789797782898\n",
      "epoch 7 iter 46 loss=0.8030070662498474\n",
      "epoch 7 iter 47 loss=0.615164577960968\n",
      "epoch 7 iter 48 loss=0.5061774849891663\n",
      "epoch 7 iter 49 loss=0.7210798263549805\n",
      "epoch 7 iter 50 loss=0.5731766223907471\n",
      "epoch 7 iter 51 loss=0.5512783527374268\n",
      "epoch 7 iter 52 loss=0.8062994480133057\n",
      "epoch 7 iter 53 loss=0.478395938873291\n",
      "epoch 7 iter 54 loss=1.2931665182113647\n",
      "epoch 7 iter 55 loss=0.4987338185310364\n",
      "epoch 7 iter 56 loss=0.6606769561767578\n",
      "epoch 7 iter 57 loss=0.5462771654129028\n",
      "epoch 7 iter 58 loss=0.9609125256538391\n",
      "epoch 7 iter 59 loss=0.552122950553894\n",
      "epoch 7 iter 60 loss=0.5686015486717224\n",
      "epoch 7 iter 61 loss=0.7799338102340698\n",
      "epoch 7 iter 62 loss=0.4105938673019409\n",
      "epoch 7 iter 63 loss=0.36623668670654297\n",
      "epoch 7 iter 64 loss=0.45030829310417175\n",
      "epoch 7 iter 65 loss=0.918487548828125\n",
      "epoch 7 iter 66 loss=0.4462721347808838\n",
      "epoch 7 iter 67 loss=0.6103346943855286\n",
      "epoch 7 iter 68 loss=0.5090200901031494\n",
      "epoch 7 iter 69 loss=0.8526616096496582\n",
      "epoch 7 iter 70 loss=0.3936089873313904\n",
      "epoch 7 iter 71 loss=0.49661117792129517\n",
      "epoch 7 iter 72 loss=0.44021618366241455\n",
      "epoch 7 iter 73 loss=0.6920092105865479\n",
      "epoch 7 iter 74 loss=0.49460750818252563\n",
      "epoch 8 iter 0 loss=0.43612968921661377\n",
      "epoch 8 iter 1 loss=0.32865557074546814\n",
      "epoch 8 iter 2 loss=0.46980857849121094\n",
      "epoch 8 iter 3 loss=0.8739134073257446\n",
      "epoch 8 iter 4 loss=0.344140887260437\n",
      "epoch 8 iter 5 loss=0.6708506345748901\n",
      "epoch 8 iter 6 loss=0.7271177768707275\n",
      "epoch 8 iter 7 loss=0.48900386691093445\n",
      "epoch 8 iter 8 loss=0.4659040570259094\n",
      "epoch 8 iter 9 loss=0.5456156134605408\n",
      "epoch 8 iter 10 loss=0.3891446590423584\n",
      "epoch 8 iter 11 loss=0.5718791484832764\n",
      "epoch 8 iter 12 loss=0.5666776299476624\n",
      "epoch 8 iter 13 loss=0.6788656711578369\n",
      "epoch 8 iter 14 loss=0.9303009510040283\n",
      "epoch 8 iter 15 loss=0.6926982402801514\n",
      "epoch 8 iter 16 loss=0.4857756495475769\n",
      "epoch 8 iter 17 loss=0.929801881313324\n",
      "epoch 8 iter 18 loss=0.49126502871513367\n",
      "epoch 8 iter 19 loss=0.4598388969898224\n",
      "epoch 8 iter 20 loss=0.5306879878044128\n",
      "epoch 8 iter 21 loss=0.6188568472862244\n",
      "epoch 8 iter 22 loss=0.6054757833480835\n",
      "epoch 8 iter 23 loss=0.7122976183891296\n",
      "epoch 8 iter 24 loss=0.5579849481582642\n",
      "epoch 8 iter 25 loss=0.797222375869751\n",
      "epoch 8 iter 26 loss=0.47257691621780396\n",
      "epoch 8 iter 27 loss=0.9399286508560181\n",
      "epoch 8 iter 28 loss=0.827355682849884\n",
      "epoch 8 iter 29 loss=0.5905698537826538\n",
      "epoch 8 iter 30 loss=0.7719595432281494\n",
      "epoch 8 iter 31 loss=1.1580103635787964\n",
      "epoch 8 iter 32 loss=0.4214963912963867\n",
      "epoch 8 iter 33 loss=0.5055235028266907\n",
      "epoch 8 iter 34 loss=0.3457564413547516\n",
      "epoch 8 iter 35 loss=0.35334691405296326\n",
      "epoch 8 iter 36 loss=0.5568395853042603\n",
      "epoch 8 iter 37 loss=1.174355387687683\n",
      "epoch 8 iter 38 loss=0.6902589201927185\n",
      "epoch 8 iter 39 loss=0.8155809640884399\n",
      "epoch 8 iter 40 loss=0.3273521065711975\n",
      "epoch 8 iter 41 loss=0.41451236605644226\n",
      "epoch 8 iter 42 loss=0.5900509357452393\n",
      "epoch 8 iter 43 loss=0.5917081832885742\n",
      "epoch 8 iter 44 loss=0.40334802865982056\n",
      "epoch 8 iter 45 loss=0.6621338725090027\n",
      "epoch 8 iter 46 loss=0.48124971985816956\n",
      "epoch 8 iter 47 loss=0.3645770847797394\n",
      "epoch 8 iter 48 loss=0.3205128610134125\n",
      "epoch 8 iter 49 loss=0.5357803106307983\n",
      "epoch 8 iter 50 loss=0.5810294151306152\n",
      "epoch 8 iter 51 loss=1.1092592477798462\n",
      "epoch 8 iter 52 loss=0.5019020438194275\n",
      "epoch 8 iter 53 loss=0.36611008644104004\n",
      "epoch 8 iter 54 loss=0.46643638610839844\n",
      "epoch 8 iter 55 loss=0.8285024166107178\n",
      "epoch 8 iter 56 loss=0.6465619206428528\n",
      "epoch 8 iter 57 loss=0.5589024424552917\n",
      "epoch 8 iter 58 loss=0.5247089862823486\n",
      "epoch 8 iter 59 loss=0.5377209782600403\n",
      "epoch 8 iter 60 loss=0.3352007567882538\n",
      "epoch 8 iter 61 loss=0.7079713940620422\n",
      "epoch 8 iter 62 loss=0.5690595507621765\n",
      "epoch 8 iter 63 loss=0.44400113821029663\n",
      "epoch 8 iter 64 loss=0.7996971011161804\n",
      "epoch 8 iter 65 loss=0.734288215637207\n",
      "epoch 8 iter 66 loss=0.5324325561523438\n",
      "epoch 8 iter 67 loss=0.7732279300689697\n",
      "epoch 8 iter 68 loss=0.3370082676410675\n",
      "epoch 8 iter 69 loss=0.47097301483154297\n",
      "epoch 8 iter 70 loss=0.5593714118003845\n",
      "epoch 8 iter 71 loss=0.5778919458389282\n",
      "epoch 8 iter 72 loss=0.26208606362342834\n",
      "epoch 8 iter 73 loss=0.47444504499435425\n",
      "epoch 8 iter 74 loss=0.5772637724876404\n",
      "epoch 9 iter 0 loss=0.6464579105377197\n",
      "epoch 9 iter 1 loss=0.3563147485256195\n",
      "epoch 9 iter 2 loss=0.3592415153980255\n",
      "epoch 9 iter 3 loss=0.54038405418396\n",
      "epoch 9 iter 4 loss=0.42194151878356934\n",
      "epoch 9 iter 5 loss=0.7844095230102539\n",
      "epoch 9 iter 6 loss=0.9292336702346802\n",
      "epoch 9 iter 7 loss=0.6852447390556335\n",
      "epoch 9 iter 8 loss=0.4772004187107086\n",
      "epoch 9 iter 9 loss=0.746565043926239\n",
      "epoch 9 iter 10 loss=0.6235162615776062\n",
      "epoch 9 iter 11 loss=0.6037321090698242\n",
      "epoch 9 iter 12 loss=0.6562042236328125\n",
      "epoch 9 iter 13 loss=0.4108222424983978\n",
      "epoch 9 iter 14 loss=0.3962244987487793\n",
      "epoch 9 iter 15 loss=0.5836713314056396\n",
      "epoch 9 iter 16 loss=0.6053040623664856\n",
      "epoch 9 iter 17 loss=0.8463395237922668\n",
      "epoch 9 iter 18 loss=0.3600340783596039\n",
      "epoch 9 iter 19 loss=1.0386779308319092\n",
      "epoch 9 iter 20 loss=0.47052428126335144\n",
      "epoch 9 iter 21 loss=0.4445178508758545\n",
      "epoch 9 iter 22 loss=0.45210063457489014\n",
      "epoch 9 iter 23 loss=0.39709290862083435\n",
      "epoch 9 iter 24 loss=0.4931778311729431\n",
      "epoch 9 iter 25 loss=0.5619167685508728\n",
      "epoch 9 iter 26 loss=0.7733933329582214\n",
      "epoch 9 iter 27 loss=0.7174975872039795\n",
      "epoch 9 iter 28 loss=0.46480703353881836\n",
      "epoch 9 iter 29 loss=0.5525736212730408\n",
      "epoch 9 iter 30 loss=0.6590483784675598\n",
      "epoch 9 iter 31 loss=0.5009365677833557\n",
      "epoch 9 iter 32 loss=1.2093827724456787\n",
      "epoch 9 iter 33 loss=0.6404139995574951\n",
      "epoch 9 iter 34 loss=0.5643495321273804\n",
      "epoch 9 iter 35 loss=0.6729193329811096\n",
      "epoch 9 iter 36 loss=0.8206495642662048\n",
      "epoch 9 iter 37 loss=0.42929697036743164\n",
      "epoch 9 iter 38 loss=0.49360448122024536\n",
      "epoch 9 iter 39 loss=0.4152067005634308\n",
      "epoch 9 iter 40 loss=0.2987491488456726\n",
      "epoch 9 iter 41 loss=0.5590040683746338\n",
      "epoch 9 iter 42 loss=0.8135096430778503\n",
      "epoch 9 iter 43 loss=0.6203007102012634\n",
      "epoch 9 iter 44 loss=0.43225690722465515\n",
      "epoch 9 iter 45 loss=0.8569084405899048\n",
      "epoch 9 iter 46 loss=0.4656444191932678\n",
      "epoch 9 iter 47 loss=0.7661057114601135\n",
      "epoch 9 iter 48 loss=0.3703266680240631\n",
      "epoch 9 iter 49 loss=0.37555915117263794\n",
      "epoch 9 iter 50 loss=0.4776277244091034\n",
      "epoch 9 iter 51 loss=0.5741905570030212\n",
      "epoch 9 iter 52 loss=0.36962050199508667\n",
      "epoch 9 iter 53 loss=0.6254882216453552\n",
      "epoch 9 iter 54 loss=0.6015300750732422\n",
      "epoch 9 iter 55 loss=0.8977174162864685\n",
      "epoch 9 iter 56 loss=0.722866415977478\n",
      "epoch 9 iter 57 loss=0.405516117811203\n",
      "epoch 9 iter 58 loss=0.4806726276874542\n",
      "epoch 9 iter 59 loss=0.6278769969940186\n",
      "epoch 9 iter 60 loss=0.40735888481140137\n",
      "epoch 9 iter 61 loss=0.41019129753112793\n",
      "epoch 9 iter 62 loss=0.47292834520339966\n",
      "epoch 9 iter 63 loss=0.6802672147750854\n",
      "epoch 9 iter 64 loss=0.2653258144855499\n",
      "epoch 9 iter 65 loss=0.3145410418510437\n",
      "epoch 9 iter 66 loss=0.286200612783432\n",
      "epoch 9 iter 67 loss=0.8867149949073792\n",
      "epoch 9 iter 68 loss=0.3186253309249878\n",
      "epoch 9 iter 69 loss=0.33725616335868835\n",
      "epoch 9 iter 70 loss=0.3791082203388214\n",
      "epoch 9 iter 71 loss=0.859741747379303\n",
      "epoch 9 iter 72 loss=0.35241708159446716\n",
      "epoch 9 iter 73 loss=0.3841206133365631\n",
      "epoch 9 iter 74 loss=0.3764735758304596\n",
      "epoch 10 iter 0 loss=0.5984142422676086\n",
      "epoch 10 iter 1 loss=0.4350828528404236\n",
      "epoch 10 iter 2 loss=0.46441784501075745\n",
      "epoch 10 iter 3 loss=0.380526065826416\n",
      "epoch 10 iter 4 loss=0.8666535019874573\n",
      "epoch 10 iter 5 loss=0.26221737265586853\n",
      "epoch 10 iter 6 loss=0.3592461347579956\n",
      "epoch 10 iter 7 loss=0.8213478326797485\n",
      "epoch 10 iter 8 loss=0.267465204000473\n",
      "epoch 10 iter 9 loss=0.5133769512176514\n",
      "epoch 10 iter 10 loss=0.5082399249076843\n",
      "epoch 10 iter 11 loss=0.539136528968811\n",
      "epoch 10 iter 12 loss=0.38427168130874634\n",
      "epoch 10 iter 13 loss=0.5291858911514282\n",
      "epoch 10 iter 14 loss=0.6657770276069641\n",
      "epoch 10 iter 15 loss=0.714176595211029\n",
      "epoch 10 iter 16 loss=0.5270698666572571\n",
      "epoch 10 iter 17 loss=0.45668652653694153\n",
      "epoch 10 iter 18 loss=0.3360796868801117\n",
      "epoch 10 iter 19 loss=0.38517171144485474\n",
      "epoch 10 iter 20 loss=0.551688015460968\n",
      "epoch 10 iter 21 loss=0.497986376285553\n",
      "epoch 10 iter 22 loss=0.6387677788734436\n",
      "epoch 10 iter 23 loss=0.4981783926486969\n",
      "epoch 10 iter 24 loss=0.5032669305801392\n",
      "epoch 10 iter 25 loss=0.3190367817878723\n",
      "epoch 10 iter 26 loss=0.6159016489982605\n",
      "epoch 10 iter 27 loss=0.7794623970985413\n",
      "epoch 10 iter 28 loss=0.8344181180000305\n",
      "epoch 10 iter 29 loss=0.5508167147636414\n",
      "epoch 10 iter 30 loss=0.3356897234916687\n",
      "epoch 10 iter 31 loss=0.2583023011684418\n",
      "epoch 10 iter 32 loss=0.3186909258365631\n",
      "epoch 10 iter 33 loss=0.40635931491851807\n",
      "epoch 10 iter 34 loss=0.4507647454738617\n",
      "epoch 10 iter 35 loss=0.36112120747566223\n",
      "epoch 10 iter 36 loss=0.5373186469078064\n",
      "epoch 10 iter 37 loss=0.5813606381416321\n",
      "epoch 10 iter 38 loss=0.41062408685684204\n",
      "epoch 10 iter 39 loss=0.601716935634613\n",
      "epoch 10 iter 40 loss=1.2094191312789917\n",
      "epoch 10 iter 41 loss=1.3795956373214722\n",
      "epoch 10 iter 42 loss=0.41348978877067566\n",
      "epoch 10 iter 43 loss=0.5141707062721252\n",
      "epoch 10 iter 44 loss=1.3250714540481567\n",
      "epoch 10 iter 45 loss=0.5001617670059204\n",
      "epoch 10 iter 46 loss=0.5471321940422058\n",
      "epoch 10 iter 47 loss=0.4119465947151184\n",
      "epoch 10 iter 48 loss=0.41712096333503723\n",
      "epoch 10 iter 49 loss=0.5728045105934143\n",
      "epoch 10 iter 50 loss=0.6651784181594849\n",
      "epoch 10 iter 51 loss=0.6158719658851624\n",
      "epoch 10 iter 52 loss=0.3947528004646301\n",
      "epoch 10 iter 53 loss=0.604401707649231\n",
      "epoch 10 iter 54 loss=0.31520184874534607\n",
      "epoch 10 iter 55 loss=0.3928397297859192\n",
      "epoch 10 iter 56 loss=0.3750905990600586\n",
      "epoch 10 iter 57 loss=0.22557388246059418\n",
      "epoch 10 iter 58 loss=0.764439582824707\n",
      "epoch 10 iter 59 loss=0.9823954105377197\n",
      "epoch 10 iter 60 loss=0.30190470814704895\n",
      "epoch 10 iter 61 loss=0.47213342785835266\n",
      "epoch 10 iter 62 loss=1.0118001699447632\n",
      "epoch 10 iter 63 loss=0.6945672631263733\n",
      "epoch 10 iter 64 loss=0.6673075556755066\n",
      "epoch 10 iter 65 loss=0.5241158604621887\n",
      "epoch 10 iter 66 loss=0.39328157901763916\n",
      "epoch 10 iter 67 loss=0.518049418926239\n",
      "epoch 10 iter 68 loss=0.4836631119251251\n",
      "epoch 10 iter 69 loss=0.8991214036941528\n",
      "epoch 10 iter 70 loss=0.42468610405921936\n",
      "epoch 10 iter 71 loss=0.6954339742660522\n",
      "epoch 10 iter 72 loss=0.6004498600959778\n",
      "epoch 10 iter 73 loss=0.5656390190124512\n",
      "epoch 10 iter 74 loss=0.33884257078170776\n",
      "epoch 11 iter 0 loss=0.4280698299407959\n",
      "epoch 11 iter 1 loss=0.6492048501968384\n",
      "epoch 11 iter 2 loss=0.6390253901481628\n",
      "epoch 11 iter 3 loss=0.5302794575691223\n",
      "epoch 11 iter 4 loss=0.5210019946098328\n",
      "epoch 11 iter 5 loss=0.8130602240562439\n",
      "epoch 11 iter 6 loss=0.7794802188873291\n",
      "epoch 11 iter 7 loss=0.3878089189529419\n",
      "epoch 11 iter 8 loss=0.6162802577018738\n",
      "epoch 11 iter 9 loss=0.4567514955997467\n",
      "epoch 11 iter 10 loss=0.5506746172904968\n",
      "epoch 11 iter 11 loss=0.39256802201271057\n",
      "epoch 11 iter 12 loss=0.5478712916374207\n",
      "epoch 11 iter 13 loss=0.37624356150627136\n",
      "epoch 11 iter 14 loss=1.0621674060821533\n",
      "epoch 11 iter 15 loss=0.5320403575897217\n",
      "epoch 11 iter 16 loss=0.47479864954948425\n",
      "epoch 11 iter 17 loss=0.3403330445289612\n",
      "epoch 11 iter 18 loss=0.39497241377830505\n",
      "epoch 11 iter 19 loss=0.4679447114467621\n",
      "epoch 11 iter 20 loss=0.4871410131454468\n",
      "epoch 11 iter 21 loss=0.4009537994861603\n",
      "epoch 11 iter 22 loss=0.6367046236991882\n",
      "epoch 11 iter 23 loss=0.676230788230896\n",
      "epoch 11 iter 24 loss=0.4912435710430145\n",
      "epoch 11 iter 25 loss=0.3301328122615814\n",
      "epoch 11 iter 26 loss=0.7203521728515625\n",
      "epoch 11 iter 27 loss=0.48016950488090515\n",
      "epoch 11 iter 28 loss=0.7720921635627747\n",
      "epoch 11 iter 29 loss=0.34998664259910583\n",
      "epoch 11 iter 30 loss=0.3761765956878662\n",
      "epoch 11 iter 31 loss=0.34845468401908875\n",
      "epoch 11 iter 32 loss=0.35686230659484863\n",
      "epoch 11 iter 33 loss=0.4439331591129303\n",
      "epoch 11 iter 34 loss=0.25404244661331177\n",
      "epoch 11 iter 35 loss=0.4517515301704407\n",
      "epoch 11 iter 36 loss=0.4371778964996338\n",
      "epoch 11 iter 37 loss=0.4575754404067993\n",
      "epoch 11 iter 38 loss=0.6003039479255676\n",
      "epoch 11 iter 39 loss=0.5790723562240601\n",
      "epoch 11 iter 40 loss=0.5224144458770752\n",
      "epoch 11 iter 41 loss=0.3977852165699005\n",
      "epoch 11 iter 42 loss=0.5234810709953308\n",
      "epoch 11 iter 43 loss=0.6545627117156982\n",
      "epoch 11 iter 44 loss=0.7913299798965454\n",
      "epoch 11 iter 45 loss=0.5096569657325745\n",
      "epoch 11 iter 46 loss=0.45815518498420715\n",
      "epoch 11 iter 47 loss=0.2868826687335968\n",
      "epoch 11 iter 48 loss=0.40961042046546936\n",
      "epoch 11 iter 49 loss=0.8933719396591187\n",
      "epoch 11 iter 50 loss=0.47726204991340637\n",
      "epoch 11 iter 51 loss=0.46433207392692566\n",
      "epoch 11 iter 52 loss=0.49538660049438477\n",
      "epoch 11 iter 53 loss=1.2432632446289062\n",
      "epoch 11 iter 54 loss=0.4460136592388153\n",
      "epoch 11 iter 55 loss=0.5100663304328918\n",
      "epoch 11 iter 56 loss=0.4779426157474518\n",
      "epoch 11 iter 57 loss=0.38253864645957947\n",
      "epoch 11 iter 58 loss=0.3791685998439789\n",
      "epoch 11 iter 59 loss=0.4297640025615692\n",
      "epoch 11 iter 60 loss=0.7412744164466858\n",
      "epoch 11 iter 61 loss=0.5502760410308838\n",
      "epoch 11 iter 62 loss=0.40350502729415894\n",
      "epoch 11 iter 63 loss=0.5004430413246155\n",
      "epoch 11 iter 64 loss=0.5635796785354614\n",
      "epoch 11 iter 65 loss=0.4786936044692993\n",
      "epoch 11 iter 66 loss=0.6162884831428528\n",
      "epoch 11 iter 67 loss=0.4170580208301544\n",
      "epoch 11 iter 68 loss=0.3293304741382599\n",
      "epoch 11 iter 69 loss=0.5719829797744751\n",
      "epoch 11 iter 70 loss=0.5396219491958618\n",
      "epoch 11 iter 71 loss=0.39587846398353577\n",
      "epoch 11 iter 72 loss=0.5887805819511414\n",
      "epoch 11 iter 73 loss=0.4158383905887604\n",
      "epoch 11 iter 74 loss=0.3429138660430908\n",
      "epoch 12 iter 0 loss=0.7868385314941406\n",
      "epoch 12 iter 1 loss=0.8591881990432739\n",
      "epoch 12 iter 2 loss=0.3257777988910675\n",
      "epoch 12 iter 3 loss=0.5222219228744507\n",
      "epoch 12 iter 4 loss=1.0336946249008179\n",
      "epoch 12 iter 5 loss=0.6229807734489441\n",
      "epoch 12 iter 6 loss=0.5563014149665833\n",
      "epoch 12 iter 7 loss=0.8560096025466919\n",
      "epoch 12 iter 8 loss=0.5064919590950012\n",
      "epoch 12 iter 9 loss=0.4326252341270447\n",
      "epoch 12 iter 10 loss=0.7423555254936218\n",
      "epoch 12 iter 11 loss=0.40426817536354065\n",
      "epoch 12 iter 12 loss=0.7683013081550598\n",
      "epoch 12 iter 13 loss=0.4818077087402344\n",
      "epoch 12 iter 14 loss=0.3670858144760132\n",
      "epoch 12 iter 15 loss=0.636468231678009\n",
      "epoch 12 iter 16 loss=0.48008832335472107\n",
      "epoch 12 iter 17 loss=0.5553624033927917\n",
      "epoch 12 iter 18 loss=0.2845982313156128\n",
      "epoch 12 iter 19 loss=0.4605047106742859\n",
      "epoch 12 iter 20 loss=0.38553741574287415\n",
      "epoch 12 iter 21 loss=0.6694671511650085\n",
      "epoch 12 iter 22 loss=0.3854828178882599\n",
      "epoch 12 iter 23 loss=0.4287889301776886\n",
      "epoch 12 iter 24 loss=0.46003445982933044\n",
      "epoch 12 iter 25 loss=0.34312376379966736\n",
      "epoch 12 iter 26 loss=0.6025311350822449\n",
      "epoch 12 iter 27 loss=0.5106031894683838\n",
      "epoch 12 iter 28 loss=0.439447283744812\n",
      "epoch 12 iter 29 loss=0.9265686869621277\n",
      "epoch 12 iter 30 loss=0.5667836666107178\n",
      "epoch 12 iter 31 loss=0.975043773651123\n",
      "epoch 12 iter 32 loss=0.33224713802337646\n",
      "epoch 12 iter 33 loss=0.6085658073425293\n",
      "epoch 12 iter 34 loss=0.452700674533844\n",
      "epoch 12 iter 35 loss=0.4377511739730835\n",
      "epoch 12 iter 36 loss=0.6081925630569458\n",
      "epoch 12 iter 37 loss=0.3971678614616394\n",
      "epoch 12 iter 38 loss=0.5187175273895264\n",
      "epoch 12 iter 39 loss=0.5800129771232605\n",
      "epoch 12 iter 40 loss=0.3380129933357239\n",
      "epoch 12 iter 41 loss=0.7488248348236084\n",
      "epoch 12 iter 42 loss=0.4066521227359772\n",
      "epoch 12 iter 43 loss=0.4842830002307892\n",
      "epoch 12 iter 44 loss=0.3722781836986542\n",
      "epoch 12 iter 45 loss=0.3517497479915619\n",
      "epoch 12 iter 46 loss=0.4307887554168701\n",
      "epoch 12 iter 47 loss=0.30550915002822876\n",
      "epoch 12 iter 48 loss=0.555056095123291\n",
      "epoch 12 iter 49 loss=0.2647944390773773\n",
      "epoch 12 iter 50 loss=0.7500991821289062\n",
      "epoch 12 iter 51 loss=0.27827492356300354\n",
      "epoch 12 iter 52 loss=0.664222002029419\n",
      "epoch 12 iter 53 loss=0.2927815914154053\n",
      "epoch 12 iter 54 loss=0.48837172985076904\n",
      "epoch 12 iter 55 loss=0.3578701615333557\n",
      "epoch 12 iter 56 loss=0.3923785388469696\n",
      "epoch 12 iter 57 loss=0.3136463463306427\n",
      "epoch 12 iter 58 loss=0.4814944565296173\n",
      "epoch 12 iter 59 loss=0.515381395816803\n",
      "epoch 12 iter 60 loss=0.4541603624820709\n",
      "epoch 12 iter 61 loss=0.5005868673324585\n",
      "epoch 12 iter 62 loss=0.4249285161495209\n",
      "epoch 12 iter 63 loss=0.3209662139415741\n",
      "epoch 12 iter 64 loss=0.5734629034996033\n",
      "epoch 12 iter 65 loss=0.7554033994674683\n",
      "epoch 12 iter 66 loss=0.43862631916999817\n",
      "epoch 12 iter 67 loss=0.4005680978298187\n",
      "epoch 12 iter 68 loss=0.41409409046173096\n",
      "epoch 12 iter 69 loss=0.34648409485816956\n",
      "epoch 12 iter 70 loss=0.47234538197517395\n",
      "epoch 12 iter 71 loss=0.34123513102531433\n",
      "epoch 12 iter 72 loss=0.5426961779594421\n",
      "epoch 12 iter 73 loss=0.45897504687309265\n",
      "epoch 12 iter 74 loss=0.3028554320335388\n",
      "epoch 13 iter 0 loss=0.64189612865448\n",
      "epoch 13 iter 1 loss=0.5506988763809204\n",
      "epoch 13 iter 2 loss=0.2736791968345642\n",
      "epoch 13 iter 3 loss=0.29454463720321655\n",
      "epoch 13 iter 4 loss=0.5059675574302673\n",
      "epoch 13 iter 5 loss=0.7266742587089539\n",
      "epoch 13 iter 6 loss=0.7567355036735535\n",
      "epoch 13 iter 7 loss=0.43320637941360474\n",
      "epoch 13 iter 8 loss=0.3794066607952118\n",
      "epoch 13 iter 9 loss=0.487520694732666\n",
      "epoch 13 iter 10 loss=0.3872855603694916\n",
      "epoch 13 iter 11 loss=0.297113835811615\n",
      "epoch 13 iter 12 loss=0.48105931282043457\n",
      "epoch 13 iter 13 loss=0.5232016444206238\n",
      "epoch 13 iter 14 loss=0.4908089339733124\n",
      "epoch 13 iter 15 loss=0.4380747079849243\n",
      "epoch 13 iter 16 loss=0.6447089910507202\n",
      "epoch 13 iter 17 loss=0.5113493204116821\n",
      "epoch 13 iter 18 loss=0.36707791686058044\n",
      "epoch 13 iter 19 loss=0.31182119250297546\n",
      "epoch 13 iter 20 loss=0.8930416107177734\n",
      "epoch 13 iter 21 loss=0.4883345067501068\n",
      "epoch 13 iter 22 loss=0.2741381525993347\n",
      "epoch 13 iter 23 loss=0.3262113332748413\n",
      "epoch 13 iter 24 loss=0.46002647280693054\n",
      "epoch 13 iter 25 loss=0.3662342131137848\n",
      "epoch 13 iter 26 loss=0.3770168125629425\n",
      "epoch 13 iter 27 loss=0.6559692025184631\n",
      "epoch 13 iter 28 loss=0.701177716255188\n",
      "epoch 13 iter 29 loss=0.5845211744308472\n",
      "epoch 13 iter 30 loss=0.36986100673675537\n",
      "epoch 13 iter 31 loss=0.1979396939277649\n",
      "epoch 13 iter 32 loss=0.3949286639690399\n",
      "epoch 13 iter 33 loss=0.2777213752269745\n",
      "epoch 13 iter 34 loss=0.6230942010879517\n",
      "epoch 13 iter 35 loss=0.6282973885536194\n",
      "epoch 13 iter 36 loss=0.30361732840538025\n",
      "epoch 13 iter 37 loss=0.595992922782898\n",
      "epoch 13 iter 38 loss=0.4763084948062897\n",
      "epoch 13 iter 39 loss=0.2507438063621521\n",
      "epoch 13 iter 40 loss=0.48373115062713623\n",
      "epoch 13 iter 41 loss=0.4903561472892761\n",
      "epoch 13 iter 42 loss=0.4970945417881012\n",
      "epoch 13 iter 43 loss=0.4472509026527405\n",
      "epoch 13 iter 44 loss=0.5127212405204773\n",
      "epoch 13 iter 45 loss=0.3081091642379761\n",
      "epoch 13 iter 46 loss=0.41864970326423645\n",
      "epoch 13 iter 47 loss=0.4967450797557831\n",
      "epoch 13 iter 48 loss=0.2949994206428528\n",
      "epoch 13 iter 49 loss=0.5056443214416504\n",
      "epoch 13 iter 50 loss=0.5222351551055908\n",
      "epoch 13 iter 51 loss=0.37097546458244324\n",
      "epoch 13 iter 52 loss=0.5342108011245728\n",
      "epoch 13 iter 53 loss=0.3070286214351654\n",
      "epoch 13 iter 54 loss=0.7189875245094299\n",
      "epoch 13 iter 55 loss=0.22670677304267883\n",
      "epoch 13 iter 56 loss=0.8663964867591858\n",
      "epoch 13 iter 57 loss=0.35473722219467163\n",
      "epoch 13 iter 58 loss=0.3831756114959717\n",
      "epoch 13 iter 59 loss=0.4937632381916046\n",
      "epoch 13 iter 60 loss=0.34199658036231995\n",
      "epoch 13 iter 61 loss=0.6442201733589172\n",
      "epoch 13 iter 62 loss=0.2524394094944\n",
      "epoch 13 iter 63 loss=0.984911322593689\n",
      "epoch 13 iter 64 loss=0.5221444368362427\n",
      "epoch 13 iter 65 loss=0.42148256301879883\n",
      "epoch 13 iter 66 loss=1.041166067123413\n",
      "epoch 13 iter 67 loss=0.3200641870498657\n",
      "epoch 13 iter 68 loss=0.5742787718772888\n",
      "epoch 13 iter 69 loss=0.5655196905136108\n",
      "epoch 13 iter 70 loss=0.32174429297447205\n",
      "epoch 13 iter 71 loss=0.4746597111225128\n",
      "epoch 13 iter 72 loss=0.41862890124320984\n",
      "epoch 13 iter 73 loss=0.5754997134208679\n",
      "epoch 13 iter 74 loss=0.5180372595787048\n",
      "epoch 14 iter 0 loss=0.32897719740867615\n",
      "epoch 14 iter 1 loss=0.35159140825271606\n",
      "epoch 14 iter 2 loss=0.5136937499046326\n",
      "epoch 14 iter 3 loss=0.39916348457336426\n",
      "epoch 14 iter 4 loss=0.29008784890174866\n",
      "epoch 14 iter 5 loss=0.3916884660720825\n",
      "epoch 14 iter 6 loss=0.22849337756633759\n",
      "epoch 14 iter 7 loss=0.39658457040786743\n",
      "epoch 14 iter 8 loss=0.4966239333152771\n",
      "epoch 14 iter 9 loss=0.5094540119171143\n",
      "epoch 14 iter 10 loss=0.32899025082588196\n",
      "epoch 14 iter 11 loss=0.5130031704902649\n",
      "epoch 14 iter 12 loss=0.364180326461792\n",
      "epoch 14 iter 13 loss=0.4218149781227112\n",
      "epoch 14 iter 14 loss=0.24934007227420807\n",
      "epoch 14 iter 15 loss=0.24029086530208588\n",
      "epoch 14 iter 16 loss=0.6048043370246887\n",
      "epoch 14 iter 17 loss=0.36025774478912354\n",
      "epoch 14 iter 18 loss=0.4524565637111664\n",
      "epoch 14 iter 19 loss=0.5230770707130432\n",
      "epoch 14 iter 20 loss=0.3740069270133972\n",
      "epoch 14 iter 21 loss=0.3444208800792694\n",
      "epoch 14 iter 22 loss=0.2755625545978546\n",
      "epoch 14 iter 23 loss=0.433564692735672\n",
      "epoch 14 iter 24 loss=0.8513288497924805\n",
      "epoch 14 iter 25 loss=0.48368966579437256\n",
      "epoch 14 iter 26 loss=0.5245545506477356\n",
      "epoch 14 iter 27 loss=0.5114902853965759\n",
      "epoch 14 iter 28 loss=0.5283295512199402\n",
      "epoch 14 iter 29 loss=0.3596302568912506\n",
      "epoch 14 iter 30 loss=0.2764280140399933\n",
      "epoch 14 iter 31 loss=0.3155229687690735\n",
      "epoch 14 iter 32 loss=0.3811616599559784\n",
      "epoch 14 iter 33 loss=0.28223976492881775\n",
      "epoch 14 iter 34 loss=0.22918762266635895\n",
      "epoch 14 iter 35 loss=0.6980698704719543\n",
      "epoch 14 iter 36 loss=0.4723418056964874\n",
      "epoch 14 iter 37 loss=0.5045472383499146\n",
      "epoch 14 iter 38 loss=0.71646648645401\n",
      "epoch 14 iter 39 loss=0.5411610007286072\n",
      "epoch 14 iter 40 loss=0.44352617859840393\n",
      "epoch 14 iter 41 loss=0.318265825510025\n",
      "epoch 14 iter 42 loss=0.5868583917617798\n",
      "epoch 14 iter 43 loss=0.38445335626602173\n",
      "epoch 14 iter 44 loss=0.3135267496109009\n",
      "epoch 14 iter 45 loss=0.2460772544145584\n",
      "epoch 14 iter 46 loss=0.32118844985961914\n",
      "epoch 14 iter 47 loss=0.3687421977519989\n",
      "epoch 14 iter 48 loss=0.3238707184791565\n",
      "epoch 14 iter 49 loss=0.5212851762771606\n",
      "epoch 14 iter 50 loss=0.3560076951980591\n",
      "epoch 14 iter 51 loss=0.6034358739852905\n",
      "epoch 14 iter 52 loss=0.34825947880744934\n",
      "epoch 14 iter 53 loss=0.3492848873138428\n",
      "epoch 14 iter 54 loss=0.8616822361946106\n",
      "epoch 14 iter 55 loss=0.3913460075855255\n",
      "epoch 14 iter 56 loss=0.23824086785316467\n",
      "epoch 14 iter 57 loss=0.6221404671669006\n",
      "epoch 14 iter 58 loss=0.20178519189357758\n",
      "epoch 14 iter 59 loss=0.3372383117675781\n",
      "epoch 14 iter 60 loss=0.512226939201355\n",
      "epoch 14 iter 61 loss=0.5320591330528259\n",
      "epoch 14 iter 62 loss=0.3369911313056946\n",
      "epoch 14 iter 63 loss=0.45006564259529114\n",
      "epoch 14 iter 64 loss=0.5222692489624023\n",
      "epoch 14 iter 65 loss=0.43184518814086914\n",
      "epoch 14 iter 66 loss=1.0428829193115234\n",
      "epoch 14 iter 67 loss=0.24989978969097137\n",
      "epoch 14 iter 68 loss=0.605300784111023\n",
      "epoch 14 iter 69 loss=0.7037854194641113\n",
      "epoch 14 iter 70 loss=0.33120197057724\n",
      "epoch 14 iter 71 loss=0.8174319863319397\n",
      "epoch 14 iter 72 loss=0.4001104235649109\n",
      "epoch 14 iter 73 loss=0.38195645809173584\n",
      "epoch 14 iter 74 loss=0.4505672752857208\n",
      "epoch 15 iter 0 loss=0.39041459560394287\n",
      "epoch 15 iter 1 loss=0.6749957799911499\n",
      "epoch 15 iter 2 loss=0.3054012656211853\n",
      "epoch 15 iter 3 loss=0.36377769708633423\n",
      "epoch 15 iter 4 loss=0.27853381633758545\n",
      "epoch 15 iter 5 loss=0.2832968235015869\n",
      "epoch 15 iter 6 loss=0.23940803110599518\n",
      "epoch 15 iter 7 loss=0.634752094745636\n",
      "epoch 15 iter 8 loss=0.4816599190235138\n",
      "epoch 15 iter 9 loss=0.3171125650405884\n",
      "epoch 15 iter 10 loss=0.5819727182388306\n",
      "epoch 15 iter 11 loss=0.3029353618621826\n",
      "epoch 15 iter 12 loss=0.3218252658843994\n",
      "epoch 15 iter 13 loss=0.6457585096359253\n",
      "epoch 15 iter 14 loss=0.2932821214199066\n",
      "epoch 15 iter 15 loss=0.35437631607055664\n",
      "epoch 15 iter 16 loss=0.375290185213089\n",
      "epoch 15 iter 17 loss=0.38323232531547546\n",
      "epoch 15 iter 18 loss=0.46675822138786316\n",
      "epoch 15 iter 19 loss=0.4262126386165619\n",
      "epoch 15 iter 20 loss=0.47028571367263794\n",
      "epoch 15 iter 21 loss=0.4307035803794861\n",
      "epoch 15 iter 22 loss=0.467373788356781\n",
      "epoch 15 iter 23 loss=0.2465853989124298\n",
      "epoch 15 iter 24 loss=0.2656687796115875\n",
      "epoch 15 iter 25 loss=0.2955320179462433\n",
      "epoch 15 iter 26 loss=0.4417576491832733\n",
      "epoch 15 iter 27 loss=0.5310954451560974\n",
      "epoch 15 iter 28 loss=0.45504817366600037\n",
      "epoch 15 iter 29 loss=0.39563053846359253\n",
      "epoch 15 iter 30 loss=0.3140356242656708\n",
      "epoch 15 iter 31 loss=0.18179845809936523\n",
      "epoch 15 iter 32 loss=0.42595651745796204\n",
      "epoch 15 iter 33 loss=0.5984310507774353\n",
      "epoch 15 iter 34 loss=0.4131772220134735\n",
      "epoch 15 iter 35 loss=0.4413891136646271\n",
      "epoch 15 iter 36 loss=0.25216296315193176\n",
      "epoch 15 iter 37 loss=0.29218825697898865\n",
      "epoch 15 iter 38 loss=0.2709822356700897\n",
      "epoch 15 iter 39 loss=0.22586005926132202\n",
      "epoch 15 iter 40 loss=0.23873326182365417\n",
      "epoch 15 iter 41 loss=0.5184539556503296\n",
      "epoch 15 iter 42 loss=0.4708326756954193\n",
      "epoch 15 iter 43 loss=0.28143802285194397\n",
      "epoch 15 iter 44 loss=0.3558184504508972\n",
      "epoch 15 iter 45 loss=0.5785841941833496\n",
      "epoch 15 iter 46 loss=0.5111365914344788\n",
      "epoch 15 iter 47 loss=0.3136630058288574\n",
      "epoch 15 iter 48 loss=0.36328330636024475\n",
      "epoch 15 iter 49 loss=0.46611446142196655\n",
      "epoch 15 iter 50 loss=0.7215870022773743\n",
      "epoch 15 iter 51 loss=0.4409615397453308\n",
      "epoch 15 iter 52 loss=1.0021830797195435\n",
      "epoch 15 iter 53 loss=0.5153510570526123\n",
      "epoch 15 iter 54 loss=0.513780415058136\n",
      "epoch 15 iter 55 loss=0.7859019637107849\n",
      "epoch 15 iter 56 loss=0.48590707778930664\n",
      "epoch 15 iter 57 loss=0.3593740165233612\n",
      "epoch 15 iter 58 loss=0.590538501739502\n",
      "epoch 15 iter 59 loss=0.4883856475353241\n",
      "epoch 15 iter 60 loss=0.4129006862640381\n",
      "epoch 15 iter 61 loss=0.33452466130256653\n",
      "epoch 15 iter 62 loss=0.3316517174243927\n",
      "epoch 15 iter 63 loss=0.2627706825733185\n",
      "epoch 15 iter 64 loss=0.2104533612728119\n",
      "epoch 15 iter 65 loss=0.42883291840553284\n",
      "epoch 15 iter 66 loss=0.36043426394462585\n",
      "epoch 15 iter 67 loss=0.2689099609851837\n",
      "epoch 15 iter 68 loss=0.44571056962013245\n",
      "epoch 15 iter 69 loss=0.5461679100990295\n",
      "epoch 15 iter 70 loss=0.4507100582122803\n",
      "epoch 15 iter 71 loss=0.49819374084472656\n",
      "epoch 15 iter 72 loss=0.5639972686767578\n",
      "epoch 15 iter 73 loss=0.6344863772392273\n",
      "epoch 15 iter 74 loss=0.3605774939060211\n",
      "epoch 16 iter 0 loss=0.41817158460617065\n",
      "epoch 16 iter 1 loss=0.3922167420387268\n",
      "epoch 16 iter 2 loss=0.28893783688545227\n",
      "epoch 16 iter 3 loss=0.3022943139076233\n",
      "epoch 16 iter 4 loss=0.4801897704601288\n",
      "epoch 16 iter 5 loss=0.29022863507270813\n",
      "epoch 16 iter 6 loss=0.2987383306026459\n",
      "epoch 16 iter 7 loss=0.6095215082168579\n",
      "epoch 16 iter 8 loss=0.3404234051704407\n",
      "epoch 16 iter 9 loss=0.3119525909423828\n",
      "epoch 16 iter 10 loss=0.4863513112068176\n",
      "epoch 16 iter 11 loss=0.4397878646850586\n",
      "epoch 16 iter 12 loss=0.2525085210800171\n",
      "epoch 16 iter 13 loss=0.5172827839851379\n",
      "epoch 16 iter 14 loss=0.35359591245651245\n",
      "epoch 16 iter 15 loss=0.2700121998786926\n",
      "epoch 16 iter 16 loss=0.33192354440689087\n",
      "epoch 16 iter 17 loss=0.3912341892719269\n",
      "epoch 16 iter 18 loss=0.3525707423686981\n",
      "epoch 16 iter 19 loss=0.5977119207382202\n",
      "epoch 16 iter 20 loss=0.49177980422973633\n",
      "epoch 16 iter 21 loss=0.3835843801498413\n",
      "epoch 16 iter 22 loss=0.4662306606769562\n",
      "epoch 16 iter 23 loss=0.546349287033081\n",
      "epoch 16 iter 24 loss=0.4875093102455139\n",
      "epoch 16 iter 25 loss=0.24706631898880005\n",
      "epoch 16 iter 26 loss=0.5912120938301086\n",
      "epoch 16 iter 27 loss=0.37357866764068604\n",
      "epoch 16 iter 28 loss=0.5239762663841248\n",
      "epoch 16 iter 29 loss=0.317005455493927\n",
      "epoch 16 iter 30 loss=0.5208792686462402\n",
      "epoch 16 iter 31 loss=0.4750354588031769\n",
      "epoch 16 iter 32 loss=0.2600267231464386\n",
      "epoch 16 iter 33 loss=0.43566784262657166\n",
      "epoch 16 iter 34 loss=0.31543081998825073\n",
      "epoch 16 iter 35 loss=0.32332366704940796\n",
      "epoch 16 iter 36 loss=0.45882830023765564\n",
      "epoch 16 iter 37 loss=0.3585653305053711\n",
      "epoch 16 iter 38 loss=0.3334329128265381\n",
      "epoch 16 iter 39 loss=0.3009093701839447\n",
      "epoch 16 iter 40 loss=0.3433043956756592\n",
      "epoch 16 iter 41 loss=0.8774908781051636\n",
      "epoch 16 iter 42 loss=0.45933473110198975\n",
      "epoch 16 iter 43 loss=0.39966341853141785\n",
      "epoch 16 iter 44 loss=0.27289852499961853\n",
      "epoch 16 iter 45 loss=0.5600799322128296\n",
      "epoch 16 iter 46 loss=0.648070752620697\n",
      "epoch 16 iter 47 loss=0.5070423483848572\n",
      "epoch 16 iter 48 loss=0.3589809536933899\n",
      "epoch 16 iter 49 loss=0.4589850902557373\n",
      "epoch 16 iter 50 loss=0.4231460690498352\n",
      "epoch 16 iter 51 loss=0.2853929400444031\n",
      "epoch 16 iter 52 loss=0.6189025640487671\n",
      "epoch 16 iter 53 loss=0.47837164998054504\n",
      "epoch 16 iter 54 loss=0.6959860920906067\n",
      "epoch 16 iter 55 loss=0.28167685866355896\n",
      "epoch 16 iter 56 loss=0.4058162569999695\n",
      "epoch 16 iter 57 loss=0.4893956780433655\n",
      "epoch 16 iter 58 loss=0.39267170429229736\n",
      "epoch 16 iter 59 loss=0.2942775785923004\n",
      "epoch 16 iter 60 loss=0.41219475865364075\n",
      "epoch 16 iter 61 loss=0.39139264822006226\n",
      "epoch 16 iter 62 loss=0.47774073481559753\n",
      "epoch 16 iter 63 loss=0.31617552042007446\n",
      "epoch 16 iter 64 loss=0.3194354772567749\n",
      "epoch 16 iter 65 loss=0.4853654205799103\n",
      "epoch 16 iter 66 loss=0.3374025821685791\n",
      "epoch 16 iter 67 loss=0.35988733172416687\n",
      "epoch 16 iter 68 loss=0.2752181589603424\n",
      "epoch 16 iter 69 loss=0.31205078959465027\n",
      "epoch 16 iter 70 loss=0.25677359104156494\n",
      "epoch 16 iter 71 loss=0.41098541021347046\n",
      "epoch 16 iter 72 loss=0.5220385193824768\n",
      "epoch 16 iter 73 loss=0.2590143084526062\n",
      "epoch 16 iter 74 loss=0.22115685045719147\n",
      "epoch 17 iter 0 loss=0.30778956413269043\n",
      "epoch 17 iter 1 loss=0.332740843296051\n",
      "epoch 17 iter 2 loss=0.29422467947006226\n",
      "epoch 17 iter 3 loss=0.3250455856323242\n",
      "epoch 17 iter 4 loss=0.3097184896469116\n",
      "epoch 17 iter 5 loss=0.22799231112003326\n",
      "epoch 17 iter 6 loss=0.38152649998664856\n",
      "epoch 17 iter 7 loss=0.37563297152519226\n",
      "epoch 17 iter 8 loss=0.2320934236049652\n",
      "epoch 17 iter 9 loss=0.36457234621047974\n",
      "epoch 17 iter 10 loss=0.3394368290901184\n",
      "epoch 17 iter 11 loss=0.4748494029045105\n",
      "epoch 17 iter 12 loss=0.36118537187576294\n",
      "epoch 17 iter 13 loss=0.26359423995018005\n",
      "epoch 17 iter 14 loss=0.2598644196987152\n",
      "epoch 17 iter 15 loss=0.25749656558036804\n",
      "epoch 17 iter 16 loss=0.39571452140808105\n",
      "epoch 17 iter 17 loss=0.36913377046585083\n",
      "epoch 17 iter 18 loss=0.19969682395458221\n",
      "epoch 17 iter 19 loss=0.2976002097129822\n",
      "epoch 17 iter 20 loss=0.18174266815185547\n",
      "epoch 17 iter 21 loss=0.511620819568634\n",
      "epoch 17 iter 22 loss=0.33745473623275757\n",
      "epoch 17 iter 23 loss=0.3969447612762451\n",
      "epoch 17 iter 24 loss=0.3223775625228882\n",
      "epoch 17 iter 25 loss=0.5732344388961792\n",
      "epoch 17 iter 26 loss=0.19998633861541748\n",
      "epoch 17 iter 27 loss=0.5817759037017822\n",
      "epoch 17 iter 28 loss=0.3635562062263489\n",
      "epoch 17 iter 29 loss=0.28778088092803955\n",
      "epoch 17 iter 30 loss=0.37407663464546204\n",
      "epoch 17 iter 31 loss=0.4534684419631958\n",
      "epoch 17 iter 32 loss=0.2734517753124237\n",
      "epoch 17 iter 33 loss=0.24963949620723724\n",
      "epoch 17 iter 34 loss=0.49640753865242004\n",
      "epoch 17 iter 35 loss=0.5156386494636536\n",
      "epoch 17 iter 36 loss=0.8452239036560059\n",
      "epoch 17 iter 37 loss=1.291783094406128\n",
      "epoch 17 iter 38 loss=0.6960151195526123\n",
      "epoch 17 iter 39 loss=0.2577458322048187\n",
      "epoch 17 iter 40 loss=0.49191510677337646\n",
      "epoch 17 iter 41 loss=0.4496019184589386\n",
      "epoch 17 iter 42 loss=0.42562049627304077\n",
      "epoch 17 iter 43 loss=0.6155374646186829\n",
      "epoch 17 iter 44 loss=0.6179651021957397\n",
      "epoch 17 iter 45 loss=0.35668322443962097\n",
      "epoch 17 iter 46 loss=0.24684089422225952\n",
      "epoch 17 iter 47 loss=0.5583629608154297\n",
      "epoch 17 iter 48 loss=0.3746189475059509\n",
      "epoch 17 iter 49 loss=0.3472977578639984\n",
      "epoch 17 iter 50 loss=0.6411409378051758\n",
      "epoch 17 iter 51 loss=0.300167441368103\n",
      "epoch 17 iter 52 loss=0.41305193305015564\n",
      "epoch 17 iter 53 loss=0.31155186891555786\n",
      "epoch 17 iter 54 loss=0.8984413146972656\n",
      "epoch 17 iter 55 loss=0.43682101368904114\n",
      "epoch 17 iter 56 loss=0.23743648827075958\n",
      "epoch 17 iter 57 loss=0.30156412720680237\n",
      "epoch 17 iter 58 loss=0.2876671552658081\n",
      "epoch 17 iter 59 loss=0.34286633133888245\n",
      "epoch 17 iter 60 loss=0.5929737091064453\n",
      "epoch 17 iter 61 loss=0.4952894151210785\n",
      "epoch 17 iter 62 loss=0.4976361393928528\n",
      "epoch 17 iter 63 loss=0.47608983516693115\n",
      "epoch 17 iter 64 loss=0.5051246285438538\n",
      "epoch 17 iter 65 loss=0.392758309841156\n",
      "epoch 17 iter 66 loss=0.30675798654556274\n",
      "epoch 17 iter 67 loss=0.365996778011322\n",
      "epoch 17 iter 68 loss=0.7489123344421387\n",
      "epoch 17 iter 69 loss=0.7932486534118652\n",
      "epoch 17 iter 70 loss=0.43822601437568665\n",
      "epoch 17 iter 71 loss=0.2579519748687744\n",
      "epoch 17 iter 72 loss=0.3754802644252777\n",
      "epoch 17 iter 73 loss=0.35184744000434875\n",
      "epoch 17 iter 74 loss=0.349188894033432\n",
      "epoch 18 iter 0 loss=0.18433475494384766\n",
      "epoch 18 iter 1 loss=0.5370177030563354\n",
      "epoch 18 iter 2 loss=0.35394611954689026\n",
      "epoch 18 iter 3 loss=0.48975294828414917\n",
      "epoch 18 iter 4 loss=0.36402174830436707\n",
      "epoch 18 iter 5 loss=0.2872227430343628\n",
      "epoch 18 iter 6 loss=0.3798339366912842\n",
      "epoch 18 iter 7 loss=0.3788137137889862\n",
      "epoch 18 iter 8 loss=0.4209599196910858\n",
      "epoch 18 iter 9 loss=0.40144822001457214\n",
      "epoch 18 iter 10 loss=0.4366733133792877\n",
      "epoch 18 iter 11 loss=0.4382250905036926\n",
      "epoch 18 iter 12 loss=0.234613835811615\n",
      "epoch 18 iter 13 loss=0.4338376820087433\n",
      "epoch 18 iter 14 loss=0.49697789549827576\n",
      "epoch 18 iter 15 loss=0.5809787511825562\n",
      "epoch 18 iter 16 loss=0.23699533939361572\n",
      "epoch 18 iter 17 loss=0.4752116799354553\n",
      "epoch 18 iter 18 loss=0.3747144639492035\n",
      "epoch 18 iter 19 loss=0.31520137190818787\n",
      "epoch 18 iter 20 loss=0.3138335645198822\n",
      "epoch 18 iter 21 loss=0.30617082118988037\n",
      "epoch 18 iter 22 loss=0.444132536649704\n",
      "epoch 18 iter 23 loss=0.6764417290687561\n",
      "epoch 18 iter 24 loss=0.3720360994338989\n",
      "epoch 18 iter 25 loss=0.4145086109638214\n",
      "epoch 18 iter 26 loss=0.21800611913204193\n",
      "epoch 18 iter 27 loss=0.27018848061561584\n",
      "epoch 18 iter 28 loss=0.36881548166275024\n",
      "epoch 18 iter 29 loss=0.31083473563194275\n",
      "epoch 18 iter 30 loss=0.4105765223503113\n",
      "epoch 18 iter 31 loss=0.3427281677722931\n",
      "epoch 18 iter 32 loss=0.3699193596839905\n",
      "epoch 18 iter 33 loss=0.43259474635124207\n",
      "epoch 18 iter 34 loss=0.26784053444862366\n",
      "epoch 18 iter 35 loss=0.22943973541259766\n",
      "epoch 18 iter 36 loss=0.2774733304977417\n",
      "epoch 18 iter 37 loss=0.6596143245697021\n",
      "epoch 18 iter 38 loss=0.54438716173172\n",
      "epoch 18 iter 39 loss=0.2813494801521301\n",
      "epoch 18 iter 40 loss=0.3186035752296448\n",
      "epoch 18 iter 41 loss=0.20239566266536713\n",
      "epoch 18 iter 42 loss=0.26404285430908203\n",
      "epoch 18 iter 43 loss=0.46381908655166626\n",
      "epoch 18 iter 44 loss=0.6131812334060669\n",
      "epoch 18 iter 45 loss=0.48122236132621765\n",
      "epoch 18 iter 46 loss=0.28970104455947876\n",
      "epoch 18 iter 47 loss=0.3916618227958679\n",
      "epoch 18 iter 48 loss=0.4909301996231079\n",
      "epoch 18 iter 49 loss=0.2990306317806244\n",
      "epoch 18 iter 50 loss=0.614914059638977\n",
      "epoch 18 iter 51 loss=0.5327234268188477\n",
      "epoch 18 iter 52 loss=0.30000585317611694\n",
      "epoch 18 iter 53 loss=0.23753274977207184\n",
      "epoch 18 iter 54 loss=0.4073294401168823\n",
      "epoch 18 iter 55 loss=0.2941133379936218\n",
      "epoch 18 iter 56 loss=0.2453344464302063\n",
      "epoch 18 iter 57 loss=0.44309765100479126\n",
      "epoch 18 iter 58 loss=0.23840118944644928\n",
      "epoch 18 iter 59 loss=0.48007041215896606\n",
      "epoch 18 iter 60 loss=0.24839191138744354\n",
      "epoch 18 iter 61 loss=0.4699763059616089\n",
      "epoch 18 iter 62 loss=0.4488321542739868\n",
      "epoch 18 iter 63 loss=0.2937682271003723\n",
      "epoch 18 iter 64 loss=0.2972836196422577\n",
      "epoch 18 iter 65 loss=0.45352646708488464\n",
      "epoch 18 iter 66 loss=0.18866591155529022\n",
      "epoch 18 iter 67 loss=0.48386290669441223\n",
      "epoch 18 iter 68 loss=0.5005626082420349\n",
      "epoch 18 iter 69 loss=0.5193241834640503\n",
      "epoch 18 iter 70 loss=0.2758045792579651\n",
      "epoch 18 iter 71 loss=0.3298613131046295\n",
      "epoch 18 iter 72 loss=0.238973930478096\n",
      "epoch 18 iter 73 loss=0.2316174954175949\n",
      "epoch 18 iter 74 loss=0.6763444542884827\n",
      "epoch 19 iter 0 loss=0.4536225497722626\n",
      "epoch 19 iter 1 loss=0.2481336146593094\n",
      "epoch 19 iter 2 loss=0.5973998308181763\n",
      "epoch 19 iter 3 loss=0.45265883207321167\n",
      "epoch 19 iter 4 loss=0.4291616976261139\n",
      "epoch 19 iter 5 loss=0.2907954752445221\n",
      "epoch 19 iter 6 loss=0.6183272004127502\n",
      "epoch 19 iter 7 loss=0.26495105028152466\n",
      "epoch 19 iter 8 loss=0.35844728350639343\n",
      "epoch 19 iter 9 loss=0.5182329416275024\n",
      "epoch 19 iter 10 loss=0.2756774127483368\n",
      "epoch 19 iter 11 loss=0.26114535331726074\n",
      "epoch 19 iter 12 loss=0.5003299713134766\n",
      "epoch 19 iter 13 loss=0.4228771924972534\n",
      "epoch 19 iter 14 loss=0.3337271213531494\n",
      "epoch 19 iter 15 loss=0.27991753816604614\n",
      "epoch 19 iter 16 loss=0.4522418677806854\n",
      "epoch 19 iter 17 loss=0.19961248338222504\n",
      "epoch 19 iter 18 loss=0.23444047570228577\n",
      "epoch 19 iter 19 loss=0.48402756452560425\n",
      "epoch 19 iter 20 loss=0.8487011790275574\n",
      "epoch 19 iter 21 loss=0.5482314229011536\n",
      "epoch 19 iter 22 loss=0.36428993940353394\n",
      "epoch 19 iter 23 loss=0.38164034485816956\n",
      "epoch 19 iter 24 loss=0.3430979251861572\n",
      "epoch 19 iter 25 loss=0.31967243552207947\n",
      "epoch 19 iter 26 loss=0.29948940873146057\n",
      "epoch 19 iter 27 loss=0.3128531873226166\n",
      "epoch 19 iter 28 loss=0.3404810428619385\n",
      "epoch 19 iter 29 loss=0.3686888813972473\n",
      "epoch 19 iter 30 loss=0.20567765831947327\n",
      "epoch 19 iter 31 loss=0.24359983205795288\n",
      "epoch 19 iter 32 loss=0.2757858633995056\n",
      "epoch 19 iter 33 loss=0.3968838155269623\n",
      "epoch 19 iter 34 loss=0.9158094525337219\n",
      "epoch 19 iter 35 loss=0.6214957237243652\n",
      "epoch 19 iter 36 loss=0.35797685384750366\n",
      "epoch 19 iter 37 loss=0.19641922414302826\n",
      "epoch 19 iter 38 loss=0.508886992931366\n",
      "epoch 19 iter 39 loss=0.6294615268707275\n",
      "epoch 19 iter 40 loss=0.621188759803772\n",
      "epoch 19 iter 41 loss=0.35639503598213196\n",
      "epoch 19 iter 42 loss=0.26519879698753357\n",
      "epoch 19 iter 43 loss=0.4311264157295227\n",
      "epoch 19 iter 44 loss=0.4199198782444\n",
      "epoch 19 iter 45 loss=0.37385156750679016\n",
      "epoch 19 iter 46 loss=0.6182295680046082\n",
      "epoch 19 iter 47 loss=0.231521338224411\n",
      "epoch 19 iter 48 loss=0.35884034633636475\n",
      "epoch 19 iter 49 loss=0.254344642162323\n",
      "epoch 19 iter 50 loss=0.40723562240600586\n",
      "epoch 19 iter 51 loss=0.5895626544952393\n",
      "epoch 19 iter 52 loss=0.29757070541381836\n",
      "epoch 19 iter 53 loss=0.4024454951286316\n",
      "epoch 19 iter 54 loss=0.28114596009254456\n",
      "epoch 19 iter 55 loss=0.38757529854774475\n",
      "epoch 19 iter 56 loss=0.19885623455047607\n",
      "epoch 19 iter 57 loss=0.46537816524505615\n",
      "epoch 19 iter 58 loss=0.4371086657047272\n",
      "epoch 19 iter 59 loss=0.254172682762146\n",
      "epoch 19 iter 60 loss=0.24344228208065033\n",
      "epoch 19 iter 61 loss=0.4201548993587494\n",
      "epoch 19 iter 62 loss=0.5253661274909973\n",
      "epoch 19 iter 63 loss=0.2956809997558594\n",
      "epoch 19 iter 64 loss=0.3244796097278595\n",
      "epoch 19 iter 65 loss=0.25200241804122925\n",
      "epoch 19 iter 66 loss=0.3852401077747345\n",
      "epoch 19 iter 67 loss=0.15360578894615173\n",
      "epoch 19 iter 68 loss=0.37971732020378113\n",
      "epoch 19 iter 69 loss=0.28777745366096497\n",
      "epoch 19 iter 70 loss=0.20748195052146912\n",
      "epoch 19 iter 71 loss=0.2935035824775696\n",
      "epoch 19 iter 72 loss=0.3991791307926178\n",
      "epoch 19 iter 73 loss=0.4467969536781311\n",
      "epoch 19 iter 74 loss=0.6976310014724731\n",
      "epoch 20 iter 0 loss=0.7384399175643921\n",
      "epoch 20 iter 1 loss=0.43069133162498474\n",
      "epoch 20 iter 2 loss=0.24282915890216827\n",
      "epoch 20 iter 3 loss=0.29190120100975037\n",
      "epoch 20 iter 4 loss=0.37815725803375244\n",
      "epoch 20 iter 5 loss=0.33127132058143616\n",
      "epoch 20 iter 6 loss=0.7408438920974731\n",
      "epoch 20 iter 7 loss=0.5448325872421265\n",
      "epoch 20 iter 8 loss=0.4808819890022278\n",
      "epoch 20 iter 9 loss=0.27567821741104126\n",
      "epoch 20 iter 10 loss=0.2416262924671173\n",
      "epoch 20 iter 11 loss=0.37213629484176636\n",
      "epoch 20 iter 12 loss=0.2334720343351364\n",
      "epoch 20 iter 13 loss=0.25717830657958984\n",
      "epoch 20 iter 14 loss=0.5553718209266663\n",
      "epoch 20 iter 15 loss=0.309346467256546\n",
      "epoch 20 iter 16 loss=0.3351948857307434\n",
      "epoch 20 iter 17 loss=0.3386126458644867\n",
      "epoch 20 iter 18 loss=0.3778226673603058\n",
      "epoch 20 iter 19 loss=0.2613428235054016\n",
      "epoch 20 iter 20 loss=0.428243488073349\n",
      "epoch 20 iter 21 loss=0.7243545651435852\n",
      "epoch 20 iter 22 loss=0.3397534191608429\n",
      "epoch 20 iter 23 loss=0.38571077585220337\n",
      "epoch 20 iter 24 loss=0.3094410300254822\n",
      "epoch 20 iter 25 loss=0.43911370635032654\n",
      "epoch 20 iter 26 loss=0.3801577091217041\n",
      "epoch 20 iter 27 loss=0.25920161604881287\n",
      "epoch 20 iter 28 loss=0.4690289795398712\n",
      "epoch 20 iter 29 loss=0.33976614475250244\n",
      "epoch 20 iter 30 loss=0.312497615814209\n",
      "epoch 20 iter 31 loss=0.31964191794395447\n",
      "epoch 20 iter 32 loss=0.23467043042182922\n",
      "epoch 20 iter 33 loss=0.2985387444496155\n",
      "epoch 20 iter 34 loss=0.5523825883865356\n",
      "epoch 20 iter 35 loss=0.2911374866962433\n",
      "epoch 20 iter 36 loss=0.22977280616760254\n",
      "epoch 20 iter 37 loss=0.32487553358078003\n",
      "epoch 20 iter 38 loss=0.21460388600826263\n",
      "epoch 20 iter 39 loss=0.7064228653907776\n",
      "epoch 20 iter 40 loss=0.3408357799053192\n",
      "epoch 20 iter 41 loss=0.49059203267097473\n",
      "epoch 20 iter 42 loss=0.2618580162525177\n",
      "epoch 20 iter 43 loss=0.30350977182388306\n",
      "epoch 20 iter 44 loss=0.21935029327869415\n",
      "epoch 20 iter 45 loss=0.29908081889152527\n",
      "epoch 20 iter 46 loss=0.458993524312973\n",
      "epoch 20 iter 47 loss=0.39278650283813477\n",
      "epoch 20 iter 48 loss=0.46996599435806274\n",
      "epoch 20 iter 49 loss=0.44340431690216064\n",
      "epoch 20 iter 50 loss=0.6045517921447754\n",
      "epoch 20 iter 51 loss=0.27407774329185486\n",
      "epoch 20 iter 52 loss=0.6259751319885254\n",
      "epoch 20 iter 53 loss=0.47010284662246704\n",
      "epoch 20 iter 54 loss=0.29224076867103577\n",
      "epoch 20 iter 55 loss=0.39922794699668884\n",
      "epoch 20 iter 56 loss=0.2956886887550354\n",
      "epoch 20 iter 57 loss=0.2149086892604828\n",
      "epoch 20 iter 58 loss=0.2850606441497803\n",
      "epoch 20 iter 59 loss=0.30180177092552185\n",
      "epoch 20 iter 60 loss=0.24792510271072388\n",
      "epoch 20 iter 61 loss=0.27319493889808655\n",
      "epoch 20 iter 62 loss=0.27346813678741455\n",
      "epoch 20 iter 63 loss=0.243259996175766\n",
      "epoch 20 iter 64 loss=0.26517993211746216\n",
      "epoch 20 iter 65 loss=0.4001370072364807\n",
      "epoch 20 iter 66 loss=0.316963791847229\n",
      "epoch 20 iter 67 loss=0.429733008146286\n",
      "epoch 20 iter 68 loss=0.2373911589384079\n",
      "epoch 20 iter 69 loss=0.3331862688064575\n",
      "epoch 20 iter 70 loss=0.38241228461265564\n",
      "epoch 20 iter 71 loss=0.25925424695014954\n",
      "epoch 20 iter 72 loss=0.2793886065483093\n",
      "epoch 20 iter 73 loss=0.639053225517273\n",
      "epoch 20 iter 74 loss=0.3458842635154724\n",
      "epoch 21 iter 0 loss=0.4330785274505615\n",
      "epoch 21 iter 1 loss=0.6199596524238586\n",
      "epoch 21 iter 2 loss=0.2966064512729645\n",
      "epoch 21 iter 3 loss=0.31109464168548584\n",
      "epoch 21 iter 4 loss=0.3197448253631592\n",
      "epoch 21 iter 5 loss=0.2720576226711273\n",
      "epoch 21 iter 6 loss=0.5182169675827026\n",
      "epoch 21 iter 7 loss=0.2691345512866974\n",
      "epoch 21 iter 8 loss=0.39095792174339294\n",
      "epoch 21 iter 9 loss=0.19464977085590363\n",
      "epoch 21 iter 10 loss=0.3072466552257538\n",
      "epoch 21 iter 11 loss=0.4100058972835541\n",
      "epoch 21 iter 12 loss=0.6411650776863098\n",
      "epoch 21 iter 13 loss=0.4297599494457245\n",
      "epoch 21 iter 14 loss=0.3190721869468689\n",
      "epoch 21 iter 15 loss=0.22079090774059296\n",
      "epoch 21 iter 16 loss=0.335341215133667\n",
      "epoch 21 iter 17 loss=0.25966596603393555\n",
      "epoch 21 iter 18 loss=0.2943240702152252\n",
      "epoch 21 iter 19 loss=0.3247835040092468\n",
      "epoch 21 iter 20 loss=0.27232691645622253\n",
      "epoch 21 iter 21 loss=0.3702995181083679\n",
      "epoch 21 iter 22 loss=0.3781951665878296\n",
      "epoch 21 iter 23 loss=0.24980007112026215\n",
      "epoch 21 iter 24 loss=0.22227247059345245\n",
      "epoch 21 iter 25 loss=0.31873998045921326\n",
      "epoch 21 iter 26 loss=0.39374080300331116\n",
      "epoch 21 iter 27 loss=0.3754982650279999\n",
      "epoch 21 iter 28 loss=0.23429907858371735\n",
      "epoch 21 iter 29 loss=0.36458003520965576\n",
      "epoch 21 iter 30 loss=0.44617941975593567\n",
      "epoch 21 iter 31 loss=0.25482434034347534\n",
      "epoch 21 iter 32 loss=0.215028315782547\n",
      "epoch 21 iter 33 loss=0.24629245698451996\n",
      "epoch 21 iter 34 loss=0.4178292155265808\n",
      "epoch 21 iter 35 loss=0.3859063982963562\n",
      "epoch 21 iter 36 loss=0.27874308824539185\n",
      "epoch 21 iter 37 loss=0.30900758504867554\n",
      "epoch 21 iter 38 loss=0.2381502240896225\n",
      "epoch 21 iter 39 loss=0.31439730525016785\n",
      "epoch 21 iter 40 loss=0.3577735722064972\n",
      "epoch 21 iter 41 loss=0.24700769782066345\n",
      "epoch 21 iter 42 loss=0.14783766865730286\n",
      "epoch 21 iter 43 loss=0.2710244357585907\n",
      "epoch 21 iter 44 loss=0.30064257979393005\n",
      "epoch 21 iter 45 loss=0.6960179209709167\n",
      "epoch 21 iter 46 loss=0.2644530236721039\n",
      "epoch 21 iter 47 loss=0.5150225162506104\n",
      "epoch 21 iter 48 loss=0.4321729838848114\n",
      "epoch 21 iter 49 loss=0.6565317511558533\n",
      "epoch 21 iter 50 loss=0.3534863591194153\n",
      "epoch 21 iter 51 loss=0.2016465961933136\n",
      "epoch 21 iter 52 loss=0.31443172693252563\n",
      "epoch 21 iter 53 loss=0.3437633514404297\n",
      "epoch 21 iter 54 loss=0.3286304473876953\n",
      "epoch 21 iter 55 loss=0.370126873254776\n",
      "epoch 21 iter 56 loss=0.4028710126876831\n",
      "epoch 21 iter 57 loss=0.327990859746933\n",
      "epoch 21 iter 58 loss=0.22826571762561798\n",
      "epoch 21 iter 59 loss=0.412990003824234\n",
      "epoch 21 iter 60 loss=0.26536455750465393\n",
      "epoch 21 iter 61 loss=0.3209282159805298\n",
      "epoch 21 iter 62 loss=0.3207069933414459\n",
      "epoch 21 iter 63 loss=0.3296423554420471\n",
      "epoch 21 iter 64 loss=0.3646182715892792\n",
      "epoch 21 iter 65 loss=0.1681244820356369\n",
      "epoch 21 iter 66 loss=0.46402379870414734\n",
      "epoch 21 iter 67 loss=0.28246116638183594\n",
      "epoch 21 iter 68 loss=0.328148752450943\n",
      "epoch 21 iter 69 loss=0.6683601140975952\n",
      "epoch 21 iter 70 loss=0.4528074264526367\n",
      "epoch 21 iter 71 loss=0.267210453748703\n",
      "epoch 21 iter 72 loss=0.308788001537323\n",
      "epoch 21 iter 73 loss=0.4050605893135071\n",
      "epoch 21 iter 74 loss=0.4038068354129791\n",
      "epoch 22 iter 0 loss=0.28233206272125244\n",
      "epoch 22 iter 1 loss=0.3456204831600189\n",
      "epoch 22 iter 2 loss=0.2713850140571594\n",
      "epoch 22 iter 3 loss=0.23637980222702026\n",
      "epoch 22 iter 4 loss=0.7042127251625061\n",
      "epoch 22 iter 5 loss=0.7669406533241272\n",
      "epoch 22 iter 6 loss=0.25005048513412476\n",
      "epoch 22 iter 7 loss=0.25908294320106506\n",
      "epoch 22 iter 8 loss=0.9088239669799805\n",
      "epoch 22 iter 9 loss=0.1903849095106125\n",
      "epoch 22 iter 10 loss=0.24786578118801117\n",
      "epoch 22 iter 11 loss=0.41695064306259155\n",
      "epoch 22 iter 12 loss=0.3004639148712158\n",
      "epoch 22 iter 13 loss=0.6049579381942749\n",
      "epoch 22 iter 14 loss=0.4175103008747101\n",
      "epoch 22 iter 15 loss=0.28598785400390625\n",
      "epoch 22 iter 16 loss=0.24480882287025452\n",
      "epoch 22 iter 17 loss=0.3840871751308441\n",
      "epoch 22 iter 18 loss=0.3265617787837982\n",
      "epoch 22 iter 19 loss=0.4022679924964905\n",
      "epoch 22 iter 20 loss=0.30920225381851196\n",
      "epoch 22 iter 21 loss=0.18628662824630737\n",
      "epoch 22 iter 22 loss=0.5205934643745422\n",
      "epoch 22 iter 23 loss=0.3016660511493683\n",
      "epoch 22 iter 24 loss=0.4020870327949524\n",
      "epoch 22 iter 25 loss=0.3195827305316925\n",
      "epoch 22 iter 26 loss=0.27132177352905273\n",
      "epoch 22 iter 27 loss=0.2354908138513565\n",
      "epoch 22 iter 28 loss=0.34057503938674927\n",
      "epoch 22 iter 29 loss=0.2637093663215637\n",
      "epoch 22 iter 30 loss=0.2445443868637085\n",
      "epoch 22 iter 31 loss=0.21777810156345367\n",
      "epoch 22 iter 32 loss=0.23173289000988007\n",
      "epoch 22 iter 33 loss=0.34539464116096497\n",
      "epoch 22 iter 34 loss=0.41851916909217834\n",
      "epoch 22 iter 35 loss=0.1903700828552246\n",
      "epoch 22 iter 36 loss=0.6132321953773499\n",
      "epoch 22 iter 37 loss=0.44817808270454407\n",
      "epoch 22 iter 38 loss=0.4152350127696991\n",
      "epoch 22 iter 39 loss=0.308874249458313\n",
      "epoch 22 iter 40 loss=0.34211140871047974\n",
      "epoch 22 iter 41 loss=0.35552364587783813\n",
      "epoch 22 iter 42 loss=0.2517508268356323\n",
      "epoch 22 iter 43 loss=0.20508821308612823\n",
      "epoch 22 iter 44 loss=0.2222989946603775\n",
      "epoch 22 iter 45 loss=0.19651341438293457\n",
      "epoch 22 iter 46 loss=0.39216867089271545\n",
      "epoch 22 iter 47 loss=0.39381033182144165\n",
      "epoch 22 iter 48 loss=0.30694952607154846\n",
      "epoch 22 iter 49 loss=0.27814823389053345\n",
      "epoch 22 iter 50 loss=0.6339007616043091\n",
      "epoch 22 iter 51 loss=0.27861759066581726\n",
      "epoch 22 iter 52 loss=0.38876354694366455\n",
      "epoch 22 iter 53 loss=0.3036220073699951\n",
      "epoch 22 iter 54 loss=0.6146778464317322\n",
      "epoch 22 iter 55 loss=0.47546759247779846\n",
      "epoch 22 iter 56 loss=0.282311350107193\n",
      "epoch 22 iter 57 loss=0.26572856307029724\n",
      "epoch 22 iter 58 loss=0.33935850858688354\n",
      "epoch 22 iter 59 loss=0.32993465662002563\n",
      "epoch 22 iter 60 loss=0.29967978596687317\n",
      "epoch 22 iter 61 loss=0.2559606730937958\n",
      "epoch 22 iter 62 loss=0.4034426212310791\n",
      "epoch 22 iter 63 loss=0.5654058456420898\n",
      "epoch 22 iter 64 loss=0.22024010121822357\n",
      "epoch 22 iter 65 loss=0.2230341136455536\n",
      "epoch 22 iter 66 loss=0.29462990164756775\n",
      "epoch 22 iter 67 loss=0.5175987482070923\n",
      "epoch 22 iter 68 loss=0.4637104868888855\n",
      "epoch 22 iter 69 loss=0.3444652855396271\n",
      "epoch 22 iter 70 loss=0.28380289673805237\n",
      "epoch 22 iter 71 loss=0.2737168073654175\n",
      "epoch 22 iter 72 loss=0.3395271599292755\n",
      "epoch 22 iter 73 loss=0.3322305977344513\n",
      "epoch 22 iter 74 loss=0.23874318599700928\n",
      "epoch 23 iter 0 loss=0.32728299498558044\n",
      "epoch 23 iter 1 loss=0.32884082198143005\n",
      "epoch 23 iter 2 loss=0.20956574380397797\n",
      "epoch 23 iter 3 loss=0.5781453251838684\n",
      "epoch 23 iter 4 loss=0.2311205267906189\n",
      "epoch 23 iter 5 loss=0.1478520631790161\n",
      "epoch 23 iter 6 loss=0.1990310698747635\n",
      "epoch 23 iter 7 loss=0.4158168137073517\n",
      "epoch 23 iter 8 loss=0.3891295790672302\n",
      "epoch 23 iter 9 loss=0.27679282426834106\n",
      "epoch 23 iter 10 loss=0.33960703015327454\n",
      "epoch 23 iter 11 loss=0.27999427914619446\n",
      "epoch 23 iter 12 loss=0.3020566701889038\n",
      "epoch 23 iter 13 loss=0.18429096043109894\n",
      "epoch 23 iter 14 loss=0.2101680487394333\n",
      "epoch 23 iter 15 loss=0.19878138601779938\n",
      "epoch 23 iter 16 loss=0.2499699890613556\n",
      "epoch 23 iter 17 loss=0.3109748959541321\n",
      "epoch 23 iter 18 loss=0.23232373595237732\n",
      "epoch 23 iter 19 loss=0.2183382511138916\n",
      "epoch 23 iter 20 loss=0.2753258943557739\n",
      "epoch 23 iter 21 loss=0.37969347834587097\n",
      "epoch 23 iter 22 loss=0.3342764973640442\n",
      "epoch 23 iter 23 loss=0.3437052369117737\n",
      "epoch 23 iter 24 loss=0.31601303815841675\n",
      "epoch 23 iter 25 loss=0.3462384045124054\n",
      "epoch 23 iter 26 loss=0.22365263104438782\n",
      "epoch 23 iter 27 loss=0.2791077494621277\n",
      "epoch 23 iter 28 loss=0.1937980055809021\n",
      "epoch 23 iter 29 loss=0.3565162122249603\n",
      "epoch 23 iter 30 loss=0.490033894777298\n",
      "epoch 23 iter 31 loss=0.3377215266227722\n",
      "epoch 23 iter 32 loss=0.37373271584510803\n",
      "epoch 23 iter 33 loss=0.4196966886520386\n",
      "epoch 23 iter 34 loss=0.4312351644039154\n",
      "epoch 23 iter 35 loss=0.41785693168640137\n",
      "epoch 23 iter 36 loss=0.18777860701084137\n",
      "epoch 23 iter 37 loss=0.3366120159626007\n",
      "epoch 23 iter 38 loss=0.25550076365470886\n",
      "epoch 23 iter 39 loss=0.19096016883850098\n",
      "epoch 23 iter 40 loss=0.2603203356266022\n",
      "epoch 23 iter 41 loss=0.3179803788661957\n",
      "epoch 23 iter 42 loss=0.25436678528785706\n",
      "epoch 23 iter 43 loss=0.22374680638313293\n",
      "epoch 23 iter 44 loss=0.20524728298187256\n",
      "epoch 23 iter 45 loss=0.2558702826499939\n",
      "epoch 23 iter 46 loss=0.22870096564292908\n",
      "epoch 23 iter 47 loss=0.3237323760986328\n",
      "epoch 23 iter 48 loss=0.24969938397407532\n",
      "epoch 23 iter 49 loss=0.4013276696205139\n",
      "epoch 23 iter 50 loss=0.4032689929008484\n",
      "epoch 23 iter 51 loss=0.5138729214668274\n",
      "epoch 23 iter 52 loss=0.21730056405067444\n",
      "epoch 23 iter 53 loss=0.2186979055404663\n",
      "epoch 23 iter 54 loss=0.21810664236545563\n",
      "epoch 23 iter 55 loss=0.6101049780845642\n",
      "epoch 23 iter 56 loss=0.28615644574165344\n",
      "epoch 23 iter 57 loss=0.34851229190826416\n",
      "epoch 23 iter 58 loss=0.2951166033744812\n",
      "epoch 23 iter 59 loss=0.20346802473068237\n",
      "epoch 23 iter 60 loss=0.23786665499210358\n",
      "epoch 23 iter 61 loss=0.25411149859428406\n",
      "epoch 23 iter 62 loss=0.2806705832481384\n",
      "epoch 23 iter 63 loss=0.6309455633163452\n",
      "epoch 23 iter 64 loss=0.28025415539741516\n",
      "epoch 23 iter 65 loss=0.20503152906894684\n",
      "epoch 23 iter 66 loss=0.25268733501434326\n",
      "epoch 23 iter 67 loss=0.20276755094528198\n",
      "epoch 23 iter 68 loss=0.17682234942913055\n",
      "epoch 23 iter 69 loss=0.5048118829727173\n",
      "epoch 23 iter 70 loss=0.22073353826999664\n",
      "epoch 23 iter 71 loss=0.36908969283103943\n",
      "epoch 23 iter 72 loss=0.22036084532737732\n",
      "epoch 23 iter 73 loss=0.2843010723590851\n",
      "epoch 23 iter 74 loss=0.231556698679924\n",
      "epoch 24 iter 0 loss=0.2562462091445923\n",
      "epoch 24 iter 1 loss=0.3397774398326874\n",
      "epoch 24 iter 2 loss=0.23019568622112274\n",
      "epoch 24 iter 3 loss=0.4193801283836365\n",
      "epoch 24 iter 4 loss=0.19760267436504364\n",
      "epoch 24 iter 5 loss=0.28865373134613037\n",
      "epoch 24 iter 6 loss=0.2193165272474289\n",
      "epoch 24 iter 7 loss=0.4905225932598114\n",
      "epoch 24 iter 8 loss=0.25144001841545105\n",
      "epoch 24 iter 9 loss=0.21138587594032288\n",
      "epoch 24 iter 10 loss=0.2961187958717346\n",
      "epoch 24 iter 11 loss=0.23463691771030426\n",
      "epoch 24 iter 12 loss=0.3438972234725952\n",
      "epoch 24 iter 13 loss=0.22381065785884857\n",
      "epoch 24 iter 14 loss=0.2631552815437317\n",
      "epoch 24 iter 15 loss=0.21513454616069794\n",
      "epoch 24 iter 16 loss=0.5464805364608765\n",
      "epoch 24 iter 17 loss=0.3217035233974457\n",
      "epoch 24 iter 18 loss=0.23491068184375763\n",
      "epoch 24 iter 19 loss=0.2699059844017029\n",
      "epoch 24 iter 20 loss=0.29453587532043457\n",
      "epoch 24 iter 21 loss=0.30612385272979736\n",
      "epoch 24 iter 22 loss=0.2931603193283081\n",
      "epoch 24 iter 23 loss=0.16877900063991547\n",
      "epoch 24 iter 24 loss=0.3765908181667328\n",
      "epoch 24 iter 25 loss=0.2857152223587036\n",
      "epoch 24 iter 26 loss=0.1674318164587021\n",
      "epoch 24 iter 27 loss=0.48340627551078796\n",
      "epoch 24 iter 28 loss=0.34870266914367676\n",
      "epoch 24 iter 29 loss=0.283133864402771\n",
      "epoch 24 iter 30 loss=0.1599375307559967\n",
      "epoch 24 iter 31 loss=0.18997052311897278\n",
      "epoch 24 iter 32 loss=0.3530512750148773\n",
      "epoch 24 iter 33 loss=0.33204200863838196\n",
      "epoch 24 iter 34 loss=0.26747065782546997\n",
      "epoch 24 iter 35 loss=0.20902016758918762\n",
      "epoch 24 iter 36 loss=0.47311872243881226\n",
      "epoch 24 iter 37 loss=0.24683167040348053\n",
      "epoch 24 iter 38 loss=0.2561255693435669\n",
      "epoch 24 iter 39 loss=0.2747974097728729\n",
      "epoch 24 iter 40 loss=0.16640514135360718\n",
      "epoch 24 iter 41 loss=0.5024493336677551\n",
      "epoch 24 iter 42 loss=0.370437353849411\n",
      "epoch 24 iter 43 loss=0.48884516954421997\n",
      "epoch 24 iter 44 loss=0.2447349727153778\n",
      "epoch 24 iter 45 loss=0.22066551446914673\n",
      "epoch 24 iter 46 loss=0.3947888910770416\n",
      "epoch 24 iter 47 loss=0.17471881210803986\n",
      "epoch 24 iter 48 loss=0.16685092449188232\n",
      "epoch 24 iter 49 loss=0.2156776785850525\n",
      "epoch 24 iter 50 loss=0.2029242068529129\n",
      "epoch 24 iter 51 loss=0.25940313935279846\n",
      "epoch 24 iter 52 loss=0.2922477424144745\n",
      "epoch 24 iter 53 loss=0.4632924497127533\n",
      "epoch 24 iter 54 loss=0.22452469170093536\n",
      "epoch 24 iter 55 loss=0.285666823387146\n",
      "epoch 24 iter 56 loss=0.36233991384506226\n",
      "epoch 24 iter 57 loss=0.15638034045696259\n",
      "epoch 24 iter 58 loss=0.4474223554134369\n",
      "epoch 24 iter 59 loss=0.32696929574012756\n",
      "epoch 24 iter 60 loss=0.37898340821266174\n",
      "epoch 24 iter 61 loss=0.33330419659614563\n",
      "epoch 24 iter 62 loss=0.2963285744190216\n",
      "epoch 24 iter 63 loss=0.24552306532859802\n",
      "epoch 24 iter 64 loss=0.22211256623268127\n",
      "epoch 24 iter 65 loss=0.1524229347705841\n",
      "epoch 24 iter 66 loss=0.23363655805587769\n",
      "epoch 24 iter 67 loss=0.3099648952484131\n",
      "epoch 24 iter 68 loss=0.3885720670223236\n",
      "epoch 24 iter 69 loss=0.35791489481925964\n",
      "epoch 24 iter 70 loss=0.25257933139801025\n",
      "epoch 24 iter 71 loss=0.20540489256381989\n",
      "epoch 24 iter 72 loss=0.5439583659172058\n",
      "epoch 24 iter 73 loss=0.23544739186763763\n",
      "epoch 24 iter 74 loss=0.25857582688331604\n",
      "epoch 25 iter 0 loss=0.11989322304725647\n",
      "epoch 25 iter 1 loss=0.186098113656044\n",
      "epoch 25 iter 2 loss=0.2858414351940155\n",
      "epoch 25 iter 3 loss=0.31236064434051514\n",
      "epoch 25 iter 4 loss=0.37702056765556335\n",
      "epoch 25 iter 5 loss=0.29862871766090393\n",
      "epoch 25 iter 6 loss=0.2754858732223511\n",
      "epoch 25 iter 7 loss=0.3687378466129303\n",
      "epoch 25 iter 8 loss=0.26834022998809814\n",
      "epoch 25 iter 9 loss=0.3478158414363861\n",
      "epoch 25 iter 10 loss=0.18796393275260925\n",
      "epoch 25 iter 11 loss=0.3975364863872528\n",
      "epoch 25 iter 12 loss=0.23693427443504333\n",
      "epoch 25 iter 13 loss=0.34282824397087097\n",
      "epoch 25 iter 14 loss=0.22025199234485626\n",
      "epoch 25 iter 15 loss=0.2319936752319336\n",
      "epoch 25 iter 16 loss=0.37644365429878235\n",
      "epoch 25 iter 17 loss=0.1846936047077179\n",
      "epoch 25 iter 18 loss=0.2231045514345169\n",
      "epoch 25 iter 19 loss=0.1938098520040512\n",
      "epoch 25 iter 20 loss=0.1743413656949997\n",
      "epoch 25 iter 21 loss=0.2975076735019684\n",
      "epoch 25 iter 22 loss=0.3606489300727844\n",
      "epoch 25 iter 23 loss=0.15653076767921448\n",
      "epoch 25 iter 24 loss=0.4163227081298828\n",
      "epoch 25 iter 25 loss=0.23398470878601074\n",
      "epoch 25 iter 26 loss=0.37426358461380005\n",
      "epoch 25 iter 27 loss=0.20012208819389343\n",
      "epoch 25 iter 28 loss=0.45898064970970154\n",
      "epoch 25 iter 29 loss=0.3402436077594757\n",
      "epoch 25 iter 30 loss=0.2704790234565735\n",
      "epoch 25 iter 31 loss=0.2675640285015106\n",
      "epoch 25 iter 32 loss=0.23009951412677765\n",
      "epoch 25 iter 33 loss=0.25574991106987\n",
      "epoch 25 iter 34 loss=0.37878096103668213\n",
      "epoch 25 iter 35 loss=0.26147615909576416\n",
      "epoch 25 iter 36 loss=0.18403571844100952\n",
      "epoch 25 iter 37 loss=0.3047395348548889\n",
      "epoch 25 iter 38 loss=0.20542360842227936\n",
      "epoch 25 iter 39 loss=0.4324265122413635\n",
      "epoch 25 iter 40 loss=0.2225269377231598\n",
      "epoch 25 iter 41 loss=0.6660547852516174\n",
      "epoch 25 iter 42 loss=0.46789196133613586\n",
      "epoch 25 iter 43 loss=0.3577914535999298\n",
      "epoch 25 iter 44 loss=0.31999364495277405\n",
      "epoch 25 iter 45 loss=0.3439646363258362\n",
      "epoch 25 iter 46 loss=0.5415536761283875\n",
      "epoch 25 iter 47 loss=0.32517382502555847\n",
      "epoch 25 iter 48 loss=0.40590691566467285\n",
      "epoch 25 iter 49 loss=0.4744233787059784\n",
      "epoch 25 iter 50 loss=0.20531845092773438\n",
      "epoch 25 iter 51 loss=0.4265856444835663\n",
      "epoch 25 iter 52 loss=0.286798894405365\n",
      "epoch 25 iter 53 loss=0.178085595369339\n",
      "epoch 25 iter 54 loss=0.3887101411819458\n",
      "epoch 25 iter 55 loss=0.2882538139820099\n",
      "epoch 25 iter 56 loss=0.32955414056777954\n",
      "epoch 25 iter 57 loss=0.31453225016593933\n",
      "epoch 25 iter 58 loss=0.23599642515182495\n",
      "epoch 25 iter 59 loss=0.31913846731185913\n",
      "epoch 25 iter 60 loss=0.2659197151660919\n",
      "epoch 25 iter 61 loss=0.23901590704917908\n",
      "epoch 25 iter 62 loss=0.2370176613330841\n",
      "epoch 25 iter 63 loss=0.40623506903648376\n",
      "epoch 25 iter 64 loss=0.19396179914474487\n",
      "epoch 25 iter 65 loss=0.37637758255004883\n",
      "epoch 25 iter 66 loss=0.2889297306537628\n",
      "epoch 25 iter 67 loss=0.2470352202653885\n",
      "epoch 25 iter 68 loss=0.2685352861881256\n",
      "epoch 25 iter 69 loss=0.33421817421913147\n",
      "epoch 25 iter 70 loss=0.4345322549343109\n",
      "epoch 25 iter 71 loss=0.17769375443458557\n",
      "epoch 25 iter 72 loss=0.21373750269412994\n",
      "epoch 25 iter 73 loss=0.19617068767547607\n",
      "epoch 25 iter 74 loss=0.2232981026172638\n",
      "epoch 26 iter 0 loss=0.294903963804245\n",
      "epoch 26 iter 1 loss=0.20420555770397186\n",
      "epoch 26 iter 2 loss=0.1908338963985443\n",
      "epoch 26 iter 3 loss=0.3115381896495819\n",
      "epoch 26 iter 4 loss=0.18771807849407196\n",
      "epoch 26 iter 5 loss=0.34036749601364136\n",
      "epoch 26 iter 6 loss=0.32991141080856323\n",
      "epoch 26 iter 7 loss=0.3500898778438568\n",
      "epoch 26 iter 8 loss=0.37117958068847656\n",
      "epoch 26 iter 9 loss=0.15647880733013153\n",
      "epoch 26 iter 10 loss=0.2135557383298874\n",
      "epoch 26 iter 11 loss=0.5242694616317749\n",
      "epoch 26 iter 12 loss=0.2773301601409912\n",
      "epoch 26 iter 13 loss=0.18293224275112152\n",
      "epoch 26 iter 14 loss=0.385988712310791\n",
      "epoch 26 iter 15 loss=0.33174633979797363\n",
      "epoch 26 iter 16 loss=0.16404996812343597\n",
      "epoch 26 iter 17 loss=0.41039472818374634\n",
      "epoch 26 iter 18 loss=0.2946516275405884\n",
      "epoch 26 iter 19 loss=0.23962228000164032\n",
      "epoch 26 iter 20 loss=0.2738722562789917\n",
      "epoch 26 iter 21 loss=0.3326059579849243\n",
      "epoch 26 iter 22 loss=0.26944923400878906\n",
      "epoch 26 iter 23 loss=0.35311537981033325\n",
      "epoch 26 iter 24 loss=0.26398178935050964\n",
      "epoch 26 iter 25 loss=0.4829785227775574\n",
      "epoch 26 iter 26 loss=0.34013059735298157\n",
      "epoch 26 iter 27 loss=0.25225964188575745\n",
      "epoch 26 iter 28 loss=0.1914958655834198\n",
      "epoch 26 iter 29 loss=0.16129983961582184\n",
      "epoch 26 iter 30 loss=0.4059162437915802\n",
      "epoch 26 iter 31 loss=0.283703088760376\n",
      "epoch 26 iter 32 loss=0.3136483430862427\n",
      "epoch 26 iter 33 loss=0.22621427476406097\n",
      "epoch 26 iter 34 loss=0.20054705440998077\n",
      "epoch 26 iter 35 loss=0.21517974138259888\n",
      "epoch 26 iter 36 loss=0.2600526511669159\n",
      "epoch 26 iter 37 loss=0.14346307516098022\n",
      "epoch 26 iter 38 loss=0.29058638215065\n",
      "epoch 26 iter 39 loss=0.28695112466812134\n",
      "epoch 26 iter 40 loss=0.22404734790325165\n",
      "epoch 26 iter 41 loss=0.2667446434497833\n",
      "epoch 26 iter 42 loss=0.14185833930969238\n",
      "epoch 26 iter 43 loss=0.23588202893733978\n",
      "epoch 26 iter 44 loss=0.49235016107559204\n",
      "epoch 26 iter 45 loss=0.26724326610565186\n",
      "epoch 26 iter 46 loss=0.17461860179901123\n",
      "epoch 26 iter 47 loss=0.22655805945396423\n",
      "epoch 26 iter 48 loss=0.2269478142261505\n",
      "epoch 26 iter 49 loss=0.25549647212028503\n",
      "epoch 26 iter 50 loss=0.47042667865753174\n",
      "epoch 26 iter 51 loss=0.38883525133132935\n",
      "epoch 26 iter 52 loss=0.22623448073863983\n",
      "epoch 26 iter 53 loss=0.19852688908576965\n",
      "epoch 26 iter 54 loss=0.2120962291955948\n",
      "epoch 26 iter 55 loss=0.28653082251548767\n",
      "epoch 26 iter 56 loss=0.35693982243537903\n",
      "epoch 26 iter 57 loss=0.34731483459472656\n",
      "epoch 26 iter 58 loss=0.2850722074508667\n",
      "epoch 26 iter 59 loss=0.1309370994567871\n",
      "epoch 26 iter 60 loss=0.2319735884666443\n",
      "epoch 26 iter 61 loss=0.1751403957605362\n",
      "epoch 26 iter 62 loss=0.19171787798404694\n",
      "epoch 26 iter 63 loss=0.40223249793052673\n",
      "epoch 26 iter 64 loss=0.32408392429351807\n",
      "epoch 26 iter 65 loss=0.2634640634059906\n",
      "epoch 26 iter 66 loss=0.2740657925605774\n",
      "epoch 26 iter 67 loss=0.2425486445426941\n",
      "epoch 26 iter 68 loss=0.545563817024231\n",
      "epoch 26 iter 69 loss=0.20448260009288788\n",
      "epoch 26 iter 70 loss=0.2533171772956848\n",
      "epoch 26 iter 71 loss=0.16370375454425812\n",
      "epoch 26 iter 72 loss=0.20878522098064423\n",
      "epoch 26 iter 73 loss=0.30336692929267883\n",
      "epoch 26 iter 74 loss=0.2461540251970291\n",
      "epoch 27 iter 0 loss=0.16642381250858307\n",
      "epoch 27 iter 1 loss=0.26802462339401245\n",
      "epoch 27 iter 2 loss=0.20215661823749542\n",
      "epoch 27 iter 3 loss=0.3917244076728821\n",
      "epoch 27 iter 4 loss=0.16121633350849152\n",
      "epoch 27 iter 5 loss=0.218621626496315\n",
      "epoch 27 iter 6 loss=0.18383465707302094\n",
      "epoch 27 iter 7 loss=0.13186758756637573\n",
      "epoch 27 iter 8 loss=0.4737890958786011\n",
      "epoch 27 iter 9 loss=0.23728735744953156\n",
      "epoch 27 iter 10 loss=0.22085721790790558\n",
      "epoch 27 iter 11 loss=0.4760252833366394\n",
      "epoch 27 iter 12 loss=0.2712614834308624\n",
      "epoch 27 iter 13 loss=0.35245296359062195\n",
      "epoch 27 iter 14 loss=0.3142595589160919\n",
      "epoch 27 iter 15 loss=0.24361087381839752\n",
      "epoch 27 iter 16 loss=0.193324014544487\n",
      "epoch 27 iter 17 loss=0.356577605009079\n",
      "epoch 27 iter 18 loss=0.17421013116836548\n",
      "epoch 27 iter 19 loss=0.1919173151254654\n",
      "epoch 27 iter 20 loss=0.15484805405139923\n",
      "epoch 27 iter 21 loss=0.42499831318855286\n",
      "epoch 27 iter 22 loss=0.2732033133506775\n",
      "epoch 27 iter 23 loss=0.15061385929584503\n",
      "epoch 27 iter 24 loss=0.2933627963066101\n",
      "epoch 27 iter 25 loss=0.23516692221164703\n",
      "epoch 27 iter 26 loss=0.27805638313293457\n",
      "epoch 27 iter 27 loss=0.24158550798892975\n",
      "epoch 27 iter 28 loss=0.16813743114471436\n",
      "epoch 27 iter 29 loss=0.16947706043720245\n",
      "epoch 27 iter 30 loss=0.2762647569179535\n",
      "epoch 27 iter 31 loss=0.19937385618686676\n",
      "epoch 27 iter 32 loss=0.40473705530166626\n",
      "epoch 27 iter 33 loss=0.1726309210062027\n",
      "epoch 27 iter 34 loss=0.32253938913345337\n",
      "epoch 27 iter 35 loss=0.2826479375362396\n",
      "epoch 27 iter 36 loss=0.19601556658744812\n",
      "epoch 27 iter 37 loss=0.24966657161712646\n",
      "epoch 27 iter 38 loss=0.40240246057510376\n",
      "epoch 27 iter 39 loss=0.26568925380706787\n",
      "epoch 27 iter 40 loss=0.3976963460445404\n",
      "epoch 27 iter 41 loss=0.1973574012517929\n",
      "epoch 27 iter 42 loss=0.16600766777992249\n",
      "epoch 27 iter 43 loss=0.256818950176239\n",
      "epoch 27 iter 44 loss=0.22142113745212555\n",
      "epoch 27 iter 45 loss=0.1482786387205124\n",
      "epoch 27 iter 46 loss=0.4400734603404999\n",
      "epoch 27 iter 47 loss=0.23753207921981812\n",
      "epoch 27 iter 48 loss=0.3873862326145172\n",
      "epoch 27 iter 49 loss=0.21550343930721283\n",
      "epoch 27 iter 50 loss=0.19015313684940338\n",
      "epoch 27 iter 51 loss=0.20133481919765472\n",
      "epoch 27 iter 52 loss=0.19916896522045135\n",
      "epoch 27 iter 53 loss=0.1829291582107544\n",
      "epoch 27 iter 54 loss=0.17465461790561676\n",
      "epoch 27 iter 55 loss=0.20528644323349\n",
      "epoch 27 iter 56 loss=0.48914432525634766\n",
      "epoch 27 iter 57 loss=0.758681058883667\n",
      "epoch 27 iter 58 loss=0.18403668701648712\n",
      "epoch 27 iter 59 loss=0.24100932478904724\n",
      "epoch 27 iter 60 loss=0.29322493076324463\n",
      "epoch 27 iter 61 loss=0.5378270745277405\n",
      "epoch 27 iter 62 loss=0.19256363809108734\n",
      "epoch 27 iter 63 loss=0.26944753527641296\n",
      "epoch 27 iter 64 loss=0.28727859258651733\n",
      "epoch 27 iter 65 loss=0.1707153022289276\n",
      "epoch 27 iter 66 loss=0.30268988013267517\n",
      "epoch 27 iter 67 loss=0.2635374069213867\n",
      "epoch 27 iter 68 loss=0.244866281747818\n",
      "epoch 27 iter 69 loss=0.29548564553260803\n",
      "epoch 27 iter 70 loss=0.3892608880996704\n",
      "epoch 27 iter 71 loss=0.26539233326911926\n",
      "epoch 27 iter 72 loss=0.2686826288700104\n",
      "epoch 27 iter 73 loss=0.26129329204559326\n",
      "epoch 27 iter 74 loss=0.5106318593025208\n",
      "epoch 28 iter 0 loss=0.23822742700576782\n",
      "epoch 28 iter 1 loss=0.22065003216266632\n",
      "epoch 28 iter 2 loss=0.24863030016422272\n",
      "epoch 28 iter 3 loss=0.3152676224708557\n",
      "epoch 28 iter 4 loss=0.21126501262187958\n",
      "epoch 28 iter 5 loss=0.4521428346633911\n",
      "epoch 28 iter 6 loss=0.3797461986541748\n",
      "epoch 28 iter 7 loss=0.3390243947505951\n",
      "epoch 28 iter 8 loss=0.13590167462825775\n",
      "epoch 28 iter 9 loss=0.1964675635099411\n",
      "epoch 28 iter 10 loss=0.22511231899261475\n",
      "epoch 28 iter 11 loss=0.4459969699382782\n",
      "epoch 28 iter 12 loss=0.24332620203495026\n",
      "epoch 28 iter 13 loss=0.2653641998767853\n",
      "epoch 28 iter 14 loss=0.18230348825454712\n",
      "epoch 28 iter 15 loss=0.1472831815481186\n",
      "epoch 28 iter 16 loss=0.458938330411911\n",
      "epoch 28 iter 17 loss=0.20137549936771393\n",
      "epoch 28 iter 18 loss=0.3369006812572479\n",
      "epoch 28 iter 19 loss=0.25920143723487854\n",
      "epoch 28 iter 20 loss=0.19207192957401276\n",
      "epoch 28 iter 21 loss=0.28675609827041626\n",
      "epoch 28 iter 22 loss=0.20575624704360962\n",
      "epoch 28 iter 23 loss=0.22395800054073334\n",
      "epoch 28 iter 24 loss=0.3622080981731415\n",
      "epoch 28 iter 25 loss=0.2173423171043396\n",
      "epoch 28 iter 26 loss=0.2013728767633438\n",
      "epoch 28 iter 27 loss=0.20204681158065796\n",
      "epoch 28 iter 28 loss=0.2075756937265396\n",
      "epoch 28 iter 29 loss=0.3112971782684326\n",
      "epoch 28 iter 30 loss=0.39690977334976196\n",
      "epoch 28 iter 31 loss=0.21649812161922455\n",
      "epoch 28 iter 32 loss=0.32135722041130066\n",
      "epoch 28 iter 33 loss=0.20739050209522247\n",
      "epoch 28 iter 34 loss=0.18717390298843384\n",
      "epoch 28 iter 35 loss=0.4101817011833191\n",
      "epoch 28 iter 36 loss=0.20129148662090302\n",
      "epoch 28 iter 37 loss=0.2400466650724411\n",
      "epoch 28 iter 38 loss=0.34202200174331665\n",
      "epoch 28 iter 39 loss=0.42348727583885193\n",
      "epoch 28 iter 40 loss=0.30575525760650635\n",
      "epoch 28 iter 41 loss=0.21264539659023285\n",
      "epoch 28 iter 42 loss=0.2674553394317627\n",
      "epoch 28 iter 43 loss=0.42938411235809326\n",
      "epoch 28 iter 44 loss=0.4077760577201843\n",
      "epoch 28 iter 45 loss=0.2462819367647171\n",
      "epoch 28 iter 46 loss=0.14112482964992523\n",
      "epoch 28 iter 47 loss=0.25984689593315125\n",
      "epoch 28 iter 48 loss=0.24326711893081665\n",
      "epoch 28 iter 49 loss=0.16901519894599915\n",
      "epoch 28 iter 50 loss=0.30656853318214417\n",
      "epoch 28 iter 51 loss=0.1774529069662094\n",
      "epoch 28 iter 52 loss=0.23696690797805786\n",
      "epoch 28 iter 53 loss=0.2290314882993698\n",
      "epoch 28 iter 54 loss=0.2250557839870453\n",
      "epoch 28 iter 55 loss=0.18086384236812592\n",
      "epoch 28 iter 56 loss=0.28744959831237793\n",
      "epoch 28 iter 57 loss=0.2559489607810974\n",
      "epoch 28 iter 58 loss=0.19941827654838562\n",
      "epoch 28 iter 59 loss=0.4234394431114197\n",
      "epoch 28 iter 60 loss=0.29645973443984985\n",
      "epoch 28 iter 61 loss=0.22910867631435394\n",
      "epoch 28 iter 62 loss=0.16694028675556183\n",
      "epoch 28 iter 63 loss=0.13779838383197784\n",
      "epoch 28 iter 64 loss=0.21636566519737244\n",
      "epoch 28 iter 65 loss=0.21966835856437683\n",
      "epoch 28 iter 66 loss=0.19780206680297852\n",
      "epoch 28 iter 67 loss=0.2120121568441391\n",
      "epoch 28 iter 68 loss=0.3240397274494171\n",
      "epoch 28 iter 69 loss=0.28520554304122925\n",
      "epoch 28 iter 70 loss=0.2528529763221741\n",
      "epoch 28 iter 71 loss=0.20129132270812988\n",
      "epoch 28 iter 72 loss=0.17786599695682526\n",
      "epoch 28 iter 73 loss=0.21008919179439545\n",
      "epoch 28 iter 74 loss=0.15123403072357178\n",
      "epoch 29 iter 0 loss=0.2617870569229126\n",
      "epoch 29 iter 1 loss=0.10986826568841934\n",
      "epoch 29 iter 2 loss=0.3977755308151245\n",
      "epoch 29 iter 3 loss=0.14484497904777527\n",
      "epoch 29 iter 4 loss=0.3431699872016907\n",
      "epoch 29 iter 5 loss=0.1936606913805008\n",
      "epoch 29 iter 6 loss=0.16759884357452393\n",
      "epoch 29 iter 7 loss=0.22253547608852386\n",
      "epoch 29 iter 8 loss=0.12395132333040237\n",
      "epoch 29 iter 9 loss=0.40670180320739746\n",
      "epoch 29 iter 10 loss=0.2849380373954773\n",
      "epoch 29 iter 11 loss=0.28613874316215515\n",
      "epoch 29 iter 12 loss=0.34407708048820496\n",
      "epoch 29 iter 13 loss=0.16011054813861847\n",
      "epoch 29 iter 14 loss=0.20447629690170288\n",
      "epoch 29 iter 15 loss=0.47886142134666443\n",
      "epoch 29 iter 16 loss=0.3884463310241699\n",
      "epoch 29 iter 17 loss=0.13764317333698273\n",
      "epoch 29 iter 18 loss=0.2688625454902649\n",
      "epoch 29 iter 19 loss=0.27495884895324707\n",
      "epoch 29 iter 20 loss=0.13759241998195648\n",
      "epoch 29 iter 21 loss=0.378658264875412\n",
      "epoch 29 iter 22 loss=0.24930965900421143\n",
      "epoch 29 iter 23 loss=0.32738178968429565\n",
      "epoch 29 iter 24 loss=0.1653435230255127\n",
      "epoch 29 iter 25 loss=0.19502419233322144\n",
      "epoch 29 iter 26 loss=0.40310022234916687\n",
      "epoch 29 iter 27 loss=0.450992614030838\n",
      "epoch 29 iter 28 loss=0.1596311628818512\n",
      "epoch 29 iter 29 loss=0.3373171389102936\n",
      "epoch 29 iter 30 loss=0.2118551880121231\n",
      "epoch 29 iter 31 loss=0.28791072964668274\n",
      "epoch 29 iter 32 loss=0.1748325079679489\n",
      "epoch 29 iter 33 loss=0.18167008459568024\n",
      "epoch 29 iter 34 loss=0.18911804258823395\n",
      "epoch 29 iter 35 loss=0.25590431690216064\n",
      "epoch 29 iter 36 loss=0.288547158241272\n",
      "epoch 29 iter 37 loss=0.19642966985702515\n",
      "epoch 29 iter 38 loss=0.44319185614585876\n",
      "epoch 29 iter 39 loss=0.27530547976493835\n",
      "epoch 29 iter 40 loss=0.15461613237857819\n",
      "epoch 29 iter 41 loss=0.24087926745414734\n",
      "epoch 29 iter 42 loss=0.32441699504852295\n",
      "epoch 29 iter 43 loss=0.2441471368074417\n",
      "epoch 29 iter 44 loss=0.16259227693080902\n",
      "epoch 29 iter 45 loss=0.24260877072811127\n",
      "epoch 29 iter 46 loss=0.2775176763534546\n",
      "epoch 29 iter 47 loss=0.4034331738948822\n",
      "epoch 29 iter 48 loss=0.21942035853862762\n",
      "epoch 29 iter 49 loss=0.3827351927757263\n",
      "epoch 29 iter 50 loss=0.1952809989452362\n",
      "epoch 29 iter 51 loss=0.26197826862335205\n",
      "epoch 29 iter 52 loss=0.19139507412910461\n",
      "epoch 29 iter 53 loss=0.3114677667617798\n",
      "epoch 29 iter 54 loss=0.3258889317512512\n",
      "epoch 29 iter 55 loss=0.20380547642707825\n",
      "epoch 29 iter 56 loss=0.2693503499031067\n",
      "epoch 29 iter 57 loss=0.17155523598194122\n",
      "epoch 29 iter 58 loss=0.17880545556545258\n",
      "epoch 29 iter 59 loss=0.18921184539794922\n",
      "epoch 29 iter 60 loss=0.20840175449848175\n",
      "epoch 29 iter 61 loss=0.29302889108657837\n",
      "epoch 29 iter 62 loss=0.16653640568256378\n",
      "epoch 29 iter 63 loss=0.2634302079677582\n",
      "epoch 29 iter 64 loss=0.28450748324394226\n",
      "epoch 29 iter 65 loss=0.2276870161294937\n",
      "epoch 29 iter 66 loss=0.18770767748355865\n",
      "epoch 29 iter 67 loss=0.2553268373012543\n",
      "epoch 29 iter 68 loss=0.21836358308792114\n",
      "epoch 29 iter 69 loss=0.2514524459838867\n",
      "epoch 29 iter 70 loss=0.16991721093654633\n",
      "epoch 29 iter 71 loss=0.184248149394989\n",
      "epoch 29 iter 72 loss=0.19199268519878387\n",
      "epoch 29 iter 73 loss=0.31007513403892517\n",
      "epoch 29 iter 74 loss=0.1408071368932724\n",
      "epoch 30 iter 0 loss=0.19503171741962433\n",
      "epoch 30 iter 1 loss=0.2026202231645584\n",
      "epoch 30 iter 2 loss=0.36313796043395996\n",
      "epoch 30 iter 3 loss=0.1807539016008377\n",
      "epoch 30 iter 4 loss=0.20804178714752197\n",
      "epoch 30 iter 5 loss=0.26126745343208313\n",
      "epoch 30 iter 6 loss=0.15997737646102905\n",
      "epoch 30 iter 7 loss=0.2639112174510956\n",
      "epoch 30 iter 8 loss=0.3175232708454132\n",
      "epoch 30 iter 9 loss=0.24834206700325012\n",
      "epoch 30 iter 10 loss=0.20209841430187225\n",
      "epoch 30 iter 11 loss=0.2565763294696808\n",
      "epoch 30 iter 12 loss=0.33862465620040894\n",
      "epoch 30 iter 13 loss=0.20354026556015015\n",
      "epoch 30 iter 14 loss=0.3924344778060913\n",
      "epoch 30 iter 15 loss=0.4136449992656708\n",
      "epoch 30 iter 16 loss=0.22448404133319855\n",
      "epoch 30 iter 17 loss=0.23301181197166443\n",
      "epoch 30 iter 18 loss=0.3632562458515167\n",
      "epoch 30 iter 19 loss=0.20068103075027466\n",
      "epoch 30 iter 20 loss=0.6046775579452515\n",
      "epoch 30 iter 21 loss=0.2119007408618927\n",
      "epoch 30 iter 22 loss=0.26417866349220276\n",
      "epoch 30 iter 23 loss=0.2693920433521271\n",
      "epoch 30 iter 24 loss=0.2735278308391571\n",
      "epoch 30 iter 25 loss=0.2893001139163971\n",
      "epoch 30 iter 26 loss=0.3926069140434265\n",
      "epoch 30 iter 27 loss=0.2514939606189728\n",
      "epoch 30 iter 28 loss=0.21801866590976715\n",
      "epoch 30 iter 29 loss=0.2955804765224457\n",
      "epoch 30 iter 30 loss=0.26298022270202637\n",
      "epoch 30 iter 31 loss=0.20101571083068848\n",
      "epoch 30 iter 32 loss=0.18693317472934723\n",
      "epoch 30 iter 33 loss=0.20802104473114014\n",
      "epoch 30 iter 34 loss=0.29421690106391907\n",
      "epoch 30 iter 35 loss=0.20665378868579865\n",
      "epoch 30 iter 36 loss=0.12740346789360046\n",
      "epoch 30 iter 37 loss=0.3020312190055847\n",
      "epoch 30 iter 38 loss=0.15424297749996185\n",
      "epoch 30 iter 39 loss=0.16722802817821503\n",
      "epoch 30 iter 40 loss=0.30813103914260864\n",
      "epoch 30 iter 41 loss=0.3098280131816864\n",
      "epoch 30 iter 42 loss=0.19256752729415894\n",
      "epoch 30 iter 43 loss=0.28514564037323\n",
      "epoch 30 iter 44 loss=0.2608886659145355\n",
      "epoch 30 iter 45 loss=0.2733411192893982\n",
      "epoch 30 iter 46 loss=0.30918848514556885\n",
      "epoch 30 iter 47 loss=0.24215973913669586\n",
      "epoch 30 iter 48 loss=0.3024909198284149\n",
      "epoch 30 iter 49 loss=0.2832009196281433\n",
      "epoch 30 iter 50 loss=0.19838660955429077\n",
      "epoch 30 iter 51 loss=0.2815510332584381\n",
      "epoch 30 iter 52 loss=0.22021867334842682\n",
      "epoch 30 iter 53 loss=0.18015339970588684\n",
      "epoch 30 iter 54 loss=0.151392862200737\n",
      "epoch 30 iter 55 loss=0.25585827231407166\n",
      "epoch 30 iter 56 loss=0.32389867305755615\n",
      "epoch 30 iter 57 loss=0.1514561027288437\n",
      "epoch 30 iter 58 loss=0.3323062062263489\n",
      "epoch 30 iter 59 loss=0.17748209834098816\n",
      "epoch 30 iter 60 loss=0.12446361780166626\n",
      "epoch 30 iter 61 loss=0.44659295678138733\n",
      "epoch 30 iter 62 loss=0.27151647210121155\n",
      "epoch 30 iter 63 loss=0.21120476722717285\n",
      "epoch 30 iter 64 loss=0.35514676570892334\n",
      "epoch 30 iter 65 loss=0.24759510159492493\n",
      "epoch 30 iter 66 loss=0.41043341159820557\n",
      "epoch 30 iter 67 loss=0.23899173736572266\n",
      "epoch 30 iter 68 loss=0.2882620096206665\n",
      "epoch 30 iter 69 loss=0.28159621357917786\n",
      "epoch 30 iter 70 loss=0.4557294249534607\n",
      "epoch 30 iter 71 loss=0.4666482210159302\n",
      "epoch 30 iter 72 loss=0.2907710075378418\n",
      "epoch 30 iter 73 loss=0.43238353729248047\n",
      "epoch 30 iter 74 loss=0.3493614196777344\n",
      "epoch 31 iter 0 loss=0.5011304020881653\n",
      "epoch 31 iter 1 loss=0.30924251675605774\n",
      "epoch 31 iter 2 loss=0.25269177556037903\n",
      "epoch 31 iter 3 loss=0.23858968913555145\n",
      "epoch 31 iter 4 loss=0.2864148020744324\n",
      "epoch 31 iter 5 loss=0.27276819944381714\n",
      "epoch 31 iter 6 loss=0.25061166286468506\n",
      "epoch 31 iter 7 loss=0.27606651186943054\n",
      "epoch 31 iter 8 loss=0.3381064236164093\n",
      "epoch 31 iter 9 loss=0.23574188351631165\n",
      "epoch 31 iter 10 loss=0.20609654486179352\n",
      "epoch 31 iter 11 loss=0.20042933523654938\n",
      "epoch 31 iter 12 loss=0.20020492374897003\n",
      "epoch 31 iter 13 loss=0.15601317584514618\n",
      "epoch 31 iter 14 loss=0.42771559953689575\n",
      "epoch 31 iter 15 loss=0.19055737555027008\n",
      "epoch 31 iter 16 loss=0.2788834273815155\n",
      "epoch 31 iter 17 loss=0.22121429443359375\n",
      "epoch 31 iter 18 loss=0.33060088753700256\n",
      "epoch 31 iter 19 loss=0.17612995207309723\n",
      "epoch 31 iter 20 loss=0.3665498197078705\n",
      "epoch 31 iter 21 loss=0.19868366420269012\n",
      "epoch 31 iter 22 loss=0.4264463186264038\n",
      "epoch 31 iter 23 loss=0.3466200828552246\n",
      "epoch 31 iter 24 loss=0.26641184091567993\n",
      "epoch 31 iter 25 loss=0.15196816623210907\n",
      "epoch 31 iter 26 loss=0.15991494059562683\n",
      "epoch 31 iter 27 loss=0.2032937854528427\n",
      "epoch 31 iter 28 loss=0.3677493631839752\n",
      "epoch 31 iter 29 loss=0.172119602560997\n",
      "epoch 31 iter 30 loss=0.18299388885498047\n",
      "epoch 31 iter 31 loss=0.3154182434082031\n",
      "epoch 31 iter 32 loss=0.13720324635505676\n",
      "epoch 31 iter 33 loss=0.20887243747711182\n",
      "epoch 31 iter 34 loss=0.4183647930622101\n",
      "epoch 31 iter 35 loss=0.2869468629360199\n",
      "epoch 31 iter 36 loss=0.22242458164691925\n",
      "epoch 31 iter 37 loss=0.1839861422777176\n",
      "epoch 31 iter 38 loss=0.2007163166999817\n",
      "epoch 31 iter 39 loss=0.4231609106063843\n",
      "epoch 31 iter 40 loss=0.2808850109577179\n",
      "epoch 31 iter 41 loss=0.23606669902801514\n",
      "epoch 31 iter 42 loss=0.14200498163700104\n",
      "epoch 31 iter 43 loss=0.21349240839481354\n",
      "epoch 31 iter 44 loss=0.22873520851135254\n",
      "epoch 31 iter 45 loss=0.25172603130340576\n",
      "epoch 31 iter 46 loss=0.27363234758377075\n",
      "epoch 31 iter 47 loss=0.2977897822856903\n",
      "epoch 31 iter 48 loss=0.14674851298332214\n",
      "epoch 31 iter 49 loss=0.23699404299259186\n",
      "epoch 31 iter 50 loss=0.7626178860664368\n",
      "epoch 31 iter 51 loss=0.19049914181232452\n",
      "epoch 31 iter 52 loss=0.19510206580162048\n",
      "epoch 31 iter 53 loss=0.16749601066112518\n",
      "epoch 31 iter 54 loss=0.22339841723442078\n",
      "epoch 31 iter 55 loss=0.30156704783439636\n",
      "epoch 31 iter 56 loss=0.2361944317817688\n",
      "epoch 31 iter 57 loss=0.19652333855628967\n",
      "epoch 31 iter 58 loss=0.3166493773460388\n",
      "epoch 31 iter 59 loss=0.2829802632331848\n",
      "epoch 31 iter 60 loss=0.17664264142513275\n",
      "epoch 31 iter 61 loss=0.15567845106124878\n",
      "epoch 31 iter 62 loss=0.21470297873020172\n",
      "epoch 31 iter 63 loss=0.1930977702140808\n",
      "epoch 31 iter 64 loss=0.25024694204330444\n",
      "epoch 31 iter 65 loss=0.24198515713214874\n",
      "epoch 31 iter 66 loss=0.3557024896144867\n",
      "epoch 31 iter 67 loss=0.1583351045846939\n",
      "epoch 31 iter 68 loss=0.1899128556251526\n",
      "epoch 31 iter 69 loss=0.14423666894435883\n",
      "epoch 31 iter 70 loss=0.24869288504123688\n",
      "epoch 31 iter 71 loss=0.26535025238990784\n",
      "epoch 31 iter 72 loss=0.20980210602283478\n",
      "epoch 31 iter 73 loss=0.2006092518568039\n",
      "epoch 31 iter 74 loss=0.2482607364654541\n",
      "epoch 32 iter 0 loss=0.37658947706222534\n",
      "epoch 32 iter 1 loss=0.23864726722240448\n",
      "epoch 32 iter 2 loss=0.22678206861019135\n",
      "epoch 32 iter 3 loss=0.15759395062923431\n",
      "epoch 32 iter 4 loss=0.16382543742656708\n",
      "epoch 32 iter 5 loss=0.21479524672031403\n",
      "epoch 32 iter 6 loss=0.20654495060443878\n",
      "epoch 32 iter 7 loss=0.2461419254541397\n",
      "epoch 32 iter 8 loss=0.2197466939687729\n",
      "epoch 32 iter 9 loss=0.34193331003189087\n",
      "epoch 32 iter 10 loss=0.2436452955007553\n",
      "epoch 32 iter 11 loss=0.183796226978302\n",
      "epoch 32 iter 12 loss=0.2898496091365814\n",
      "epoch 32 iter 13 loss=0.18985183537006378\n",
      "epoch 32 iter 14 loss=0.20811079442501068\n",
      "epoch 32 iter 15 loss=0.1270495057106018\n",
      "epoch 32 iter 16 loss=0.13630223274230957\n",
      "epoch 32 iter 17 loss=0.2510569095611572\n",
      "epoch 32 iter 18 loss=0.21212036907672882\n",
      "epoch 32 iter 19 loss=0.16421958804130554\n",
      "epoch 32 iter 20 loss=0.1481071412563324\n",
      "epoch 32 iter 21 loss=0.1704425811767578\n",
      "epoch 32 iter 22 loss=0.20035916566848755\n",
      "epoch 32 iter 23 loss=0.1152203381061554\n",
      "epoch 32 iter 24 loss=0.22089892625808716\n",
      "epoch 32 iter 25 loss=0.24589107930660248\n",
      "epoch 32 iter 26 loss=0.14183703064918518\n",
      "epoch 32 iter 27 loss=0.16359341144561768\n",
      "epoch 32 iter 28 loss=0.28069597482681274\n",
      "epoch 32 iter 29 loss=0.3162659704685211\n",
      "epoch 32 iter 30 loss=0.17493271827697754\n",
      "epoch 32 iter 31 loss=0.1994394212961197\n",
      "epoch 32 iter 32 loss=0.1413840651512146\n",
      "epoch 32 iter 33 loss=0.1951805204153061\n",
      "epoch 32 iter 34 loss=0.20851147174835205\n",
      "epoch 32 iter 35 loss=0.20987795293331146\n",
      "epoch 32 iter 36 loss=0.17069686949253082\n",
      "epoch 32 iter 37 loss=0.5904247760772705\n",
      "epoch 32 iter 38 loss=0.1641526073217392\n",
      "epoch 32 iter 39 loss=0.25078555941581726\n",
      "epoch 32 iter 40 loss=0.1477963626384735\n",
      "epoch 32 iter 41 loss=0.15769533812999725\n",
      "epoch 32 iter 42 loss=0.18950696289539337\n",
      "epoch 32 iter 43 loss=0.33185991644859314\n",
      "epoch 32 iter 44 loss=0.21711857616901398\n",
      "epoch 32 iter 45 loss=0.2939636707305908\n",
      "epoch 32 iter 46 loss=0.21763285994529724\n",
      "epoch 32 iter 47 loss=0.17763063311576843\n",
      "epoch 32 iter 48 loss=0.16802850365638733\n",
      "epoch 32 iter 49 loss=0.15420158207416534\n",
      "epoch 32 iter 50 loss=0.2224900722503662\n",
      "epoch 32 iter 51 loss=0.27123183012008667\n",
      "epoch 32 iter 52 loss=0.2068335860967636\n",
      "epoch 32 iter 53 loss=0.20056849718093872\n",
      "epoch 32 iter 54 loss=0.33670055866241455\n",
      "epoch 32 iter 55 loss=0.23894280195236206\n",
      "epoch 32 iter 56 loss=0.15276184678077698\n",
      "epoch 32 iter 57 loss=0.15116794407367706\n",
      "epoch 32 iter 58 loss=0.21901315450668335\n",
      "epoch 32 iter 59 loss=0.22803065180778503\n",
      "epoch 32 iter 60 loss=0.174811452627182\n",
      "epoch 32 iter 61 loss=0.16377076506614685\n",
      "epoch 32 iter 62 loss=0.1371620148420334\n",
      "epoch 32 iter 63 loss=0.20015856623649597\n",
      "epoch 32 iter 64 loss=0.13328416645526886\n",
      "epoch 32 iter 65 loss=0.1818227916955948\n",
      "epoch 32 iter 66 loss=0.20365935564041138\n",
      "epoch 32 iter 67 loss=0.17099063098430634\n",
      "epoch 32 iter 68 loss=0.12152740359306335\n",
      "epoch 32 iter 69 loss=0.16252529621124268\n",
      "epoch 32 iter 70 loss=0.32951483130455017\n",
      "epoch 32 iter 71 loss=0.2933827042579651\n",
      "epoch 32 iter 72 loss=0.27070358395576477\n",
      "epoch 32 iter 73 loss=0.126022607088089\n",
      "epoch 32 iter 74 loss=0.1576600968837738\n",
      "epoch 33 iter 0 loss=0.112550750374794\n",
      "epoch 33 iter 1 loss=0.1153855174779892\n",
      "epoch 33 iter 2 loss=0.27016010880470276\n",
      "epoch 33 iter 3 loss=0.15943630039691925\n",
      "epoch 33 iter 4 loss=0.16807197034358978\n",
      "epoch 33 iter 5 loss=0.1284247636795044\n",
      "epoch 33 iter 6 loss=0.17014288902282715\n",
      "epoch 33 iter 7 loss=0.2962411344051361\n",
      "epoch 33 iter 8 loss=0.15906965732574463\n",
      "epoch 33 iter 9 loss=0.15169158577919006\n",
      "epoch 33 iter 10 loss=0.21462547779083252\n",
      "epoch 33 iter 11 loss=0.08875098824501038\n",
      "epoch 33 iter 12 loss=0.2017577737569809\n",
      "epoch 33 iter 13 loss=0.16823849081993103\n",
      "epoch 33 iter 14 loss=0.1325220912694931\n",
      "epoch 33 iter 15 loss=0.210004985332489\n",
      "epoch 33 iter 16 loss=0.24546243250370026\n",
      "epoch 33 iter 17 loss=0.37732940912246704\n",
      "epoch 33 iter 18 loss=0.19872862100601196\n",
      "epoch 33 iter 19 loss=0.19033841788768768\n",
      "epoch 33 iter 20 loss=0.18697763979434967\n",
      "epoch 33 iter 21 loss=0.2125461846590042\n",
      "epoch 33 iter 22 loss=0.17387105524539948\n",
      "epoch 33 iter 23 loss=0.18424458801746368\n",
      "epoch 33 iter 24 loss=0.21903455257415771\n",
      "epoch 33 iter 25 loss=0.8751060366630554\n",
      "epoch 33 iter 26 loss=0.12044203281402588\n",
      "epoch 33 iter 27 loss=0.18660153448581696\n",
      "epoch 33 iter 28 loss=0.21707232296466827\n",
      "epoch 33 iter 29 loss=0.3725692331790924\n",
      "epoch 33 iter 30 loss=0.44469982385635376\n",
      "epoch 33 iter 31 loss=0.18403440713882446\n",
      "epoch 33 iter 32 loss=0.17958348989486694\n",
      "epoch 33 iter 33 loss=0.15705890953540802\n",
      "epoch 33 iter 34 loss=0.2384769469499588\n",
      "epoch 33 iter 35 loss=0.37997838854789734\n",
      "epoch 33 iter 36 loss=0.23181027173995972\n",
      "epoch 33 iter 37 loss=0.16266129910945892\n",
      "epoch 33 iter 38 loss=0.20419900119304657\n",
      "epoch 33 iter 39 loss=0.3091714382171631\n",
      "epoch 33 iter 40 loss=0.20124469697475433\n",
      "epoch 33 iter 41 loss=0.23304566740989685\n",
      "epoch 33 iter 42 loss=0.25916996598243713\n",
      "epoch 33 iter 43 loss=0.21783798933029175\n",
      "epoch 33 iter 44 loss=0.16511693596839905\n",
      "epoch 33 iter 45 loss=0.35799673199653625\n",
      "epoch 33 iter 46 loss=0.2851257622241974\n",
      "epoch 33 iter 47 loss=0.2356337606906891\n",
      "epoch 33 iter 48 loss=0.2721472382545471\n",
      "epoch 33 iter 49 loss=0.26725754141807556\n",
      "epoch 33 iter 50 loss=0.22279855608940125\n",
      "epoch 33 iter 51 loss=0.297481894493103\n",
      "epoch 33 iter 52 loss=0.3380223214626312\n",
      "epoch 33 iter 53 loss=0.23031792044639587\n",
      "epoch 33 iter 54 loss=0.23690539598464966\n",
      "epoch 33 iter 55 loss=0.34607985615730286\n",
      "epoch 33 iter 56 loss=0.1453360915184021\n",
      "epoch 33 iter 57 loss=0.18369841575622559\n",
      "epoch 33 iter 58 loss=0.19373834133148193\n",
      "epoch 33 iter 59 loss=0.16692516207695007\n",
      "epoch 33 iter 60 loss=0.27568548917770386\n",
      "epoch 33 iter 61 loss=0.2658536732196808\n",
      "epoch 33 iter 62 loss=0.1522868275642395\n",
      "epoch 33 iter 63 loss=0.11778854578733444\n",
      "epoch 33 iter 64 loss=0.1971643716096878\n",
      "epoch 33 iter 65 loss=0.22714880108833313\n",
      "epoch 33 iter 66 loss=0.20496775209903717\n",
      "epoch 33 iter 67 loss=0.23965944349765778\n",
      "epoch 33 iter 68 loss=0.12910343706607819\n",
      "epoch 33 iter 69 loss=0.22204385697841644\n",
      "epoch 33 iter 70 loss=0.16860470175743103\n",
      "epoch 33 iter 71 loss=0.46443596482276917\n",
      "epoch 33 iter 72 loss=0.2717958688735962\n",
      "epoch 33 iter 73 loss=0.13692757487297058\n",
      "epoch 33 iter 74 loss=0.17920634150505066\n",
      "epoch 34 iter 0 loss=0.16410225629806519\n",
      "epoch 34 iter 1 loss=0.24349208176136017\n",
      "epoch 34 iter 2 loss=0.2818714380264282\n",
      "epoch 34 iter 3 loss=0.33811211585998535\n",
      "epoch 34 iter 4 loss=0.1482790857553482\n",
      "epoch 34 iter 5 loss=0.1223783791065216\n",
      "epoch 34 iter 6 loss=0.15373937785625458\n",
      "epoch 34 iter 7 loss=0.16314083337783813\n",
      "epoch 34 iter 8 loss=0.1523047685623169\n",
      "epoch 34 iter 9 loss=0.22500713169574738\n",
      "epoch 34 iter 10 loss=0.17885631322860718\n",
      "epoch 34 iter 11 loss=0.24869781732559204\n",
      "epoch 34 iter 12 loss=0.17811979353427887\n",
      "epoch 34 iter 13 loss=0.161874920129776\n",
      "epoch 34 iter 14 loss=0.10429809987545013\n",
      "epoch 34 iter 15 loss=0.12669719755649567\n",
      "epoch 34 iter 16 loss=0.1712305098772049\n",
      "epoch 34 iter 17 loss=0.1552799493074417\n",
      "epoch 34 iter 18 loss=0.27307963371276855\n",
      "epoch 34 iter 19 loss=0.24969319999217987\n",
      "epoch 34 iter 20 loss=0.14040450751781464\n",
      "epoch 34 iter 21 loss=0.2128836214542389\n",
      "epoch 34 iter 22 loss=0.1463596522808075\n",
      "epoch 34 iter 23 loss=0.2096439003944397\n",
      "epoch 34 iter 24 loss=0.1849973052740097\n",
      "epoch 34 iter 25 loss=0.15819141268730164\n",
      "epoch 34 iter 26 loss=0.15541544556617737\n",
      "epoch 34 iter 27 loss=0.3339841365814209\n",
      "epoch 34 iter 28 loss=0.11049187183380127\n",
      "epoch 34 iter 29 loss=0.10487432777881622\n",
      "epoch 34 iter 30 loss=0.4295150637626648\n",
      "epoch 34 iter 31 loss=0.23876598477363586\n",
      "epoch 34 iter 32 loss=0.14139840006828308\n",
      "epoch 34 iter 33 loss=0.2731721103191376\n",
      "epoch 34 iter 34 loss=0.47208064794540405\n",
      "epoch 34 iter 35 loss=0.2780719995498657\n",
      "epoch 34 iter 36 loss=0.31124982237815857\n",
      "epoch 34 iter 37 loss=0.25606709718704224\n",
      "epoch 34 iter 38 loss=0.18968728184700012\n",
      "epoch 34 iter 39 loss=0.18518465757369995\n",
      "epoch 34 iter 40 loss=0.3612247407436371\n",
      "epoch 34 iter 41 loss=0.15974551439285278\n",
      "epoch 34 iter 42 loss=0.19928616285324097\n",
      "epoch 34 iter 43 loss=0.2099553346633911\n",
      "epoch 34 iter 44 loss=0.11536462604999542\n",
      "epoch 34 iter 45 loss=0.1635199338197708\n",
      "epoch 34 iter 46 loss=0.1851755529642105\n",
      "epoch 34 iter 47 loss=0.3357245922088623\n",
      "epoch 34 iter 48 loss=0.18253152072429657\n",
      "epoch 34 iter 49 loss=0.2887560725212097\n",
      "epoch 34 iter 50 loss=0.21082144975662231\n",
      "epoch 34 iter 51 loss=0.35200032591819763\n",
      "epoch 34 iter 52 loss=0.24550463259220123\n",
      "epoch 34 iter 53 loss=0.22777849435806274\n",
      "epoch 34 iter 54 loss=0.16632269322872162\n",
      "epoch 34 iter 55 loss=0.2771308422088623\n",
      "epoch 34 iter 56 loss=0.5304276347160339\n",
      "epoch 34 iter 57 loss=0.268200546503067\n",
      "epoch 34 iter 58 loss=0.2899739742279053\n",
      "epoch 34 iter 59 loss=0.7052346467971802\n",
      "epoch 34 iter 60 loss=0.14614525437355042\n",
      "epoch 34 iter 61 loss=0.242808535695076\n",
      "epoch 34 iter 62 loss=0.16628707945346832\n",
      "epoch 34 iter 63 loss=0.16353915631771088\n",
      "epoch 34 iter 64 loss=0.3231489062309265\n",
      "epoch 34 iter 65 loss=0.25748586654663086\n",
      "epoch 34 iter 66 loss=0.30310800671577454\n",
      "epoch 34 iter 67 loss=0.2851472795009613\n",
      "epoch 34 iter 68 loss=0.12539374828338623\n",
      "epoch 34 iter 69 loss=0.3058371841907501\n",
      "epoch 34 iter 70 loss=0.1630743443965912\n",
      "epoch 34 iter 71 loss=0.2534336447715759\n",
      "epoch 34 iter 72 loss=0.27606847882270813\n",
      "epoch 34 iter 73 loss=0.3199748992919922\n",
      "epoch 34 iter 74 loss=0.19957523047924042\n",
      "epoch 35 iter 0 loss=0.3010466992855072\n",
      "epoch 35 iter 1 loss=0.2138526886701584\n",
      "epoch 35 iter 2 loss=0.1257496476173401\n",
      "epoch 35 iter 3 loss=0.20239342749118805\n",
      "epoch 35 iter 4 loss=0.19008241593837738\n",
      "epoch 35 iter 5 loss=0.37574443221092224\n",
      "epoch 35 iter 6 loss=0.23358125984668732\n",
      "epoch 35 iter 7 loss=0.18685780465602875\n",
      "epoch 35 iter 8 loss=0.1319536566734314\n",
      "epoch 35 iter 9 loss=0.3184564411640167\n",
      "epoch 35 iter 10 loss=0.3337603211402893\n",
      "epoch 35 iter 11 loss=0.24776984751224518\n",
      "epoch 35 iter 12 loss=0.18958058953285217\n",
      "epoch 35 iter 13 loss=0.2181284874677658\n",
      "epoch 35 iter 14 loss=0.13211795687675476\n",
      "epoch 35 iter 15 loss=0.17657318711280823\n",
      "epoch 35 iter 16 loss=0.1918380707502365\n",
      "epoch 35 iter 17 loss=0.34599965810775757\n",
      "epoch 35 iter 18 loss=0.3074687719345093\n",
      "epoch 35 iter 19 loss=0.2942486107349396\n",
      "epoch 35 iter 20 loss=0.29565897583961487\n",
      "epoch 35 iter 21 loss=0.2104519158601761\n",
      "epoch 35 iter 22 loss=0.2745986580848694\n",
      "epoch 35 iter 23 loss=0.23474116623401642\n",
      "epoch 35 iter 24 loss=0.22371400892734528\n",
      "epoch 35 iter 25 loss=0.23464037477970123\n",
      "epoch 35 iter 26 loss=0.17801153659820557\n",
      "epoch 35 iter 27 loss=0.185150146484375\n",
      "epoch 35 iter 28 loss=0.3063640892505646\n",
      "epoch 35 iter 29 loss=0.23524795472621918\n",
      "epoch 35 iter 30 loss=0.2642945349216461\n",
      "epoch 35 iter 31 loss=0.26393309235572815\n",
      "epoch 35 iter 32 loss=0.16908307373523712\n",
      "epoch 35 iter 33 loss=0.25990161299705505\n",
      "epoch 35 iter 34 loss=0.1317315697669983\n",
      "epoch 35 iter 35 loss=0.186478391289711\n",
      "epoch 35 iter 36 loss=0.14942045509815216\n",
      "epoch 35 iter 37 loss=0.3402616083621979\n",
      "epoch 35 iter 38 loss=0.20479357242584229\n",
      "epoch 35 iter 39 loss=0.21422883868217468\n",
      "epoch 35 iter 40 loss=0.19539734721183777\n",
      "epoch 35 iter 41 loss=0.255890429019928\n",
      "epoch 35 iter 42 loss=0.2726339101791382\n",
      "epoch 35 iter 43 loss=0.4208981394767761\n",
      "epoch 35 iter 44 loss=0.13036605715751648\n",
      "epoch 35 iter 45 loss=0.251864492893219\n",
      "epoch 35 iter 46 loss=0.32126209139823914\n",
      "epoch 35 iter 47 loss=0.1746029108762741\n",
      "epoch 35 iter 48 loss=0.17304366827011108\n",
      "epoch 35 iter 49 loss=0.1821696162223816\n",
      "epoch 35 iter 50 loss=0.17591217160224915\n",
      "epoch 35 iter 51 loss=0.17917302250862122\n",
      "epoch 35 iter 52 loss=0.22550489008426666\n",
      "epoch 35 iter 53 loss=0.1460583657026291\n",
      "epoch 35 iter 54 loss=0.13194264471530914\n",
      "epoch 35 iter 55 loss=0.19134066998958588\n",
      "epoch 35 iter 56 loss=0.2915020287036896\n",
      "epoch 35 iter 57 loss=0.17035892605781555\n",
      "epoch 35 iter 58 loss=0.18401019275188446\n",
      "epoch 35 iter 59 loss=0.2727542221546173\n",
      "epoch 35 iter 60 loss=0.13506768643856049\n",
      "epoch 35 iter 61 loss=0.23467563092708588\n",
      "epoch 35 iter 62 loss=0.18335393071174622\n",
      "epoch 35 iter 63 loss=0.23268373310565948\n",
      "epoch 35 iter 64 loss=0.5077902674674988\n",
      "epoch 35 iter 65 loss=0.2554217278957367\n",
      "epoch 35 iter 66 loss=0.13440822064876556\n",
      "epoch 35 iter 67 loss=0.17308668792247772\n",
      "epoch 35 iter 68 loss=0.27465295791625977\n",
      "epoch 35 iter 69 loss=0.21474944055080414\n",
      "epoch 35 iter 70 loss=0.12880229949951172\n",
      "epoch 35 iter 71 loss=0.247192844748497\n",
      "epoch 35 iter 72 loss=0.27056607604026794\n",
      "epoch 35 iter 73 loss=0.22159099578857422\n",
      "epoch 35 iter 74 loss=0.14338085055351257\n",
      "epoch 36 iter 0 loss=0.30766433477401733\n",
      "epoch 36 iter 1 loss=0.13938376307487488\n",
      "epoch 36 iter 2 loss=0.2652152478694916\n",
      "epoch 36 iter 3 loss=0.20641599595546722\n",
      "epoch 36 iter 4 loss=0.21834607422351837\n",
      "epoch 36 iter 5 loss=0.22463873028755188\n",
      "epoch 36 iter 6 loss=0.20367754995822906\n",
      "epoch 36 iter 7 loss=0.19449642300605774\n",
      "epoch 36 iter 8 loss=0.1799268275499344\n",
      "epoch 36 iter 9 loss=0.28633761405944824\n",
      "epoch 36 iter 10 loss=0.17363138496875763\n",
      "epoch 36 iter 11 loss=0.1523606926202774\n",
      "epoch 36 iter 12 loss=0.3008769750595093\n",
      "epoch 36 iter 13 loss=0.12340939044952393\n",
      "epoch 36 iter 14 loss=0.18751467764377594\n",
      "epoch 36 iter 15 loss=0.13237929344177246\n",
      "epoch 36 iter 16 loss=0.35350069403648376\n",
      "epoch 36 iter 17 loss=0.14942124485969543\n",
      "epoch 36 iter 18 loss=0.19116608798503876\n",
      "epoch 36 iter 19 loss=0.13390427827835083\n",
      "epoch 36 iter 20 loss=0.3367086350917816\n",
      "epoch 36 iter 21 loss=0.21054460108280182\n",
      "epoch 36 iter 22 loss=0.6550865769386292\n",
      "epoch 36 iter 23 loss=0.20290182530879974\n",
      "epoch 36 iter 24 loss=0.13246981799602509\n",
      "epoch 36 iter 25 loss=0.23511822521686554\n",
      "epoch 36 iter 26 loss=0.1595294028520584\n",
      "epoch 36 iter 27 loss=0.15845102071762085\n",
      "epoch 36 iter 28 loss=0.44671276211738586\n",
      "epoch 36 iter 29 loss=0.1876121610403061\n",
      "epoch 36 iter 30 loss=0.1823779046535492\n",
      "epoch 36 iter 31 loss=0.2008952647447586\n",
      "epoch 36 iter 32 loss=0.2513616979122162\n",
      "epoch 36 iter 33 loss=0.17947185039520264\n",
      "epoch 36 iter 34 loss=0.21546250581741333\n",
      "epoch 36 iter 35 loss=0.260972261428833\n",
      "epoch 36 iter 36 loss=0.15797698497772217\n",
      "epoch 36 iter 37 loss=0.21819020807743073\n",
      "epoch 36 iter 38 loss=0.14980441331863403\n",
      "epoch 36 iter 39 loss=0.28467342257499695\n",
      "epoch 36 iter 40 loss=0.22163937985897064\n",
      "epoch 36 iter 41 loss=0.1566106677055359\n",
      "epoch 36 iter 42 loss=0.546356201171875\n",
      "epoch 36 iter 43 loss=0.3393861651420593\n",
      "epoch 36 iter 44 loss=0.19306805729866028\n",
      "epoch 36 iter 45 loss=0.2179756462574005\n",
      "epoch 36 iter 46 loss=0.1577315628528595\n",
      "epoch 36 iter 47 loss=0.17680494487285614\n",
      "epoch 36 iter 48 loss=0.23584260046482086\n",
      "epoch 36 iter 49 loss=0.15279734134674072\n",
      "epoch 36 iter 50 loss=0.24746814370155334\n",
      "epoch 36 iter 51 loss=0.19973663985729218\n",
      "epoch 36 iter 52 loss=0.14860142767429352\n",
      "epoch 36 iter 53 loss=0.11727369576692581\n",
      "epoch 36 iter 54 loss=0.19290797412395477\n",
      "epoch 36 iter 55 loss=0.22546622157096863\n",
      "epoch 36 iter 56 loss=0.13385134935379028\n",
      "epoch 36 iter 57 loss=0.13683588802814484\n",
      "epoch 36 iter 58 loss=0.13499508798122406\n",
      "epoch 36 iter 59 loss=0.12906377017498016\n",
      "epoch 36 iter 60 loss=0.20850920677185059\n",
      "epoch 36 iter 61 loss=0.2310846447944641\n",
      "epoch 36 iter 62 loss=0.16931959986686707\n",
      "epoch 36 iter 63 loss=0.3225933015346527\n",
      "epoch 36 iter 64 loss=0.35249629616737366\n",
      "epoch 36 iter 65 loss=0.2823296785354614\n",
      "epoch 36 iter 66 loss=0.1668775975704193\n",
      "epoch 36 iter 67 loss=0.3019908368587494\n",
      "epoch 36 iter 68 loss=0.198356032371521\n",
      "epoch 36 iter 69 loss=0.2846386730670929\n",
      "epoch 36 iter 70 loss=0.20204323530197144\n",
      "epoch 36 iter 71 loss=0.15639349818229675\n",
      "epoch 36 iter 72 loss=0.1539524495601654\n",
      "epoch 36 iter 73 loss=0.18773698806762695\n",
      "epoch 36 iter 74 loss=0.14720837771892548\n",
      "epoch 37 iter 0 loss=0.19457460939884186\n",
      "epoch 37 iter 1 loss=0.17062680423259735\n",
      "epoch 37 iter 2 loss=0.15982197225093842\n",
      "epoch 37 iter 3 loss=0.48832201957702637\n",
      "epoch 37 iter 4 loss=0.1624586433172226\n",
      "epoch 37 iter 5 loss=0.15811847150325775\n",
      "epoch 37 iter 6 loss=0.19343428313732147\n",
      "epoch 37 iter 7 loss=0.17759114503860474\n",
      "epoch 37 iter 8 loss=0.2722776532173157\n",
      "epoch 37 iter 9 loss=0.19614896178245544\n",
      "epoch 37 iter 10 loss=0.18061323463916779\n",
      "epoch 37 iter 11 loss=0.16193650662899017\n",
      "epoch 37 iter 12 loss=0.19803737103939056\n",
      "epoch 37 iter 13 loss=0.1995677649974823\n",
      "epoch 37 iter 14 loss=0.16925635933876038\n",
      "epoch 37 iter 15 loss=0.14530135691165924\n",
      "epoch 37 iter 16 loss=0.1154637411236763\n",
      "epoch 37 iter 17 loss=0.24129211902618408\n",
      "epoch 37 iter 18 loss=0.17605218291282654\n",
      "epoch 37 iter 19 loss=0.1521024852991104\n",
      "epoch 37 iter 20 loss=0.15319082140922546\n",
      "epoch 37 iter 21 loss=0.20514926314353943\n",
      "epoch 37 iter 22 loss=0.30389440059661865\n",
      "epoch 37 iter 23 loss=0.15195821225643158\n",
      "epoch 37 iter 24 loss=0.18149881064891815\n",
      "epoch 37 iter 25 loss=0.17420576512813568\n",
      "epoch 37 iter 26 loss=0.11131856590509415\n",
      "epoch 37 iter 27 loss=0.3199366629123688\n",
      "epoch 37 iter 28 loss=0.16284553706645966\n",
      "epoch 37 iter 29 loss=0.08088565617799759\n",
      "epoch 37 iter 30 loss=0.17622047662734985\n",
      "epoch 37 iter 31 loss=0.11281635612249374\n",
      "epoch 37 iter 32 loss=0.18825814127922058\n",
      "epoch 37 iter 33 loss=0.25099968910217285\n",
      "epoch 37 iter 34 loss=0.1510000079870224\n",
      "epoch 37 iter 35 loss=0.20434489846229553\n",
      "epoch 37 iter 36 loss=0.1622900664806366\n",
      "epoch 37 iter 37 loss=0.10304855555295944\n",
      "epoch 37 iter 38 loss=0.15478567779064178\n",
      "epoch 37 iter 39 loss=0.13331449031829834\n",
      "epoch 37 iter 40 loss=0.12274881452322006\n",
      "epoch 37 iter 41 loss=0.09183556586503983\n",
      "epoch 37 iter 42 loss=0.17993628978729248\n",
      "epoch 37 iter 43 loss=0.13444089889526367\n",
      "epoch 37 iter 44 loss=0.25819388031959534\n",
      "epoch 37 iter 45 loss=0.21249434351921082\n",
      "epoch 37 iter 46 loss=0.22711484134197235\n",
      "epoch 37 iter 47 loss=0.12680229544639587\n",
      "epoch 37 iter 48 loss=0.12794125080108643\n",
      "epoch 37 iter 49 loss=0.15835866332054138\n",
      "epoch 37 iter 50 loss=0.09936149418354034\n",
      "epoch 37 iter 51 loss=0.232343852519989\n",
      "epoch 37 iter 52 loss=0.10763333737850189\n",
      "epoch 37 iter 53 loss=0.175375297665596\n",
      "epoch 37 iter 54 loss=0.16495421528816223\n",
      "epoch 37 iter 55 loss=0.22871215641498566\n",
      "epoch 37 iter 56 loss=0.18948961794376373\n",
      "epoch 37 iter 57 loss=0.4787302315235138\n",
      "epoch 37 iter 58 loss=0.1928248107433319\n",
      "epoch 37 iter 59 loss=0.13177768886089325\n",
      "epoch 37 iter 60 loss=0.1884986162185669\n",
      "epoch 37 iter 61 loss=0.16751624643802643\n",
      "epoch 37 iter 62 loss=0.12599913775920868\n",
      "epoch 37 iter 63 loss=0.13669665157794952\n",
      "epoch 37 iter 64 loss=0.14670701324939728\n",
      "epoch 37 iter 65 loss=0.20186848938465118\n",
      "epoch 37 iter 66 loss=0.18380562961101532\n",
      "epoch 37 iter 67 loss=0.27735471725463867\n",
      "epoch 37 iter 68 loss=0.30217719078063965\n",
      "epoch 37 iter 69 loss=0.20664209127426147\n",
      "epoch 37 iter 70 loss=0.25369441509246826\n",
      "epoch 37 iter 71 loss=0.1865835338830948\n",
      "epoch 37 iter 72 loss=0.19696517288684845\n",
      "epoch 37 iter 73 loss=0.21481797099113464\n",
      "epoch 37 iter 74 loss=0.2254900485277176\n",
      "epoch 38 iter 0 loss=0.21806131303310394\n",
      "epoch 38 iter 1 loss=0.1687399446964264\n",
      "epoch 38 iter 2 loss=0.1733587682247162\n",
      "epoch 38 iter 3 loss=0.1447618305683136\n",
      "epoch 38 iter 4 loss=0.13417088985443115\n",
      "epoch 38 iter 5 loss=0.14239618182182312\n",
      "epoch 38 iter 6 loss=0.13398604094982147\n",
      "epoch 38 iter 7 loss=0.14613917469978333\n",
      "epoch 38 iter 8 loss=0.19334052503108978\n",
      "epoch 38 iter 9 loss=0.290327787399292\n",
      "epoch 38 iter 10 loss=0.15684832632541656\n",
      "epoch 38 iter 11 loss=0.2650746703147888\n",
      "epoch 38 iter 12 loss=0.20150238275527954\n",
      "epoch 38 iter 13 loss=0.3068626821041107\n",
      "epoch 38 iter 14 loss=0.22371363639831543\n",
      "epoch 38 iter 15 loss=0.202434703707695\n",
      "epoch 38 iter 16 loss=0.25368183851242065\n",
      "epoch 38 iter 17 loss=0.11631115525960922\n",
      "epoch 38 iter 18 loss=0.21617461740970612\n",
      "epoch 38 iter 19 loss=0.1926865428686142\n",
      "epoch 38 iter 20 loss=0.12649455666542053\n",
      "epoch 38 iter 21 loss=0.20990829169750214\n",
      "epoch 38 iter 22 loss=0.15431450307369232\n",
      "epoch 38 iter 23 loss=0.21304532885551453\n",
      "epoch 38 iter 24 loss=0.11761464923620224\n",
      "epoch 38 iter 25 loss=0.1880776435136795\n",
      "epoch 38 iter 26 loss=0.3088614344596863\n",
      "epoch 38 iter 27 loss=0.19930091500282288\n",
      "epoch 38 iter 28 loss=0.1108633428812027\n",
      "epoch 38 iter 29 loss=0.11226633191108704\n",
      "epoch 38 iter 30 loss=0.13954325020313263\n",
      "epoch 38 iter 31 loss=0.22367540001869202\n",
      "epoch 38 iter 32 loss=0.18347656726837158\n",
      "epoch 38 iter 33 loss=0.21367456018924713\n",
      "epoch 38 iter 34 loss=0.13814708590507507\n",
      "epoch 38 iter 35 loss=0.14663167297840118\n",
      "epoch 38 iter 36 loss=0.23009996116161346\n",
      "epoch 38 iter 37 loss=0.22752448916435242\n",
      "epoch 38 iter 38 loss=0.13451583683490753\n",
      "epoch 38 iter 39 loss=0.1433556079864502\n",
      "epoch 38 iter 40 loss=0.14168892800807953\n",
      "epoch 38 iter 41 loss=0.14049796760082245\n",
      "epoch 38 iter 42 loss=0.24679823219776154\n",
      "epoch 38 iter 43 loss=0.14614973962306976\n",
      "epoch 38 iter 44 loss=0.2231193482875824\n",
      "epoch 38 iter 45 loss=0.14633141458034515\n",
      "epoch 38 iter 46 loss=0.18587402999401093\n",
      "epoch 38 iter 47 loss=0.17662668228149414\n",
      "epoch 38 iter 48 loss=0.23401539027690887\n",
      "epoch 38 iter 49 loss=0.2228436917066574\n",
      "epoch 38 iter 50 loss=0.14923380315303802\n",
      "epoch 38 iter 51 loss=0.12791314721107483\n",
      "epoch 38 iter 52 loss=0.19822290539741516\n",
      "epoch 38 iter 53 loss=0.14977425336837769\n",
      "epoch 38 iter 54 loss=0.10481710731983185\n",
      "epoch 38 iter 55 loss=0.26474887132644653\n",
      "epoch 38 iter 56 loss=0.2538833022117615\n",
      "epoch 38 iter 57 loss=0.20735909044742584\n",
      "epoch 38 iter 58 loss=0.18613015115261078\n",
      "epoch 38 iter 59 loss=0.11374446749687195\n",
      "epoch 38 iter 60 loss=0.176145538687706\n",
      "epoch 38 iter 61 loss=0.28671953082084656\n",
      "epoch 38 iter 62 loss=0.2372073233127594\n",
      "epoch 38 iter 63 loss=0.16049931943416595\n",
      "epoch 38 iter 64 loss=0.104793980717659\n",
      "epoch 38 iter 65 loss=0.3179989457130432\n",
      "epoch 38 iter 66 loss=0.1403128206729889\n",
      "epoch 38 iter 67 loss=0.28112155199050903\n",
      "epoch 38 iter 68 loss=0.1988292932510376\n",
      "epoch 38 iter 69 loss=0.2162896990776062\n",
      "epoch 38 iter 70 loss=0.16736368834972382\n",
      "epoch 38 iter 71 loss=0.21814972162246704\n",
      "epoch 38 iter 72 loss=0.2081547975540161\n",
      "epoch 38 iter 73 loss=0.14340995252132416\n",
      "epoch 38 iter 74 loss=0.13613468408584595\n",
      "epoch 39 iter 0 loss=0.1564454585313797\n",
      "epoch 39 iter 1 loss=0.11521146446466446\n",
      "epoch 39 iter 2 loss=0.10612371563911438\n",
      "epoch 39 iter 3 loss=0.381797194480896\n",
      "epoch 39 iter 4 loss=0.2658383548259735\n",
      "epoch 39 iter 5 loss=0.22327351570129395\n",
      "epoch 39 iter 6 loss=0.11852659285068512\n",
      "epoch 39 iter 7 loss=0.3557503819465637\n",
      "epoch 39 iter 8 loss=0.1311206966638565\n",
      "epoch 39 iter 9 loss=0.17935633659362793\n",
      "epoch 39 iter 10 loss=0.17308959364891052\n",
      "epoch 39 iter 11 loss=0.141661137342453\n",
      "epoch 39 iter 12 loss=0.18176878988742828\n",
      "epoch 39 iter 13 loss=0.18916447460651398\n",
      "epoch 39 iter 14 loss=0.22688816487789154\n",
      "epoch 39 iter 15 loss=0.13112224638462067\n",
      "epoch 39 iter 16 loss=0.2271452248096466\n",
      "epoch 39 iter 17 loss=0.19164031744003296\n",
      "epoch 39 iter 18 loss=0.18339908123016357\n",
      "epoch 39 iter 19 loss=0.21319954097270966\n",
      "epoch 39 iter 20 loss=0.18933223187923431\n",
      "epoch 39 iter 21 loss=0.15953445434570312\n",
      "epoch 39 iter 22 loss=0.12665148079395294\n",
      "epoch 39 iter 23 loss=0.12987679243087769\n",
      "epoch 39 iter 24 loss=0.346147745847702\n",
      "epoch 39 iter 25 loss=0.11614242941141129\n",
      "epoch 39 iter 26 loss=0.14421908557415009\n",
      "epoch 39 iter 27 loss=0.22455526888370514\n",
      "epoch 39 iter 28 loss=0.1448153704404831\n",
      "epoch 39 iter 29 loss=0.2173403948545456\n",
      "epoch 39 iter 30 loss=0.19709806144237518\n",
      "epoch 39 iter 31 loss=0.1184181272983551\n",
      "epoch 39 iter 32 loss=0.1504865139722824\n",
      "epoch 39 iter 33 loss=0.18745072185993195\n",
      "epoch 39 iter 34 loss=0.13085541129112244\n",
      "epoch 39 iter 35 loss=0.11475874483585358\n",
      "epoch 39 iter 36 loss=0.1492546945810318\n",
      "epoch 39 iter 37 loss=0.13421417772769928\n",
      "epoch 39 iter 38 loss=0.2273915708065033\n",
      "epoch 39 iter 39 loss=0.3215811848640442\n",
      "epoch 39 iter 40 loss=0.19197730720043182\n",
      "epoch 39 iter 41 loss=0.18892250955104828\n",
      "epoch 39 iter 42 loss=0.0888240858912468\n",
      "epoch 39 iter 43 loss=0.20208536088466644\n",
      "epoch 39 iter 44 loss=0.25604623556137085\n",
      "epoch 39 iter 45 loss=0.16501207649707794\n",
      "epoch 39 iter 46 loss=0.21518947184085846\n",
      "epoch 39 iter 47 loss=0.13028354942798615\n",
      "epoch 39 iter 48 loss=0.13205699622631073\n",
      "epoch 39 iter 49 loss=0.2247430980205536\n",
      "epoch 39 iter 50 loss=0.1129307895898819\n",
      "epoch 39 iter 51 loss=0.2318200021982193\n",
      "epoch 39 iter 52 loss=0.2032196968793869\n",
      "epoch 39 iter 53 loss=0.20047175884246826\n",
      "epoch 39 iter 54 loss=0.23075562715530396\n",
      "epoch 39 iter 55 loss=0.12018755078315735\n",
      "epoch 39 iter 56 loss=0.21097639203071594\n",
      "epoch 39 iter 57 loss=0.1698063611984253\n",
      "epoch 39 iter 58 loss=0.1634586751461029\n",
      "epoch 39 iter 59 loss=0.24673709273338318\n",
      "epoch 39 iter 60 loss=0.1559128314256668\n",
      "epoch 39 iter 61 loss=0.11443520337343216\n",
      "epoch 39 iter 62 loss=0.14898602664470673\n",
      "epoch 39 iter 63 loss=0.14716382324695587\n",
      "epoch 39 iter 64 loss=0.13827349245548248\n",
      "epoch 39 iter 65 loss=0.11318938434123993\n",
      "epoch 39 iter 66 loss=0.12661640346050262\n",
      "epoch 39 iter 67 loss=0.7085723876953125\n",
      "epoch 39 iter 68 loss=0.3342854678630829\n",
      "epoch 39 iter 69 loss=0.1365625411272049\n",
      "epoch 39 iter 70 loss=0.1630220115184784\n",
      "epoch 39 iter 71 loss=0.22273705899715424\n",
      "epoch 39 iter 72 loss=0.1872999668121338\n",
      "epoch 39 iter 73 loss=0.24373771250247955\n",
      "epoch 39 iter 74 loss=0.31553271412849426\n",
      "epoch 40 iter 0 loss=0.14617781341075897\n",
      "epoch 40 iter 1 loss=0.16981573402881622\n",
      "epoch 40 iter 2 loss=0.1687561422586441\n",
      "epoch 40 iter 3 loss=0.17927151918411255\n",
      "epoch 40 iter 4 loss=0.2863440215587616\n",
      "epoch 40 iter 5 loss=0.22315765917301178\n",
      "epoch 40 iter 6 loss=0.2747957706451416\n",
      "epoch 40 iter 7 loss=0.24160674214363098\n",
      "epoch 40 iter 8 loss=0.4639245867729187\n",
      "epoch 40 iter 9 loss=0.1572469174861908\n",
      "epoch 40 iter 10 loss=0.11200778186321259\n",
      "epoch 40 iter 11 loss=0.2280697226524353\n",
      "epoch 40 iter 12 loss=0.1786745935678482\n",
      "epoch 40 iter 13 loss=0.1477002203464508\n",
      "epoch 40 iter 14 loss=0.2861475348472595\n",
      "epoch 40 iter 15 loss=0.19171632826328278\n",
      "epoch 40 iter 16 loss=0.13845202326774597\n",
      "epoch 40 iter 17 loss=0.12086843699216843\n",
      "epoch 40 iter 18 loss=0.23487529158592224\n",
      "epoch 40 iter 19 loss=0.12262646853923798\n",
      "epoch 40 iter 20 loss=0.19701215624809265\n",
      "epoch 40 iter 21 loss=0.13098783791065216\n",
      "epoch 40 iter 22 loss=0.25412556529045105\n",
      "epoch 40 iter 23 loss=0.2185339778661728\n",
      "epoch 40 iter 24 loss=0.27137133479118347\n",
      "epoch 40 iter 25 loss=0.12176875025033951\n",
      "epoch 40 iter 26 loss=0.40088948607444763\n",
      "epoch 40 iter 27 loss=0.16468432545661926\n",
      "epoch 40 iter 28 loss=0.13346418738365173\n",
      "epoch 40 iter 29 loss=0.2016388326883316\n",
      "epoch 40 iter 30 loss=0.15875686705112457\n",
      "epoch 40 iter 31 loss=0.15686747431755066\n",
      "epoch 40 iter 32 loss=0.2737421691417694\n",
      "epoch 40 iter 33 loss=0.19689004123210907\n",
      "epoch 40 iter 34 loss=0.13538968563079834\n",
      "epoch 40 iter 35 loss=0.23607933521270752\n",
      "epoch 40 iter 36 loss=0.10211633145809174\n",
      "epoch 40 iter 37 loss=0.17987963557243347\n",
      "epoch 40 iter 38 loss=0.1509084701538086\n",
      "epoch 40 iter 39 loss=0.11810503154993057\n",
      "epoch 40 iter 40 loss=0.1264951229095459\n",
      "epoch 40 iter 41 loss=0.21971282362937927\n",
      "epoch 40 iter 42 loss=0.18850448727607727\n",
      "epoch 40 iter 43 loss=0.14877638220787048\n",
      "epoch 40 iter 44 loss=0.2018149495124817\n",
      "epoch 40 iter 45 loss=0.11696971952915192\n",
      "epoch 40 iter 46 loss=0.18363316357135773\n",
      "epoch 40 iter 47 loss=0.3266632854938507\n",
      "epoch 40 iter 48 loss=0.18592526018619537\n",
      "epoch 40 iter 49 loss=0.34027501940727234\n",
      "epoch 40 iter 50 loss=0.21190035343170166\n",
      "epoch 40 iter 51 loss=0.14812307059764862\n",
      "epoch 40 iter 52 loss=0.25261634588241577\n",
      "epoch 40 iter 53 loss=0.2123376578092575\n",
      "epoch 40 iter 54 loss=0.1160329282283783\n",
      "epoch 40 iter 55 loss=0.18692952394485474\n",
      "epoch 40 iter 56 loss=0.1286066472530365\n",
      "epoch 40 iter 57 loss=0.10456150770187378\n",
      "epoch 40 iter 58 loss=0.19119277596473694\n",
      "epoch 40 iter 59 loss=0.15366263687610626\n",
      "epoch 40 iter 60 loss=0.13975390791893005\n",
      "epoch 40 iter 61 loss=0.16799543797969818\n",
      "epoch 40 iter 62 loss=0.11875898391008377\n",
      "epoch 40 iter 63 loss=0.2229486107826233\n",
      "epoch 40 iter 64 loss=0.11278964579105377\n",
      "epoch 40 iter 65 loss=0.15131255984306335\n",
      "epoch 40 iter 66 loss=0.1747792661190033\n",
      "epoch 40 iter 67 loss=0.11441712081432343\n",
      "epoch 40 iter 68 loss=0.1843739151954651\n",
      "epoch 40 iter 69 loss=0.12958042323589325\n",
      "epoch 40 iter 70 loss=0.16395896673202515\n",
      "epoch 40 iter 71 loss=0.17057551443576813\n",
      "epoch 40 iter 72 loss=0.19280211627483368\n",
      "epoch 40 iter 73 loss=0.21835321187973022\n",
      "epoch 40 iter 74 loss=0.148263081908226\n",
      "epoch 41 iter 0 loss=0.15595877170562744\n",
      "epoch 41 iter 1 loss=0.08361738920211792\n",
      "epoch 41 iter 2 loss=0.1460546851158142\n",
      "epoch 41 iter 3 loss=0.09530888497829437\n",
      "epoch 41 iter 4 loss=0.13069604337215424\n",
      "epoch 41 iter 5 loss=0.14804308116436005\n",
      "epoch 41 iter 6 loss=0.13425438106060028\n",
      "epoch 41 iter 7 loss=0.10814356803894043\n",
      "epoch 41 iter 8 loss=0.14583903551101685\n",
      "epoch 41 iter 9 loss=0.1436031311750412\n",
      "epoch 41 iter 10 loss=0.16601087152957916\n",
      "epoch 41 iter 11 loss=0.19413059949874878\n",
      "epoch 41 iter 12 loss=0.14269520342350006\n",
      "epoch 41 iter 13 loss=0.25744038820266724\n",
      "epoch 41 iter 14 loss=0.1251717507839203\n",
      "epoch 41 iter 15 loss=0.13805486261844635\n",
      "epoch 41 iter 16 loss=0.14638672769069672\n",
      "epoch 41 iter 17 loss=0.0889117494225502\n",
      "epoch 41 iter 18 loss=0.17028997838497162\n",
      "epoch 41 iter 19 loss=0.10610808432102203\n",
      "epoch 41 iter 20 loss=0.10055188089609146\n",
      "epoch 41 iter 21 loss=0.27250364422798157\n",
      "epoch 41 iter 22 loss=0.1239863708615303\n",
      "epoch 41 iter 23 loss=0.11618679761886597\n",
      "epoch 41 iter 24 loss=0.14624615013599396\n",
      "epoch 41 iter 25 loss=0.2450120449066162\n",
      "epoch 41 iter 26 loss=0.19865860044956207\n",
      "epoch 41 iter 27 loss=0.15378904342651367\n",
      "epoch 41 iter 28 loss=0.10520254075527191\n",
      "epoch 41 iter 29 loss=0.1603589802980423\n",
      "epoch 41 iter 30 loss=0.11645667254924774\n",
      "epoch 41 iter 31 loss=0.15670950710773468\n",
      "epoch 41 iter 32 loss=0.15455543994903564\n",
      "epoch 41 iter 33 loss=0.24030883610248566\n",
      "epoch 41 iter 34 loss=0.1111881285905838\n",
      "epoch 41 iter 35 loss=0.13323290646076202\n",
      "epoch 41 iter 36 loss=0.1020686998963356\n",
      "epoch 41 iter 37 loss=0.11003630608320236\n",
      "epoch 41 iter 38 loss=0.21438392996788025\n",
      "epoch 41 iter 39 loss=0.12377890944480896\n",
      "epoch 41 iter 40 loss=0.15522681176662445\n",
      "epoch 41 iter 41 loss=0.1400633007287979\n",
      "epoch 41 iter 42 loss=0.19250164926052094\n",
      "epoch 41 iter 43 loss=0.14077967405319214\n",
      "epoch 41 iter 44 loss=0.1676289141178131\n",
      "epoch 41 iter 45 loss=0.1506882756948471\n",
      "epoch 41 iter 46 loss=0.17177125811576843\n",
      "epoch 41 iter 47 loss=0.21222954988479614\n",
      "epoch 41 iter 48 loss=0.17458602786064148\n",
      "epoch 41 iter 49 loss=0.1803789883852005\n",
      "epoch 41 iter 50 loss=0.1381942331790924\n",
      "epoch 41 iter 51 loss=0.11961038410663605\n",
      "epoch 41 iter 52 loss=0.2865026295185089\n",
      "epoch 41 iter 53 loss=0.11973011493682861\n",
      "epoch 41 iter 54 loss=0.304004430770874\n",
      "epoch 41 iter 55 loss=0.10725487023591995\n",
      "epoch 41 iter 56 loss=0.12898385524749756\n",
      "epoch 41 iter 57 loss=0.17583200335502625\n",
      "epoch 41 iter 58 loss=0.19211067259311676\n",
      "epoch 41 iter 59 loss=0.1939178705215454\n",
      "epoch 41 iter 60 loss=0.17016860842704773\n",
      "epoch 41 iter 61 loss=0.22152234613895416\n",
      "epoch 41 iter 62 loss=0.12827928364276886\n",
      "epoch 41 iter 63 loss=0.1688617467880249\n",
      "epoch 41 iter 64 loss=0.1698935478925705\n",
      "epoch 41 iter 65 loss=0.09155310690402985\n",
      "epoch 41 iter 66 loss=0.16087967157363892\n",
      "epoch 41 iter 67 loss=0.21631616353988647\n",
      "epoch 41 iter 68 loss=0.19864174723625183\n",
      "epoch 41 iter 69 loss=0.21064427495002747\n",
      "epoch 41 iter 70 loss=0.1488257199525833\n",
      "epoch 41 iter 71 loss=0.5427091717720032\n",
      "epoch 41 iter 72 loss=0.15523913502693176\n",
      "epoch 41 iter 73 loss=0.16683469712734222\n",
      "epoch 41 iter 74 loss=0.12949004769325256\n",
      "epoch 42 iter 0 loss=0.13663895428180695\n",
      "epoch 42 iter 1 loss=0.17594438791275024\n",
      "epoch 42 iter 2 loss=0.24762968719005585\n",
      "epoch 42 iter 3 loss=0.20223984122276306\n",
      "epoch 42 iter 4 loss=0.180138498544693\n",
      "epoch 42 iter 5 loss=0.16195236146450043\n",
      "epoch 42 iter 6 loss=0.12175868451595306\n",
      "epoch 42 iter 7 loss=0.09107883274555206\n",
      "epoch 42 iter 8 loss=0.1620747447013855\n",
      "epoch 42 iter 9 loss=0.18967165052890778\n",
      "epoch 42 iter 10 loss=0.13084006309509277\n",
      "epoch 42 iter 11 loss=0.2851182520389557\n",
      "epoch 42 iter 12 loss=0.21333979070186615\n",
      "epoch 42 iter 13 loss=0.1244109570980072\n",
      "epoch 42 iter 14 loss=0.11381980776786804\n",
      "epoch 42 iter 15 loss=0.21855302155017853\n",
      "epoch 42 iter 16 loss=0.1002519354224205\n",
      "epoch 42 iter 17 loss=0.1555609554052353\n",
      "epoch 42 iter 18 loss=0.12177233397960663\n",
      "epoch 42 iter 19 loss=0.19848883152008057\n",
      "epoch 42 iter 20 loss=0.15647974610328674\n",
      "epoch 42 iter 21 loss=0.18218638002872467\n",
      "epoch 42 iter 22 loss=0.08145176619291306\n",
      "epoch 42 iter 23 loss=0.21793632209300995\n",
      "epoch 42 iter 24 loss=0.4126996099948883\n",
      "epoch 42 iter 25 loss=0.08595804125070572\n",
      "epoch 42 iter 26 loss=0.18240946531295776\n",
      "epoch 42 iter 27 loss=0.14504502713680267\n",
      "epoch 42 iter 28 loss=0.11265987902879715\n",
      "epoch 42 iter 29 loss=0.14548727869987488\n",
      "epoch 42 iter 30 loss=0.20167219638824463\n",
      "epoch 42 iter 31 loss=0.17613746225833893\n",
      "epoch 42 iter 32 loss=0.154928058385849\n",
      "epoch 42 iter 33 loss=0.11903178691864014\n",
      "epoch 42 iter 34 loss=0.12261079996824265\n",
      "epoch 42 iter 35 loss=0.1764010637998581\n",
      "epoch 42 iter 36 loss=0.13429351150989532\n",
      "epoch 42 iter 37 loss=0.16509686410427094\n",
      "epoch 42 iter 38 loss=0.13060523569583893\n",
      "epoch 42 iter 39 loss=0.208528533577919\n",
      "epoch 42 iter 40 loss=0.37485042214393616\n",
      "epoch 42 iter 41 loss=0.2488647848367691\n",
      "epoch 42 iter 42 loss=0.15882520377635956\n",
      "epoch 42 iter 43 loss=0.1739511787891388\n",
      "epoch 42 iter 44 loss=0.20962022244930267\n",
      "epoch 42 iter 45 loss=0.3554232716560364\n",
      "epoch 42 iter 46 loss=0.12159822881221771\n",
      "epoch 42 iter 47 loss=0.15200991928577423\n",
      "epoch 42 iter 48 loss=0.1661285012960434\n",
      "epoch 42 iter 49 loss=0.30774110555648804\n",
      "epoch 42 iter 50 loss=0.141041100025177\n",
      "epoch 42 iter 51 loss=0.14317414164543152\n",
      "epoch 42 iter 52 loss=0.20762477815151215\n",
      "epoch 42 iter 53 loss=0.16295994818210602\n",
      "epoch 42 iter 54 loss=0.14722265303134918\n",
      "epoch 42 iter 55 loss=0.10780119895935059\n",
      "epoch 42 iter 56 loss=0.13139460980892181\n",
      "epoch 42 iter 57 loss=0.15544578433036804\n",
      "epoch 42 iter 58 loss=0.09993256628513336\n",
      "epoch 42 iter 59 loss=0.14678648114204407\n",
      "epoch 42 iter 60 loss=0.1650833636522293\n",
      "epoch 42 iter 61 loss=0.0963650494813919\n",
      "epoch 42 iter 62 loss=0.13785560429096222\n",
      "epoch 42 iter 63 loss=0.10585873574018478\n",
      "epoch 42 iter 64 loss=0.25207436084747314\n",
      "epoch 42 iter 65 loss=0.15206098556518555\n",
      "epoch 42 iter 66 loss=0.20111055672168732\n",
      "epoch 42 iter 67 loss=0.20369932055473328\n",
      "epoch 42 iter 68 loss=0.16445183753967285\n",
      "epoch 42 iter 69 loss=0.16149652004241943\n",
      "epoch 42 iter 70 loss=0.17846375703811646\n",
      "epoch 42 iter 71 loss=0.1521095484495163\n",
      "epoch 42 iter 72 loss=0.228533074259758\n",
      "epoch 42 iter 73 loss=0.22039684653282166\n",
      "epoch 42 iter 74 loss=0.09467839449644089\n",
      "epoch 43 iter 0 loss=0.18533949553966522\n",
      "epoch 43 iter 1 loss=0.15537129342556\n",
      "epoch 43 iter 2 loss=0.11839473247528076\n",
      "epoch 43 iter 3 loss=0.11809046566486359\n",
      "epoch 43 iter 4 loss=0.13661116361618042\n",
      "epoch 43 iter 5 loss=0.10038048774003983\n",
      "epoch 43 iter 6 loss=0.08727418631315231\n",
      "epoch 43 iter 7 loss=0.11213099956512451\n",
      "epoch 43 iter 8 loss=0.13696879148483276\n",
      "epoch 43 iter 9 loss=0.20434780418872833\n",
      "epoch 43 iter 10 loss=0.14540080726146698\n",
      "epoch 43 iter 11 loss=0.16365869343280792\n",
      "epoch 43 iter 12 loss=0.14661654829978943\n",
      "epoch 43 iter 13 loss=0.1796363890171051\n",
      "epoch 43 iter 14 loss=0.13061343133449554\n",
      "epoch 43 iter 15 loss=0.1760452836751938\n",
      "epoch 43 iter 16 loss=0.18749691545963287\n",
      "epoch 43 iter 17 loss=0.17441655695438385\n",
      "epoch 43 iter 18 loss=0.14938496053218842\n",
      "epoch 43 iter 19 loss=0.2257167249917984\n",
      "epoch 43 iter 20 loss=0.13924914598464966\n",
      "epoch 43 iter 21 loss=0.11635779589414597\n",
      "epoch 43 iter 22 loss=0.1333887130022049\n",
      "epoch 43 iter 23 loss=0.2390182614326477\n",
      "epoch 43 iter 24 loss=0.1353973001241684\n",
      "epoch 43 iter 25 loss=0.08856651186943054\n",
      "epoch 43 iter 26 loss=0.17395642399787903\n",
      "epoch 43 iter 27 loss=0.23473086953163147\n",
      "epoch 43 iter 28 loss=0.11239886283874512\n",
      "epoch 43 iter 29 loss=0.22879666090011597\n",
      "epoch 43 iter 30 loss=0.22448837757110596\n",
      "epoch 43 iter 31 loss=0.10781896859407425\n",
      "epoch 43 iter 32 loss=0.18898625671863556\n",
      "epoch 43 iter 33 loss=0.11608351767063141\n",
      "epoch 43 iter 34 loss=0.12469237297773361\n",
      "epoch 43 iter 35 loss=0.13661350309848785\n",
      "epoch 43 iter 36 loss=0.14465267956256866\n",
      "epoch 43 iter 37 loss=0.08496707677841187\n",
      "epoch 43 iter 38 loss=0.08789793401956558\n",
      "epoch 43 iter 39 loss=0.10927101224660873\n",
      "epoch 43 iter 40 loss=0.2894676923751831\n",
      "epoch 43 iter 41 loss=0.15551455318927765\n",
      "epoch 43 iter 42 loss=0.17091457545757294\n",
      "epoch 43 iter 43 loss=0.20425987243652344\n",
      "epoch 43 iter 44 loss=0.14182379841804504\n",
      "epoch 43 iter 45 loss=0.12081553041934967\n",
      "epoch 43 iter 46 loss=0.14005015790462494\n",
      "epoch 43 iter 47 loss=0.1932302862405777\n",
      "epoch 43 iter 48 loss=0.15096484124660492\n",
      "epoch 43 iter 49 loss=0.2264929711818695\n",
      "epoch 43 iter 50 loss=0.1312938928604126\n",
      "epoch 43 iter 51 loss=0.16129882633686066\n",
      "epoch 43 iter 52 loss=0.1903284788131714\n",
      "epoch 43 iter 53 loss=0.163378044962883\n",
      "epoch 43 iter 54 loss=0.28275078535079956\n",
      "epoch 43 iter 55 loss=0.16675809025764465\n",
      "epoch 43 iter 56 loss=0.13380196690559387\n",
      "epoch 43 iter 57 loss=0.16536378860473633\n",
      "epoch 43 iter 58 loss=0.198993980884552\n",
      "epoch 43 iter 59 loss=0.17936690151691437\n",
      "epoch 43 iter 60 loss=0.16684795916080475\n",
      "epoch 43 iter 61 loss=0.12262167036533356\n",
      "epoch 43 iter 62 loss=0.12419506162405014\n",
      "epoch 43 iter 63 loss=0.19297455251216888\n",
      "epoch 43 iter 64 loss=0.08131296187639236\n",
      "epoch 43 iter 65 loss=0.16335967183113098\n",
      "epoch 43 iter 66 loss=0.1289069503545761\n",
      "epoch 43 iter 67 loss=0.08242775499820709\n",
      "epoch 43 iter 68 loss=0.1840728223323822\n",
      "epoch 43 iter 69 loss=0.08766845613718033\n",
      "epoch 43 iter 70 loss=0.16095088422298431\n",
      "epoch 43 iter 71 loss=0.11021999269723892\n",
      "epoch 43 iter 72 loss=0.1547522097826004\n",
      "epoch 43 iter 73 loss=0.1114879623055458\n",
      "epoch 43 iter 74 loss=0.13124807178974152\n",
      "epoch 44 iter 0 loss=0.131769061088562\n",
      "epoch 44 iter 1 loss=0.1649920493364334\n",
      "epoch 44 iter 2 loss=0.1504276543855667\n",
      "epoch 44 iter 3 loss=0.10051180422306061\n",
      "epoch 44 iter 4 loss=0.07981324195861816\n",
      "epoch 44 iter 5 loss=0.12158288061618805\n",
      "epoch 44 iter 6 loss=0.10731805115938187\n",
      "epoch 44 iter 7 loss=0.15314342081546783\n",
      "epoch 44 iter 8 loss=0.09485233575105667\n",
      "epoch 44 iter 9 loss=0.1547551453113556\n",
      "epoch 44 iter 10 loss=0.09542755782604218\n",
      "epoch 44 iter 11 loss=0.08311988413333893\n",
      "epoch 44 iter 12 loss=0.1354144662618637\n",
      "epoch 44 iter 13 loss=0.13705427944660187\n",
      "epoch 44 iter 14 loss=0.13745471835136414\n",
      "epoch 44 iter 15 loss=0.10269045829772949\n",
      "epoch 44 iter 16 loss=0.16796475648880005\n",
      "epoch 44 iter 17 loss=0.1600547432899475\n",
      "epoch 44 iter 18 loss=0.23287096619606018\n",
      "epoch 44 iter 19 loss=0.1262492537498474\n",
      "epoch 44 iter 20 loss=0.12747901678085327\n",
      "epoch 44 iter 21 loss=0.09524141997098923\n",
      "epoch 44 iter 22 loss=0.18189141154289246\n",
      "epoch 44 iter 23 loss=0.12011528760194778\n",
      "epoch 44 iter 24 loss=0.11249462515115738\n",
      "epoch 44 iter 25 loss=0.20879213511943817\n",
      "epoch 44 iter 26 loss=0.18685433268547058\n",
      "epoch 44 iter 27 loss=0.15929186344146729\n",
      "epoch 44 iter 28 loss=0.19401651620864868\n",
      "epoch 44 iter 29 loss=0.13194355368614197\n",
      "epoch 44 iter 30 loss=0.15527884662151337\n",
      "epoch 44 iter 31 loss=0.14914502203464508\n",
      "epoch 44 iter 32 loss=0.12409677356481552\n",
      "epoch 44 iter 33 loss=0.1809784471988678\n",
      "epoch 44 iter 34 loss=0.1694088727235794\n",
      "epoch 44 iter 35 loss=0.13398468494415283\n",
      "epoch 44 iter 36 loss=0.1794084906578064\n",
      "epoch 44 iter 37 loss=0.09837418049573898\n",
      "epoch 44 iter 38 loss=0.13323219120502472\n",
      "epoch 44 iter 39 loss=0.24850693345069885\n",
      "epoch 44 iter 40 loss=0.0716935470700264\n",
      "epoch 44 iter 41 loss=0.19179558753967285\n",
      "epoch 44 iter 42 loss=0.12500348687171936\n",
      "epoch 44 iter 43 loss=0.20174290239810944\n",
      "epoch 44 iter 44 loss=0.20048487186431885\n",
      "epoch 44 iter 45 loss=0.1304648369550705\n",
      "epoch 44 iter 46 loss=0.17328350245952606\n",
      "epoch 44 iter 47 loss=0.21871933341026306\n",
      "epoch 44 iter 48 loss=0.10928313434123993\n",
      "epoch 44 iter 49 loss=0.11470703035593033\n",
      "epoch 44 iter 50 loss=0.08759178966283798\n",
      "epoch 44 iter 51 loss=0.09830722212791443\n",
      "epoch 44 iter 52 loss=0.12279949337244034\n",
      "epoch 44 iter 53 loss=0.19722895324230194\n",
      "epoch 44 iter 54 loss=0.13524176180362701\n",
      "epoch 44 iter 55 loss=0.12724047899246216\n",
      "epoch 44 iter 56 loss=0.0862504094839096\n",
      "epoch 44 iter 57 loss=0.09789766371250153\n",
      "epoch 44 iter 58 loss=0.10350386798381805\n",
      "epoch 44 iter 59 loss=0.11229321360588074\n",
      "epoch 44 iter 60 loss=0.12541675567626953\n",
      "epoch 44 iter 61 loss=0.14346195757389069\n",
      "epoch 44 iter 62 loss=0.23509453237056732\n",
      "epoch 44 iter 63 loss=0.12938368320465088\n",
      "epoch 44 iter 64 loss=0.17394788563251495\n",
      "epoch 44 iter 65 loss=0.10489295423030853\n",
      "epoch 44 iter 66 loss=0.13345693051815033\n",
      "epoch 44 iter 67 loss=0.11762885004281998\n",
      "epoch 44 iter 68 loss=0.1206112951040268\n",
      "epoch 44 iter 69 loss=0.08835090696811676\n",
      "epoch 44 iter 70 loss=0.19281823933124542\n",
      "epoch 44 iter 71 loss=0.12393803894519806\n",
      "epoch 44 iter 72 loss=0.0999329462647438\n",
      "epoch 44 iter 73 loss=0.19861865043640137\n",
      "epoch 44 iter 74 loss=0.2751290500164032\n",
      "epoch 45 iter 0 loss=0.15194697678089142\n",
      "epoch 45 iter 1 loss=0.17695654928684235\n",
      "epoch 45 iter 2 loss=0.10099329054355621\n",
      "epoch 45 iter 3 loss=0.07659909874200821\n",
      "epoch 45 iter 4 loss=0.1461857110261917\n",
      "epoch 45 iter 5 loss=0.08313021063804626\n",
      "epoch 45 iter 6 loss=0.20655307173728943\n",
      "epoch 45 iter 7 loss=0.07407165318727493\n",
      "epoch 45 iter 8 loss=0.10318384319543839\n",
      "epoch 45 iter 9 loss=0.08547911792993546\n",
      "epoch 45 iter 10 loss=0.14360933005809784\n",
      "epoch 45 iter 11 loss=0.15226586163043976\n",
      "epoch 45 iter 12 loss=0.12935897707939148\n",
      "epoch 45 iter 13 loss=0.18398573994636536\n",
      "epoch 45 iter 14 loss=0.14228908717632294\n",
      "epoch 45 iter 15 loss=0.09126095473766327\n",
      "epoch 45 iter 16 loss=0.19118499755859375\n",
      "epoch 45 iter 17 loss=0.0963558703660965\n",
      "epoch 45 iter 18 loss=0.0994829311966896\n",
      "epoch 45 iter 19 loss=0.09951991587877274\n",
      "epoch 45 iter 20 loss=0.1503952443599701\n",
      "epoch 45 iter 21 loss=0.07963048666715622\n",
      "epoch 45 iter 22 loss=0.1026969626545906\n",
      "epoch 45 iter 23 loss=0.14484167098999023\n",
      "epoch 45 iter 24 loss=0.12320265173912048\n",
      "epoch 45 iter 25 loss=0.16707272827625275\n",
      "epoch 45 iter 26 loss=0.12005910277366638\n",
      "epoch 45 iter 27 loss=0.12906868755817413\n",
      "epoch 45 iter 28 loss=0.15220162272453308\n",
      "epoch 45 iter 29 loss=0.11786934733390808\n",
      "epoch 45 iter 30 loss=0.10087177157402039\n",
      "epoch 45 iter 31 loss=0.1610298752784729\n",
      "epoch 45 iter 32 loss=0.09114262461662292\n",
      "epoch 45 iter 33 loss=0.1242053434252739\n",
      "epoch 45 iter 34 loss=0.12031052261590958\n",
      "epoch 45 iter 35 loss=0.08046884089708328\n",
      "epoch 45 iter 36 loss=0.08767921477556229\n",
      "epoch 45 iter 37 loss=0.08958934247493744\n",
      "epoch 45 iter 38 loss=0.11296738684177399\n",
      "epoch 45 iter 39 loss=0.1292596161365509\n",
      "epoch 45 iter 40 loss=0.136968731880188\n",
      "epoch 45 iter 41 loss=0.1155686229467392\n",
      "epoch 45 iter 42 loss=0.07185812294483185\n",
      "epoch 45 iter 43 loss=0.1586131453514099\n",
      "epoch 45 iter 44 loss=0.12648504972457886\n",
      "epoch 45 iter 45 loss=0.19284899532794952\n",
      "epoch 45 iter 46 loss=0.21881255507469177\n",
      "epoch 45 iter 47 loss=0.11095988005399704\n",
      "epoch 45 iter 48 loss=0.09322093427181244\n",
      "epoch 45 iter 49 loss=0.16978099942207336\n",
      "epoch 45 iter 50 loss=0.08907700330018997\n",
      "epoch 45 iter 51 loss=0.14671649038791656\n",
      "epoch 45 iter 52 loss=0.14868377149105072\n",
      "epoch 45 iter 53 loss=0.08377431333065033\n",
      "epoch 45 iter 54 loss=0.10650589317083359\n",
      "epoch 45 iter 55 loss=0.13575679063796997\n",
      "epoch 45 iter 56 loss=0.1640363335609436\n",
      "epoch 45 iter 57 loss=0.1936214715242386\n",
      "epoch 45 iter 58 loss=0.08764190971851349\n",
      "epoch 45 iter 59 loss=0.23205117881298065\n",
      "epoch 45 iter 60 loss=0.127669557929039\n",
      "epoch 45 iter 61 loss=0.1046198233962059\n",
      "epoch 45 iter 62 loss=0.10373923182487488\n",
      "epoch 45 iter 63 loss=0.08834969997406006\n",
      "epoch 45 iter 64 loss=0.15412983298301697\n",
      "epoch 45 iter 65 loss=0.1208583265542984\n",
      "epoch 45 iter 66 loss=0.2163410633802414\n",
      "epoch 45 iter 67 loss=0.18663901090621948\n",
      "epoch 45 iter 68 loss=0.12814390659332275\n",
      "epoch 45 iter 69 loss=0.10313159972429276\n",
      "epoch 45 iter 70 loss=0.10890073329210281\n",
      "epoch 45 iter 71 loss=0.11222007125616074\n",
      "epoch 45 iter 72 loss=0.1381910741329193\n",
      "epoch 45 iter 73 loss=0.08123469352722168\n",
      "epoch 45 iter 74 loss=0.16008317470550537\n",
      "epoch 46 iter 0 loss=0.16171523928642273\n",
      "epoch 46 iter 1 loss=0.11882691830396652\n",
      "epoch 46 iter 2 loss=0.19196893274784088\n",
      "epoch 46 iter 3 loss=0.10983914136886597\n",
      "epoch 46 iter 4 loss=0.08111602067947388\n",
      "epoch 46 iter 5 loss=0.10337396711111069\n",
      "epoch 46 iter 6 loss=0.14909997582435608\n",
      "epoch 46 iter 7 loss=0.09768754988908768\n",
      "epoch 46 iter 8 loss=0.1247749999165535\n",
      "epoch 46 iter 9 loss=0.09108691662549973\n",
      "epoch 46 iter 10 loss=0.14320920407772064\n",
      "epoch 46 iter 11 loss=0.11526231467723846\n",
      "epoch 46 iter 12 loss=0.08388909697532654\n",
      "epoch 46 iter 13 loss=0.12737904489040375\n",
      "epoch 46 iter 14 loss=0.1358644962310791\n",
      "epoch 46 iter 15 loss=0.11908415704965591\n",
      "epoch 46 iter 16 loss=0.12311426550149918\n",
      "epoch 46 iter 17 loss=0.08812273293733597\n",
      "epoch 46 iter 18 loss=0.09402372688055038\n",
      "epoch 46 iter 19 loss=0.18124833703041077\n",
      "epoch 46 iter 20 loss=0.1787492036819458\n",
      "epoch 46 iter 21 loss=0.1340838223695755\n",
      "epoch 46 iter 22 loss=0.1202612891793251\n",
      "epoch 46 iter 23 loss=0.16907662153244019\n",
      "epoch 46 iter 24 loss=0.09558594971895218\n",
      "epoch 46 iter 25 loss=0.09996643662452698\n",
      "epoch 46 iter 26 loss=0.1225622370839119\n",
      "epoch 46 iter 27 loss=0.10837940871715546\n",
      "epoch 46 iter 28 loss=0.17573127150535583\n",
      "epoch 46 iter 29 loss=0.10295794159173965\n",
      "epoch 46 iter 30 loss=0.12004122138023376\n",
      "epoch 46 iter 31 loss=0.10842517763376236\n",
      "epoch 46 iter 32 loss=0.08557301759719849\n",
      "epoch 46 iter 33 loss=0.15154913067817688\n",
      "epoch 46 iter 34 loss=0.12292798608541489\n",
      "epoch 46 iter 35 loss=0.07979562878608704\n",
      "epoch 46 iter 36 loss=0.15864156186580658\n",
      "epoch 46 iter 37 loss=0.19302253425121307\n",
      "epoch 46 iter 38 loss=0.09431064128875732\n",
      "epoch 46 iter 39 loss=0.11865729838609695\n",
      "epoch 46 iter 40 loss=0.10514844208955765\n",
      "epoch 46 iter 41 loss=0.10442941635847092\n",
      "epoch 46 iter 42 loss=0.16566886007785797\n",
      "epoch 46 iter 43 loss=0.08973681181669235\n",
      "epoch 46 iter 44 loss=0.12859615683555603\n",
      "epoch 46 iter 45 loss=0.1807953119277954\n",
      "epoch 46 iter 46 loss=0.12558570504188538\n",
      "epoch 46 iter 47 loss=0.10678064823150635\n",
      "epoch 46 iter 48 loss=0.1025373637676239\n",
      "epoch 46 iter 49 loss=0.0877084732055664\n",
      "epoch 46 iter 50 loss=0.09873052686452866\n",
      "epoch 46 iter 51 loss=0.1290646344423294\n",
      "epoch 46 iter 52 loss=0.29165002703666687\n",
      "epoch 46 iter 53 loss=0.1262638419866562\n",
      "epoch 46 iter 54 loss=0.09352993965148926\n",
      "epoch 46 iter 55 loss=0.0907110869884491\n",
      "epoch 46 iter 56 loss=0.13348262012004852\n",
      "epoch 46 iter 57 loss=0.09060125797986984\n",
      "epoch 46 iter 58 loss=0.11809707432985306\n",
      "epoch 46 iter 59 loss=0.14684541523456573\n",
      "epoch 46 iter 60 loss=0.11895094811916351\n",
      "epoch 46 iter 61 loss=0.16911853849887848\n",
      "epoch 46 iter 62 loss=0.16920635104179382\n",
      "epoch 46 iter 63 loss=0.09027673304080963\n",
      "epoch 46 iter 64 loss=0.08390679210424423\n",
      "epoch 46 iter 65 loss=0.16933633387088776\n",
      "epoch 46 iter 66 loss=0.1475195288658142\n",
      "epoch 46 iter 67 loss=0.10329212993383408\n",
      "epoch 46 iter 68 loss=0.16199101507663727\n",
      "epoch 46 iter 69 loss=0.13263845443725586\n",
      "epoch 46 iter 70 loss=0.11490394175052643\n",
      "epoch 46 iter 71 loss=0.19206324219703674\n",
      "epoch 46 iter 72 loss=0.08385337144136429\n",
      "epoch 46 iter 73 loss=0.12220664322376251\n",
      "epoch 46 iter 74 loss=0.10261326283216476\n",
      "epoch 47 iter 0 loss=0.1401073932647705\n",
      "epoch 47 iter 1 loss=0.08585016429424286\n",
      "epoch 47 iter 2 loss=0.07806576043367386\n",
      "epoch 47 iter 3 loss=0.147895947098732\n",
      "epoch 47 iter 4 loss=0.10524676740169525\n",
      "epoch 47 iter 5 loss=0.15925005078315735\n",
      "epoch 47 iter 6 loss=0.22545351088047028\n",
      "epoch 47 iter 7 loss=0.14593902230262756\n",
      "epoch 47 iter 8 loss=0.11385156959295273\n",
      "epoch 47 iter 9 loss=0.11918838322162628\n",
      "epoch 47 iter 10 loss=0.1131710484623909\n",
      "epoch 47 iter 11 loss=0.11580673605203629\n",
      "epoch 47 iter 12 loss=0.23942482471466064\n",
      "epoch 47 iter 13 loss=0.08636140823364258\n",
      "epoch 47 iter 14 loss=0.10928874462842941\n",
      "epoch 47 iter 15 loss=0.10242735594511032\n",
      "epoch 47 iter 16 loss=0.08491356670856476\n",
      "epoch 47 iter 17 loss=0.15644268691539764\n",
      "epoch 47 iter 18 loss=0.13554617762565613\n",
      "epoch 47 iter 19 loss=0.08068960905075073\n",
      "epoch 47 iter 20 loss=0.10821587592363358\n",
      "epoch 47 iter 21 loss=0.10454965382814407\n",
      "epoch 47 iter 22 loss=0.13961684703826904\n",
      "epoch 47 iter 23 loss=0.09318801760673523\n",
      "epoch 47 iter 24 loss=0.15665164589881897\n",
      "epoch 47 iter 25 loss=0.09314766526222229\n",
      "epoch 47 iter 26 loss=0.10091449320316315\n",
      "epoch 47 iter 27 loss=0.14248062670230865\n",
      "epoch 47 iter 28 loss=0.13375219702720642\n",
      "epoch 47 iter 29 loss=0.19738541543483734\n",
      "epoch 47 iter 30 loss=0.11589661240577698\n",
      "epoch 47 iter 31 loss=0.10905569791793823\n",
      "epoch 47 iter 32 loss=0.10647895187139511\n",
      "epoch 47 iter 33 loss=0.15537087619304657\n",
      "epoch 47 iter 34 loss=0.11729961633682251\n",
      "epoch 47 iter 35 loss=0.151121586561203\n",
      "epoch 47 iter 36 loss=0.08926396071910858\n",
      "epoch 47 iter 37 loss=0.09072870761156082\n",
      "epoch 47 iter 38 loss=0.12267108261585236\n",
      "epoch 47 iter 39 loss=0.07743164151906967\n",
      "epoch 47 iter 40 loss=0.13221408426761627\n",
      "epoch 47 iter 41 loss=0.15315775573253632\n",
      "epoch 47 iter 42 loss=0.1451827436685562\n",
      "epoch 47 iter 43 loss=0.15052348375320435\n",
      "epoch 47 iter 44 loss=0.07147061824798584\n",
      "epoch 47 iter 45 loss=0.10335448384284973\n",
      "epoch 47 iter 46 loss=0.157983660697937\n",
      "epoch 47 iter 47 loss=0.13193181157112122\n",
      "epoch 47 iter 48 loss=0.13276104629039764\n",
      "epoch 47 iter 49 loss=0.07980315387248993\n",
      "epoch 47 iter 50 loss=0.09082189202308655\n",
      "epoch 47 iter 51 loss=0.15467824041843414\n",
      "epoch 47 iter 52 loss=0.13301444053649902\n",
      "epoch 47 iter 53 loss=0.11498211324214935\n",
      "epoch 47 iter 54 loss=0.12800073623657227\n",
      "epoch 47 iter 55 loss=0.11456256359815598\n",
      "epoch 47 iter 56 loss=0.2063143402338028\n",
      "epoch 47 iter 57 loss=0.17982284724712372\n",
      "epoch 47 iter 58 loss=0.14639712870121002\n",
      "epoch 47 iter 59 loss=0.11404525488615036\n",
      "epoch 47 iter 60 loss=0.09610515087842941\n",
      "epoch 47 iter 61 loss=0.09424158185720444\n",
      "epoch 47 iter 62 loss=0.13203902542591095\n",
      "epoch 47 iter 63 loss=0.11465118825435638\n",
      "epoch 47 iter 64 loss=0.11062981188297272\n",
      "epoch 47 iter 65 loss=0.10069520026445389\n",
      "epoch 47 iter 66 loss=0.1441604048013687\n",
      "epoch 47 iter 67 loss=0.20210573077201843\n",
      "epoch 47 iter 68 loss=0.13657690584659576\n",
      "epoch 47 iter 69 loss=0.1836884617805481\n",
      "epoch 47 iter 70 loss=0.12556950747966766\n",
      "epoch 47 iter 71 loss=0.07786261290311813\n",
      "epoch 47 iter 72 loss=0.09322798252105713\n",
      "epoch 47 iter 73 loss=0.09862076491117477\n",
      "epoch 47 iter 74 loss=0.18937912583351135\n",
      "epoch 48 iter 0 loss=0.06618378311395645\n",
      "epoch 48 iter 1 loss=0.11577542871236801\n",
      "epoch 48 iter 2 loss=0.130879208445549\n",
      "epoch 48 iter 3 loss=0.17868512868881226\n",
      "epoch 48 iter 4 loss=0.3056064248085022\n",
      "epoch 48 iter 5 loss=0.12395098060369492\n",
      "epoch 48 iter 6 loss=0.17269395291805267\n",
      "epoch 48 iter 7 loss=0.10096746683120728\n",
      "epoch 48 iter 8 loss=0.19186821579933167\n",
      "epoch 48 iter 9 loss=0.1235678419470787\n",
      "epoch 48 iter 10 loss=0.09741488099098206\n",
      "epoch 48 iter 11 loss=0.15498889982700348\n",
      "epoch 48 iter 12 loss=0.08418609946966171\n",
      "epoch 48 iter 13 loss=0.11215575784444809\n",
      "epoch 48 iter 14 loss=0.07943582534790039\n",
      "epoch 48 iter 15 loss=0.09007827192544937\n",
      "epoch 48 iter 16 loss=0.0856361985206604\n",
      "epoch 48 iter 17 loss=0.07992561161518097\n",
      "epoch 48 iter 18 loss=0.11797796934843063\n",
      "epoch 48 iter 19 loss=0.15603630244731903\n",
      "epoch 48 iter 20 loss=0.08786239475011826\n",
      "epoch 48 iter 21 loss=0.212805837392807\n",
      "epoch 48 iter 22 loss=0.12299898266792297\n",
      "epoch 48 iter 23 loss=0.1761721521615982\n",
      "epoch 48 iter 24 loss=0.13665206730365753\n",
      "epoch 48 iter 25 loss=0.08932045102119446\n",
      "epoch 48 iter 26 loss=0.07931207865476608\n",
      "epoch 48 iter 27 loss=0.14188410341739655\n",
      "epoch 48 iter 28 loss=0.12372809648513794\n",
      "epoch 48 iter 29 loss=0.15968406200408936\n",
      "epoch 48 iter 30 loss=0.13736839592456818\n",
      "epoch 48 iter 31 loss=0.13018019497394562\n",
      "epoch 48 iter 32 loss=0.08703561127185822\n",
      "epoch 48 iter 33 loss=0.1197420060634613\n",
      "epoch 48 iter 34 loss=0.10742238909006119\n",
      "epoch 48 iter 35 loss=0.09033480286598206\n",
      "epoch 48 iter 36 loss=0.19315142929553986\n",
      "epoch 48 iter 37 loss=0.08559910953044891\n",
      "epoch 48 iter 38 loss=0.08388771861791611\n",
      "epoch 48 iter 39 loss=0.15776760876178741\n",
      "epoch 48 iter 40 loss=0.08184905350208282\n",
      "epoch 48 iter 41 loss=0.1934475153684616\n",
      "epoch 48 iter 42 loss=0.2549642026424408\n",
      "epoch 48 iter 43 loss=0.13812600076198578\n",
      "epoch 48 iter 44 loss=0.16225185990333557\n",
      "epoch 48 iter 45 loss=0.119378961622715\n",
      "epoch 48 iter 46 loss=0.07735815644264221\n",
      "epoch 48 iter 47 loss=0.08677167445421219\n",
      "epoch 48 iter 48 loss=0.11271656304597855\n",
      "epoch 48 iter 49 loss=0.10255753248929977\n",
      "epoch 48 iter 50 loss=0.11486002057790756\n",
      "epoch 48 iter 51 loss=0.10838864743709564\n",
      "epoch 48 iter 52 loss=0.09609632939100266\n",
      "epoch 48 iter 53 loss=0.11665498465299606\n",
      "epoch 48 iter 54 loss=0.1135842353105545\n",
      "epoch 48 iter 55 loss=0.10302744805812836\n",
      "epoch 48 iter 56 loss=0.16168217360973358\n",
      "epoch 48 iter 57 loss=0.08865470439195633\n",
      "epoch 48 iter 58 loss=0.13650378584861755\n",
      "epoch 48 iter 59 loss=0.20194187760353088\n",
      "epoch 48 iter 60 loss=0.15122921764850616\n",
      "epoch 48 iter 61 loss=0.06532012671232224\n",
      "epoch 48 iter 62 loss=0.19477149844169617\n",
      "epoch 48 iter 63 loss=0.13999395072460175\n",
      "epoch 48 iter 64 loss=0.08570299297571182\n",
      "epoch 48 iter 65 loss=0.10519664734601974\n",
      "epoch 48 iter 66 loss=0.1353445202112198\n",
      "epoch 48 iter 67 loss=0.11997822672128677\n",
      "epoch 48 iter 68 loss=0.10595735162496567\n",
      "epoch 48 iter 69 loss=0.12112089991569519\n",
      "epoch 48 iter 70 loss=0.14514560997486115\n",
      "epoch 48 iter 71 loss=0.15195070207118988\n",
      "epoch 48 iter 72 loss=0.10807158797979355\n",
      "epoch 48 iter 73 loss=0.16544023156166077\n",
      "epoch 48 iter 74 loss=0.1878253072500229\n",
      "epoch 49 iter 0 loss=0.09751863777637482\n",
      "epoch 49 iter 1 loss=0.15642555058002472\n",
      "epoch 49 iter 2 loss=0.08560130000114441\n",
      "epoch 49 iter 3 loss=0.13645057380199432\n",
      "epoch 49 iter 4 loss=0.12793628871440887\n",
      "epoch 49 iter 5 loss=0.08358115702867508\n",
      "epoch 49 iter 6 loss=0.11260348558425903\n",
      "epoch 49 iter 7 loss=0.064193494617939\n",
      "epoch 49 iter 8 loss=0.2025558054447174\n",
      "epoch 49 iter 9 loss=0.2094804346561432\n",
      "epoch 49 iter 10 loss=0.1373075246810913\n",
      "epoch 49 iter 11 loss=0.19659066200256348\n",
      "epoch 49 iter 12 loss=0.1212591752409935\n",
      "epoch 49 iter 13 loss=0.07446017861366272\n",
      "epoch 49 iter 14 loss=0.1492435783147812\n",
      "epoch 49 iter 15 loss=0.1488988697528839\n",
      "epoch 49 iter 16 loss=0.16029103100299835\n",
      "epoch 49 iter 17 loss=0.1419423669576645\n",
      "epoch 49 iter 18 loss=0.1058761328458786\n",
      "epoch 49 iter 19 loss=0.11429556459188461\n",
      "epoch 49 iter 20 loss=0.13753274083137512\n",
      "epoch 49 iter 21 loss=0.13473184406757355\n",
      "epoch 49 iter 22 loss=0.19452258944511414\n",
      "epoch 49 iter 23 loss=0.19697657227516174\n",
      "epoch 49 iter 24 loss=0.07983025163412094\n",
      "epoch 49 iter 25 loss=0.2005496621131897\n",
      "epoch 49 iter 26 loss=0.09227243810892105\n",
      "epoch 49 iter 27 loss=0.0673966184258461\n",
      "epoch 49 iter 28 loss=0.13818782567977905\n",
      "epoch 49 iter 29 loss=0.1706395447254181\n",
      "epoch 49 iter 30 loss=0.08862629532814026\n",
      "epoch 49 iter 31 loss=0.18035660684108734\n",
      "epoch 49 iter 32 loss=0.09938544034957886\n",
      "epoch 49 iter 33 loss=0.17417101562023163\n",
      "epoch 49 iter 34 loss=0.10130912065505981\n",
      "epoch 49 iter 35 loss=0.1343919336795807\n",
      "epoch 49 iter 36 loss=0.09009801596403122\n",
      "epoch 49 iter 37 loss=0.0700971707701683\n",
      "epoch 49 iter 38 loss=0.10582784563302994\n",
      "epoch 49 iter 39 loss=0.18110434710979462\n",
      "epoch 49 iter 40 loss=0.11977321654558182\n",
      "epoch 49 iter 41 loss=0.13203047215938568\n",
      "epoch 49 iter 42 loss=0.10046027600765228\n",
      "epoch 49 iter 43 loss=0.11617906391620636\n",
      "epoch 49 iter 44 loss=0.10058535635471344\n",
      "epoch 49 iter 45 loss=0.16482007503509521\n",
      "epoch 49 iter 46 loss=0.15716734528541565\n",
      "epoch 49 iter 47 loss=0.17226357758045197\n",
      "epoch 49 iter 48 loss=0.10685877501964569\n",
      "epoch 49 iter 49 loss=0.1179332584142685\n",
      "epoch 49 iter 50 loss=0.1779799461364746\n",
      "epoch 49 iter 51 loss=0.2530378997325897\n",
      "epoch 49 iter 52 loss=0.1000053808093071\n",
      "epoch 49 iter 53 loss=0.14227712154388428\n",
      "epoch 49 iter 54 loss=0.18296827375888824\n",
      "epoch 49 iter 55 loss=0.14210382103919983\n",
      "epoch 49 iter 56 loss=0.15382027626037598\n",
      "epoch 49 iter 57 loss=0.18710684776306152\n",
      "epoch 49 iter 58 loss=0.12530605494976044\n",
      "epoch 49 iter 59 loss=0.09382040798664093\n",
      "epoch 49 iter 60 loss=0.12101814895868301\n",
      "epoch 49 iter 61 loss=0.09653481096029282\n",
      "epoch 49 iter 62 loss=0.11207053810358047\n",
      "epoch 49 iter 63 loss=0.12998846173286438\n",
      "epoch 49 iter 64 loss=0.10369963198900223\n",
      "epoch 49 iter 65 loss=0.12828701734542847\n",
      "epoch 49 iter 66 loss=0.15157482028007507\n",
      "epoch 49 iter 67 loss=0.1096825897693634\n",
      "epoch 49 iter 68 loss=0.16782931983470917\n",
      "epoch 49 iter 69 loss=0.11956324428319931\n",
      "epoch 49 iter 70 loss=0.09694516658782959\n",
      "epoch 49 iter 71 loss=0.09046042710542679\n",
      "epoch 49 iter 72 loss=0.14676813781261444\n",
      "epoch 49 iter 73 loss=0.16130056977272034\n",
      "epoch 49 iter 74 loss=0.2642131447792053\n",
      "epoch 50 iter 0 loss=0.1311405748128891\n",
      "epoch 50 iter 1 loss=0.11385339498519897\n",
      "epoch 50 iter 2 loss=0.11990411579608917\n",
      "epoch 50 iter 3 loss=0.14319175481796265\n",
      "epoch 50 iter 4 loss=0.09578178077936172\n",
      "epoch 50 iter 5 loss=0.10312357544898987\n",
      "epoch 50 iter 6 loss=0.10999742150306702\n",
      "epoch 50 iter 7 loss=0.12531721591949463\n",
      "epoch 50 iter 8 loss=0.09746704995632172\n",
      "epoch 50 iter 9 loss=0.12179165333509445\n",
      "epoch 50 iter 10 loss=0.15856429934501648\n",
      "epoch 50 iter 11 loss=0.16524159908294678\n",
      "epoch 50 iter 12 loss=0.15586912631988525\n",
      "epoch 50 iter 13 loss=0.12467893213033676\n",
      "epoch 50 iter 14 loss=0.10456638038158417\n",
      "epoch 50 iter 15 loss=0.09655537456274033\n",
      "epoch 50 iter 16 loss=0.11018341779708862\n",
      "epoch 50 iter 17 loss=0.11754051595926285\n",
      "epoch 50 iter 18 loss=0.17689630389213562\n",
      "epoch 50 iter 19 loss=0.10164591670036316\n",
      "epoch 50 iter 20 loss=0.0844523161649704\n",
      "epoch 50 iter 21 loss=0.1588854193687439\n",
      "epoch 50 iter 22 loss=0.1857401728630066\n",
      "epoch 50 iter 23 loss=0.09809983521699905\n",
      "epoch 50 iter 24 loss=0.22808429598808289\n",
      "epoch 50 iter 25 loss=0.12667988240718842\n",
      "epoch 50 iter 26 loss=0.12427682429552078\n",
      "epoch 50 iter 27 loss=0.14748741686344147\n",
      "epoch 50 iter 28 loss=0.11165457963943481\n",
      "epoch 50 iter 29 loss=0.13599196076393127\n",
      "epoch 50 iter 30 loss=0.10169854760169983\n",
      "epoch 50 iter 31 loss=0.19561249017715454\n",
      "epoch 50 iter 32 loss=0.19066430628299713\n",
      "epoch 50 iter 33 loss=0.15505088865756989\n",
      "epoch 50 iter 34 loss=0.14747384190559387\n",
      "epoch 50 iter 35 loss=0.10656841844320297\n",
      "epoch 50 iter 36 loss=0.12430740147829056\n",
      "epoch 50 iter 37 loss=0.11653059720993042\n",
      "epoch 50 iter 38 loss=0.1286049336194992\n",
      "epoch 50 iter 39 loss=0.19285783171653748\n",
      "epoch 50 iter 40 loss=0.250844806432724\n",
      "epoch 50 iter 41 loss=0.0878901332616806\n",
      "epoch 50 iter 42 loss=0.09043098986148834\n",
      "epoch 50 iter 43 loss=0.20517368614673615\n",
      "epoch 50 iter 44 loss=0.12588009238243103\n",
      "epoch 50 iter 45 loss=0.1375797539949417\n",
      "epoch 50 iter 46 loss=0.09980112314224243\n",
      "epoch 50 iter 47 loss=0.17164935171604156\n",
      "epoch 50 iter 48 loss=0.12097150832414627\n",
      "epoch 50 iter 49 loss=0.1727401316165924\n",
      "epoch 50 iter 50 loss=0.175796777009964\n",
      "epoch 50 iter 51 loss=0.09791143983602524\n",
      "epoch 50 iter 52 loss=0.10675632953643799\n",
      "epoch 50 iter 53 loss=0.08936803787946701\n",
      "epoch 50 iter 54 loss=0.09809508919715881\n",
      "epoch 50 iter 55 loss=0.22980040311813354\n",
      "epoch 50 iter 56 loss=0.08771568536758423\n",
      "epoch 50 iter 57 loss=0.17485512793064117\n",
      "epoch 50 iter 58 loss=0.10253792256116867\n",
      "epoch 50 iter 59 loss=0.1324557214975357\n",
      "epoch 50 iter 60 loss=0.1311107724905014\n",
      "epoch 50 iter 61 loss=0.16075009107589722\n",
      "epoch 50 iter 62 loss=0.09719225019216537\n",
      "epoch 50 iter 63 loss=0.13152973353862762\n",
      "epoch 50 iter 64 loss=0.19750966131687164\n",
      "epoch 50 iter 65 loss=0.2406948208808899\n",
      "epoch 50 iter 66 loss=0.08572929352521896\n",
      "epoch 50 iter 67 loss=0.1366722583770752\n",
      "epoch 50 iter 68 loss=0.07936662435531616\n",
      "epoch 50 iter 69 loss=0.12645277380943298\n",
      "epoch 50 iter 70 loss=0.1292903572320938\n",
      "epoch 50 iter 71 loss=0.11767255514860153\n",
      "epoch 50 iter 72 loss=0.13841386139392853\n",
      "epoch 50 iter 73 loss=0.13435019552707672\n",
      "epoch 50 iter 74 loss=0.10598142445087433\n",
      "epoch 51 iter 0 loss=0.16868847608566284\n",
      "epoch 51 iter 1 loss=0.15999796986579895\n",
      "epoch 51 iter 2 loss=0.1629769206047058\n",
      "epoch 51 iter 3 loss=0.09897029399871826\n",
      "epoch 51 iter 4 loss=0.16130146384239197\n",
      "epoch 51 iter 5 loss=0.10766454041004181\n",
      "epoch 51 iter 6 loss=0.12435495108366013\n",
      "epoch 51 iter 7 loss=0.18103903532028198\n",
      "epoch 51 iter 8 loss=0.12848429381847382\n",
      "epoch 51 iter 9 loss=0.19435545802116394\n",
      "epoch 51 iter 10 loss=0.22561362385749817\n",
      "epoch 51 iter 11 loss=0.13898305594921112\n",
      "epoch 51 iter 12 loss=0.15056529641151428\n",
      "epoch 51 iter 13 loss=0.11727702617645264\n",
      "epoch 51 iter 14 loss=0.17786464095115662\n",
      "epoch 51 iter 15 loss=0.11424916982650757\n",
      "epoch 51 iter 16 loss=0.11542642116546631\n",
      "epoch 51 iter 17 loss=0.22331640124320984\n",
      "epoch 51 iter 18 loss=0.09698908030986786\n",
      "epoch 51 iter 19 loss=0.09350810199975967\n",
      "epoch 51 iter 20 loss=0.14560578763484955\n",
      "epoch 51 iter 21 loss=0.2673199474811554\n",
      "epoch 51 iter 22 loss=0.13834871351718903\n",
      "epoch 51 iter 23 loss=0.07803191989660263\n",
      "epoch 51 iter 24 loss=0.08910759538412094\n",
      "epoch 51 iter 25 loss=0.08294916898012161\n",
      "epoch 51 iter 26 loss=0.11067642271518707\n",
      "epoch 51 iter 27 loss=0.08493383973836899\n",
      "epoch 51 iter 28 loss=0.1038360744714737\n",
      "epoch 51 iter 29 loss=0.07954858243465424\n",
      "epoch 51 iter 30 loss=0.09449268132448196\n",
      "epoch 51 iter 31 loss=0.1588515341281891\n",
      "epoch 51 iter 32 loss=0.11308837682008743\n",
      "epoch 51 iter 33 loss=0.12981446087360382\n",
      "epoch 51 iter 34 loss=0.13764744997024536\n",
      "epoch 51 iter 35 loss=0.12272219359874725\n",
      "epoch 51 iter 36 loss=0.10170412063598633\n",
      "epoch 51 iter 37 loss=0.11690270155668259\n",
      "epoch 51 iter 38 loss=0.0897403359413147\n",
      "epoch 51 iter 39 loss=0.1527908593416214\n",
      "epoch 51 iter 40 loss=0.09705271571874619\n",
      "epoch 51 iter 41 loss=0.10200619697570801\n",
      "epoch 51 iter 42 loss=0.08396685868501663\n",
      "epoch 51 iter 43 loss=0.1545930802822113\n",
      "epoch 51 iter 44 loss=0.0844123438000679\n",
      "epoch 51 iter 45 loss=0.16211019456386566\n",
      "epoch 51 iter 46 loss=0.09008996933698654\n",
      "epoch 51 iter 47 loss=0.16637110710144043\n",
      "epoch 51 iter 48 loss=0.17357319593429565\n",
      "epoch 51 iter 49 loss=0.11611656099557877\n",
      "epoch 51 iter 50 loss=0.10991082340478897\n",
      "epoch 51 iter 51 loss=0.08446929603815079\n",
      "epoch 51 iter 52 loss=0.07447144389152527\n",
      "epoch 51 iter 53 loss=0.11917635053396225\n",
      "epoch 51 iter 54 loss=0.1454484909772873\n",
      "epoch 51 iter 55 loss=0.14664708077907562\n",
      "epoch 51 iter 56 loss=0.12839674949645996\n",
      "epoch 51 iter 57 loss=0.12630051374435425\n",
      "epoch 51 iter 58 loss=0.13840289413928986\n",
      "epoch 51 iter 59 loss=0.10232876986265182\n",
      "epoch 51 iter 60 loss=0.12743523716926575\n",
      "epoch 51 iter 61 loss=0.07923819869756699\n",
      "epoch 51 iter 62 loss=0.15918664634227753\n",
      "epoch 51 iter 63 loss=0.09328053891658783\n",
      "epoch 51 iter 64 loss=0.08031675219535828\n",
      "epoch 51 iter 65 loss=0.14958345890045166\n",
      "epoch 51 iter 66 loss=0.11710227280855179\n",
      "epoch 51 iter 67 loss=0.08062495291233063\n",
      "epoch 51 iter 68 loss=0.10126817226409912\n",
      "epoch 51 iter 69 loss=0.11775389313697815\n",
      "epoch 51 iter 70 loss=0.06300956010818481\n",
      "epoch 51 iter 71 loss=0.10995356738567352\n",
      "epoch 51 iter 72 loss=0.1141057163476944\n",
      "epoch 51 iter 73 loss=0.11461620777845383\n",
      "epoch 51 iter 74 loss=0.1309771090745926\n",
      "epoch 52 iter 0 loss=0.07907328754663467\n",
      "epoch 52 iter 1 loss=0.07957233488559723\n",
      "epoch 52 iter 2 loss=0.10034408420324326\n",
      "epoch 52 iter 3 loss=0.10310034453868866\n",
      "epoch 52 iter 4 loss=0.2159672975540161\n",
      "epoch 52 iter 5 loss=0.06599463522434235\n",
      "epoch 52 iter 6 loss=0.09262154996395111\n",
      "epoch 52 iter 7 loss=0.13231894373893738\n",
      "epoch 52 iter 8 loss=0.10467885434627533\n",
      "epoch 52 iter 9 loss=0.1532711386680603\n",
      "epoch 52 iter 10 loss=0.1264689415693283\n",
      "epoch 52 iter 11 loss=0.12094902247190475\n",
      "epoch 52 iter 12 loss=0.19350223243236542\n",
      "epoch 52 iter 13 loss=0.11668381094932556\n",
      "epoch 52 iter 14 loss=0.12279526144266129\n",
      "epoch 52 iter 15 loss=0.14651472866535187\n",
      "epoch 52 iter 16 loss=0.13363869488239288\n",
      "epoch 52 iter 17 loss=0.06895403563976288\n",
      "epoch 52 iter 18 loss=0.12194231152534485\n",
      "epoch 52 iter 19 loss=0.07481271028518677\n",
      "epoch 52 iter 20 loss=0.10363824665546417\n",
      "epoch 52 iter 21 loss=0.13297294080257416\n",
      "epoch 52 iter 22 loss=0.10405625402927399\n",
      "epoch 52 iter 23 loss=0.08804572373628616\n",
      "epoch 52 iter 24 loss=0.12381239235401154\n",
      "epoch 52 iter 25 loss=0.11526162922382355\n",
      "epoch 52 iter 26 loss=0.10590957850217819\n",
      "epoch 52 iter 27 loss=0.125251904129982\n",
      "epoch 52 iter 28 loss=0.11498825252056122\n",
      "epoch 52 iter 29 loss=0.14039164781570435\n",
      "epoch 52 iter 30 loss=0.19709517061710358\n",
      "epoch 52 iter 31 loss=0.15927192568778992\n",
      "epoch 52 iter 32 loss=0.1020556092262268\n",
      "epoch 52 iter 33 loss=0.08729051053524017\n",
      "epoch 52 iter 34 loss=0.15587376058101654\n",
      "epoch 52 iter 35 loss=0.09151231497526169\n",
      "epoch 52 iter 36 loss=0.08679868280887604\n",
      "epoch 52 iter 37 loss=0.14306817948818207\n",
      "epoch 52 iter 38 loss=0.11665541678667068\n",
      "epoch 52 iter 39 loss=0.1285826861858368\n",
      "epoch 52 iter 40 loss=0.08281153440475464\n",
      "epoch 52 iter 41 loss=0.17194385826587677\n",
      "epoch 52 iter 42 loss=0.11285006254911423\n",
      "epoch 52 iter 43 loss=0.15162938833236694\n",
      "epoch 52 iter 44 loss=0.1612267792224884\n",
      "epoch 52 iter 45 loss=0.10678888112306595\n",
      "epoch 52 iter 46 loss=0.11485028266906738\n",
      "epoch 52 iter 47 loss=0.08390562236309052\n",
      "epoch 52 iter 48 loss=0.1114407479763031\n",
      "epoch 52 iter 49 loss=0.13265343010425568\n",
      "epoch 52 iter 50 loss=0.1091063842177391\n",
      "epoch 52 iter 51 loss=0.08177461475133896\n",
      "epoch 52 iter 52 loss=0.15834197402000427\n",
      "epoch 52 iter 53 loss=0.09856363385915756\n",
      "epoch 52 iter 54 loss=0.12318667024374008\n",
      "epoch 52 iter 55 loss=0.1127907931804657\n",
      "epoch 52 iter 56 loss=0.09228982031345367\n",
      "epoch 52 iter 57 loss=0.06563757359981537\n",
      "epoch 52 iter 58 loss=0.11008205264806747\n",
      "epoch 52 iter 59 loss=0.08990472555160522\n",
      "epoch 52 iter 60 loss=0.11301638185977936\n",
      "epoch 52 iter 61 loss=0.05885884538292885\n",
      "epoch 52 iter 62 loss=0.1291080266237259\n",
      "epoch 52 iter 63 loss=0.12418105453252792\n",
      "epoch 52 iter 64 loss=0.059522464871406555\n",
      "epoch 52 iter 65 loss=0.10416099429130554\n",
      "epoch 52 iter 66 loss=0.06867089122533798\n",
      "epoch 52 iter 67 loss=0.09037277102470398\n",
      "epoch 52 iter 68 loss=0.12269078940153122\n",
      "epoch 52 iter 69 loss=0.08803027123212814\n",
      "epoch 52 iter 70 loss=0.07906841486692429\n",
      "epoch 52 iter 71 loss=0.08130545169115067\n",
      "epoch 52 iter 72 loss=0.1567876785993576\n",
      "epoch 52 iter 73 loss=0.11161400377750397\n",
      "epoch 52 iter 74 loss=0.10027006268501282\n",
      "epoch 53 iter 0 loss=0.1038474440574646\n",
      "epoch 53 iter 1 loss=0.08607971668243408\n",
      "epoch 53 iter 2 loss=0.11003405600786209\n",
      "epoch 53 iter 3 loss=0.18877066671848297\n",
      "epoch 53 iter 4 loss=0.1831575632095337\n",
      "epoch 53 iter 5 loss=0.1106870174407959\n",
      "epoch 53 iter 6 loss=0.07310512661933899\n",
      "epoch 53 iter 7 loss=0.14639638364315033\n",
      "epoch 53 iter 8 loss=0.09789527952671051\n",
      "epoch 53 iter 9 loss=0.09323053807020187\n",
      "epoch 53 iter 10 loss=0.13657797873020172\n",
      "epoch 53 iter 11 loss=0.08002687245607376\n",
      "epoch 53 iter 12 loss=0.09982885420322418\n",
      "epoch 53 iter 13 loss=0.09745793044567108\n",
      "epoch 53 iter 14 loss=0.08270161598920822\n",
      "epoch 53 iter 15 loss=0.13702043890953064\n",
      "epoch 53 iter 16 loss=0.07592292875051498\n",
      "epoch 53 iter 17 loss=0.08960529416799545\n",
      "epoch 53 iter 18 loss=0.13508754968643188\n",
      "epoch 53 iter 19 loss=0.09262051433324814\n",
      "epoch 53 iter 20 loss=0.1819509118795395\n",
      "epoch 53 iter 21 loss=0.12400732189416885\n",
      "epoch 53 iter 22 loss=0.09881650656461716\n",
      "epoch 53 iter 23 loss=0.14647932350635529\n",
      "epoch 53 iter 24 loss=0.09336686134338379\n",
      "epoch 53 iter 25 loss=0.10164312273263931\n",
      "epoch 53 iter 26 loss=0.10276288539171219\n",
      "epoch 53 iter 27 loss=0.10373345762491226\n",
      "epoch 53 iter 28 loss=0.0932590588927269\n",
      "epoch 53 iter 29 loss=0.08282976597547531\n",
      "epoch 53 iter 30 loss=0.17295101284980774\n",
      "epoch 53 iter 31 loss=0.10463229566812515\n",
      "epoch 53 iter 32 loss=0.14813922345638275\n",
      "epoch 53 iter 33 loss=0.15810956060886383\n",
      "epoch 53 iter 34 loss=0.08912505209445953\n",
      "epoch 53 iter 35 loss=0.09932363778352737\n",
      "epoch 53 iter 36 loss=0.13574591279029846\n",
      "epoch 53 iter 37 loss=0.08546891808509827\n",
      "epoch 53 iter 38 loss=0.12878040969371796\n",
      "epoch 53 iter 39 loss=0.07650991529226303\n",
      "epoch 53 iter 40 loss=0.10912657529115677\n",
      "epoch 53 iter 41 loss=0.10541803389787674\n",
      "epoch 53 iter 42 loss=0.13231982290744781\n",
      "epoch 53 iter 43 loss=0.09435930103063583\n",
      "epoch 53 iter 44 loss=0.07433564215898514\n",
      "epoch 53 iter 45 loss=0.11533628404140472\n",
      "epoch 53 iter 46 loss=0.07856282591819763\n",
      "epoch 53 iter 47 loss=0.13394667208194733\n",
      "epoch 53 iter 48 loss=0.14122390747070312\n",
      "epoch 53 iter 49 loss=0.11498299241065979\n",
      "epoch 53 iter 50 loss=0.11058136075735092\n",
      "epoch 53 iter 51 loss=0.08297870308160782\n",
      "epoch 53 iter 52 loss=0.07626218348741531\n",
      "epoch 53 iter 53 loss=0.09007035195827484\n",
      "epoch 53 iter 54 loss=0.08571777492761612\n",
      "epoch 53 iter 55 loss=0.10886067152023315\n",
      "epoch 53 iter 56 loss=0.09183844178915024\n",
      "epoch 53 iter 57 loss=0.14397524297237396\n",
      "epoch 53 iter 58 loss=0.07003822177648544\n",
      "epoch 53 iter 59 loss=0.06932235509157181\n",
      "epoch 53 iter 60 loss=0.0925784632563591\n",
      "epoch 53 iter 61 loss=0.09255080670118332\n",
      "epoch 53 iter 62 loss=0.11416790634393692\n",
      "epoch 53 iter 63 loss=0.08120711147785187\n",
      "epoch 53 iter 64 loss=0.14396555721759796\n",
      "epoch 53 iter 65 loss=0.07658988237380981\n",
      "epoch 53 iter 66 loss=0.10114511102437973\n",
      "epoch 53 iter 67 loss=0.08404192328453064\n",
      "epoch 53 iter 68 loss=0.15328189730644226\n",
      "epoch 53 iter 69 loss=0.12015566974878311\n",
      "epoch 53 iter 70 loss=0.11322318762540817\n",
      "epoch 53 iter 71 loss=0.10483923554420471\n",
      "epoch 53 iter 72 loss=0.14524909853935242\n",
      "epoch 53 iter 73 loss=0.06044986844062805\n",
      "epoch 53 iter 74 loss=0.06489518284797668\n",
      "epoch 54 iter 0 loss=0.09125258028507233\n",
      "epoch 54 iter 1 loss=0.14042043685913086\n",
      "epoch 54 iter 2 loss=0.2533165514469147\n",
      "epoch 54 iter 3 loss=0.09026578068733215\n",
      "epoch 54 iter 4 loss=0.1627311408519745\n",
      "epoch 54 iter 5 loss=0.13039539754390717\n",
      "epoch 54 iter 6 loss=0.0940508171916008\n",
      "epoch 54 iter 7 loss=0.07183098793029785\n",
      "epoch 54 iter 8 loss=0.11108388751745224\n",
      "epoch 54 iter 9 loss=0.10828723013401031\n",
      "epoch 54 iter 10 loss=0.12981495261192322\n",
      "epoch 54 iter 11 loss=0.09546326100826263\n",
      "epoch 54 iter 12 loss=0.13729150593280792\n",
      "epoch 54 iter 13 loss=0.11830311268568039\n",
      "epoch 54 iter 14 loss=0.11798407137393951\n",
      "epoch 54 iter 15 loss=0.09844741970300674\n",
      "epoch 54 iter 16 loss=0.13271796703338623\n",
      "epoch 54 iter 17 loss=0.07560780644416809\n",
      "epoch 54 iter 18 loss=0.11157580465078354\n",
      "epoch 54 iter 19 loss=0.06275928765535355\n",
      "epoch 54 iter 20 loss=0.08362918347120285\n",
      "epoch 54 iter 21 loss=0.11328824609518051\n",
      "epoch 54 iter 22 loss=0.07448919117450714\n",
      "epoch 54 iter 23 loss=0.09010308235883713\n",
      "epoch 54 iter 24 loss=0.12695147097110748\n",
      "epoch 54 iter 25 loss=0.07729782909154892\n",
      "epoch 54 iter 26 loss=0.06785471737384796\n",
      "epoch 54 iter 27 loss=0.08328290283679962\n",
      "epoch 54 iter 28 loss=0.12722185254096985\n",
      "epoch 54 iter 29 loss=0.090481236577034\n",
      "epoch 54 iter 30 loss=0.12455453723669052\n",
      "epoch 54 iter 31 loss=0.0634806752204895\n",
      "epoch 54 iter 32 loss=0.08419337123632431\n",
      "epoch 54 iter 33 loss=0.0864858627319336\n",
      "epoch 54 iter 34 loss=0.1156679168343544\n",
      "epoch 54 iter 35 loss=0.08986951410770416\n",
      "epoch 54 iter 36 loss=0.15773029625415802\n",
      "epoch 54 iter 37 loss=0.05114862695336342\n",
      "epoch 54 iter 38 loss=0.07781357318162918\n",
      "epoch 54 iter 39 loss=0.09084794670343399\n",
      "epoch 54 iter 40 loss=0.10929779708385468\n",
      "epoch 54 iter 41 loss=0.09342387318611145\n",
      "epoch 54 iter 42 loss=0.12173506617546082\n",
      "epoch 54 iter 43 loss=0.09130502492189407\n",
      "epoch 54 iter 44 loss=0.11866790056228638\n",
      "epoch 54 iter 45 loss=0.06642531603574753\n",
      "epoch 54 iter 46 loss=0.15182888507843018\n",
      "epoch 54 iter 47 loss=0.1016816794872284\n",
      "epoch 54 iter 48 loss=0.10394705832004547\n",
      "epoch 54 iter 49 loss=0.10676868259906769\n",
      "epoch 54 iter 50 loss=0.11725857108831406\n",
      "epoch 54 iter 51 loss=0.09580594301223755\n",
      "epoch 54 iter 52 loss=0.12337624281644821\n",
      "epoch 54 iter 53 loss=0.07321275770664215\n",
      "epoch 54 iter 54 loss=0.10403357446193695\n",
      "epoch 54 iter 55 loss=0.11316870152950287\n",
      "epoch 54 iter 56 loss=0.07224847376346588\n",
      "epoch 54 iter 57 loss=0.13361415266990662\n",
      "epoch 54 iter 58 loss=0.13385739922523499\n",
      "epoch 54 iter 59 loss=0.08596489578485489\n",
      "epoch 54 iter 60 loss=0.1277565062046051\n",
      "epoch 54 iter 61 loss=0.095107801258564\n",
      "epoch 54 iter 62 loss=0.053297337144613266\n",
      "epoch 54 iter 63 loss=0.1455530822277069\n",
      "epoch 54 iter 64 loss=0.08681011945009232\n",
      "epoch 54 iter 65 loss=0.15498632192611694\n",
      "epoch 54 iter 66 loss=0.08745671063661575\n",
      "epoch 54 iter 67 loss=0.1032552495598793\n",
      "epoch 54 iter 68 loss=0.09965761750936508\n",
      "epoch 54 iter 69 loss=0.08721070736646652\n",
      "epoch 54 iter 70 loss=0.09765391051769257\n",
      "epoch 54 iter 71 loss=0.07896115630865097\n",
      "epoch 54 iter 72 loss=0.19604969024658203\n",
      "epoch 54 iter 73 loss=0.08127982169389725\n",
      "epoch 54 iter 74 loss=0.11938793212175369\n",
      "epoch 55 iter 0 loss=0.08625128120183945\n",
      "epoch 55 iter 1 loss=0.10955959558486938\n",
      "epoch 55 iter 2 loss=0.14213795959949493\n",
      "epoch 55 iter 3 loss=0.10472013801336288\n",
      "epoch 55 iter 4 loss=0.10268672555685043\n",
      "epoch 55 iter 5 loss=0.06846361607313156\n",
      "epoch 55 iter 6 loss=0.10005543380975723\n",
      "epoch 55 iter 7 loss=0.09685859084129333\n",
      "epoch 55 iter 8 loss=0.0875864177942276\n",
      "epoch 55 iter 9 loss=0.12162858992815018\n",
      "epoch 55 iter 10 loss=0.11044776439666748\n",
      "epoch 55 iter 11 loss=0.07662482559680939\n",
      "epoch 55 iter 12 loss=0.2356673628091812\n",
      "epoch 55 iter 13 loss=0.09213721752166748\n",
      "epoch 55 iter 14 loss=0.09448880702257156\n",
      "epoch 55 iter 15 loss=0.1052827388048172\n",
      "epoch 55 iter 16 loss=0.12802591919898987\n",
      "epoch 55 iter 17 loss=0.10023805499076843\n",
      "epoch 55 iter 18 loss=0.11349489539861679\n",
      "epoch 55 iter 19 loss=0.1022607758641243\n",
      "epoch 55 iter 20 loss=0.11938830465078354\n",
      "epoch 55 iter 21 loss=0.10233154147863388\n",
      "epoch 55 iter 22 loss=0.14519338309764862\n",
      "epoch 55 iter 23 loss=0.10822661966085434\n",
      "epoch 55 iter 24 loss=0.08146290481090546\n",
      "epoch 55 iter 25 loss=0.11052742600440979\n",
      "epoch 55 iter 26 loss=0.13018278777599335\n",
      "epoch 55 iter 27 loss=0.08054806292057037\n",
      "epoch 55 iter 28 loss=0.09733975678682327\n",
      "epoch 55 iter 29 loss=0.08866457641124725\n",
      "epoch 55 iter 30 loss=0.06897884607315063\n",
      "epoch 55 iter 31 loss=0.08879488706588745\n",
      "epoch 55 iter 32 loss=0.12601789832115173\n",
      "epoch 55 iter 33 loss=0.06705846637487411\n",
      "epoch 55 iter 34 loss=0.1366252899169922\n",
      "epoch 55 iter 35 loss=0.09803028404712677\n",
      "epoch 55 iter 36 loss=0.1050533875823021\n",
      "epoch 55 iter 37 loss=0.15026719868183136\n",
      "epoch 55 iter 38 loss=0.06425223499536514\n",
      "epoch 55 iter 39 loss=0.07635076344013214\n",
      "epoch 55 iter 40 loss=0.1019330695271492\n",
      "epoch 55 iter 41 loss=0.1263495832681656\n",
      "epoch 55 iter 42 loss=0.0811895951628685\n",
      "epoch 55 iter 43 loss=0.08747676014900208\n",
      "epoch 55 iter 44 loss=0.12031707167625427\n",
      "epoch 55 iter 45 loss=0.08977009356021881\n",
      "epoch 55 iter 46 loss=0.07687532156705856\n",
      "epoch 55 iter 47 loss=0.07209905236959457\n",
      "epoch 55 iter 48 loss=0.09334622323513031\n",
      "epoch 55 iter 49 loss=0.07679716497659683\n",
      "epoch 55 iter 50 loss=0.10506393760442734\n",
      "epoch 55 iter 51 loss=0.057444535195827484\n",
      "epoch 55 iter 52 loss=0.0771847814321518\n",
      "epoch 55 iter 53 loss=0.11304892599582672\n",
      "epoch 55 iter 54 loss=0.14430978894233704\n",
      "epoch 55 iter 55 loss=0.11706271022558212\n",
      "epoch 55 iter 56 loss=0.06769482791423798\n",
      "epoch 55 iter 57 loss=0.10768923908472061\n",
      "epoch 55 iter 58 loss=0.0828903466463089\n",
      "epoch 55 iter 59 loss=0.09946448355913162\n",
      "epoch 55 iter 60 loss=0.11418090760707855\n",
      "epoch 55 iter 61 loss=0.07021663337945938\n",
      "epoch 55 iter 62 loss=0.1127479150891304\n",
      "epoch 55 iter 63 loss=0.1784466654062271\n",
      "epoch 55 iter 64 loss=0.12007806450128555\n",
      "epoch 55 iter 65 loss=0.061287760734558105\n",
      "epoch 55 iter 66 loss=0.11291369795799255\n",
      "epoch 55 iter 67 loss=0.06359897553920746\n",
      "epoch 55 iter 68 loss=0.0718471109867096\n",
      "epoch 55 iter 69 loss=0.1256478726863861\n",
      "epoch 55 iter 70 loss=0.08662307262420654\n",
      "epoch 55 iter 71 loss=0.15061728656291962\n",
      "epoch 55 iter 72 loss=0.09655289351940155\n",
      "epoch 55 iter 73 loss=0.057564955204725266\n",
      "epoch 55 iter 74 loss=0.07093410193920135\n",
      "epoch 56 iter 0 loss=0.09016381204128265\n",
      "epoch 56 iter 1 loss=0.06409507244825363\n",
      "epoch 56 iter 2 loss=0.06592477113008499\n",
      "epoch 56 iter 3 loss=0.10864616185426712\n",
      "epoch 56 iter 4 loss=0.11258671432733536\n",
      "epoch 56 iter 5 loss=0.13804936408996582\n",
      "epoch 56 iter 6 loss=0.07916238158941269\n",
      "epoch 56 iter 7 loss=0.11783884465694427\n",
      "epoch 56 iter 8 loss=0.0915314257144928\n",
      "epoch 56 iter 9 loss=0.08604947477579117\n",
      "epoch 56 iter 10 loss=0.150954931974411\n",
      "epoch 56 iter 11 loss=0.09902962297201157\n",
      "epoch 56 iter 12 loss=0.13076183199882507\n",
      "epoch 56 iter 13 loss=0.07511982321739197\n",
      "epoch 56 iter 14 loss=0.08893908560276031\n",
      "epoch 56 iter 15 loss=0.12479787319898605\n",
      "epoch 56 iter 16 loss=0.0503179207444191\n",
      "epoch 56 iter 17 loss=0.07693694531917572\n",
      "epoch 56 iter 18 loss=0.10716419667005539\n",
      "epoch 56 iter 19 loss=0.10428686439990997\n",
      "epoch 56 iter 20 loss=0.06428197026252747\n",
      "epoch 56 iter 21 loss=0.0535847507417202\n",
      "epoch 56 iter 22 loss=0.09685701131820679\n",
      "epoch 56 iter 23 loss=0.12160151451826096\n",
      "epoch 56 iter 24 loss=0.09142924845218658\n",
      "epoch 56 iter 25 loss=0.12761841714382172\n",
      "epoch 56 iter 26 loss=0.04931185767054558\n",
      "epoch 56 iter 27 loss=0.13522717356681824\n",
      "epoch 56 iter 28 loss=0.10295882821083069\n",
      "epoch 56 iter 29 loss=0.10325697064399719\n",
      "epoch 56 iter 30 loss=0.1306430846452713\n",
      "epoch 56 iter 31 loss=0.11807888001203537\n",
      "epoch 56 iter 32 loss=0.06900953501462936\n",
      "epoch 56 iter 33 loss=0.14380596578121185\n",
      "epoch 56 iter 34 loss=0.0756915733218193\n",
      "epoch 56 iter 35 loss=0.10858527570962906\n",
      "epoch 56 iter 36 loss=0.09522180259227753\n",
      "epoch 56 iter 37 loss=0.11817531287670135\n",
      "epoch 56 iter 38 loss=0.14947256445884705\n",
      "epoch 56 iter 39 loss=0.1555377095937729\n",
      "epoch 56 iter 40 loss=0.07479195296764374\n",
      "epoch 56 iter 41 loss=0.12375661730766296\n",
      "epoch 56 iter 42 loss=0.10924598574638367\n",
      "epoch 56 iter 43 loss=0.13801568746566772\n",
      "epoch 56 iter 44 loss=0.13700981438159943\n",
      "epoch 56 iter 45 loss=0.059879109263420105\n",
      "epoch 56 iter 46 loss=0.0919930711388588\n",
      "epoch 56 iter 47 loss=0.13229265809059143\n",
      "epoch 56 iter 48 loss=0.10194496065378189\n",
      "epoch 56 iter 49 loss=0.07948876172304153\n",
      "epoch 56 iter 50 loss=0.05353011190891266\n",
      "epoch 56 iter 51 loss=0.07388409972190857\n",
      "epoch 56 iter 52 loss=0.0901523008942604\n",
      "epoch 56 iter 53 loss=0.0704774484038353\n",
      "epoch 56 iter 54 loss=0.07440527528524399\n",
      "epoch 56 iter 55 loss=0.14718545973300934\n",
      "epoch 56 iter 56 loss=0.0761924609541893\n",
      "epoch 56 iter 57 loss=0.10396416485309601\n",
      "epoch 56 iter 58 loss=0.05770543962717056\n",
      "epoch 56 iter 59 loss=0.09083952754735947\n",
      "epoch 56 iter 60 loss=0.09699682146310806\n",
      "epoch 56 iter 61 loss=0.06020062789320946\n",
      "epoch 56 iter 62 loss=0.09493834525346756\n",
      "epoch 56 iter 63 loss=0.12179116159677505\n",
      "epoch 56 iter 64 loss=0.0757465586066246\n",
      "epoch 56 iter 65 loss=0.09695879369974136\n",
      "epoch 56 iter 66 loss=0.08253473043441772\n",
      "epoch 56 iter 67 loss=0.05854686722159386\n",
      "epoch 56 iter 68 loss=0.1262030154466629\n",
      "epoch 56 iter 69 loss=0.12000849843025208\n",
      "epoch 56 iter 70 loss=0.08311470597982407\n",
      "epoch 56 iter 71 loss=0.07652873545885086\n",
      "epoch 56 iter 72 loss=0.12919582426548004\n",
      "epoch 56 iter 73 loss=0.11911756545305252\n",
      "epoch 56 iter 74 loss=0.10386014729738235\n",
      "epoch 57 iter 0 loss=0.07601643353700638\n",
      "epoch 57 iter 1 loss=0.115207239985466\n",
      "epoch 57 iter 2 loss=0.13082824647426605\n",
      "epoch 57 iter 3 loss=0.10669005662202835\n",
      "epoch 57 iter 4 loss=0.06899946928024292\n",
      "epoch 57 iter 5 loss=0.1393950879573822\n",
      "epoch 57 iter 6 loss=0.16733630001544952\n",
      "epoch 57 iter 7 loss=0.07713357359170914\n",
      "epoch 57 iter 8 loss=0.10759411007165909\n",
      "epoch 57 iter 9 loss=0.06801944971084595\n",
      "epoch 57 iter 10 loss=0.10526754707098007\n",
      "epoch 57 iter 11 loss=0.06736189872026443\n",
      "epoch 57 iter 12 loss=0.11375445127487183\n",
      "epoch 57 iter 13 loss=0.07478906959295273\n",
      "epoch 57 iter 14 loss=0.06744277477264404\n",
      "epoch 57 iter 15 loss=0.09701671451330185\n",
      "epoch 57 iter 16 loss=0.11766000092029572\n",
      "epoch 57 iter 17 loss=0.13491810858249664\n",
      "epoch 57 iter 18 loss=0.14919759333133698\n",
      "epoch 57 iter 19 loss=0.10130634903907776\n",
      "epoch 57 iter 20 loss=0.07243048399686813\n",
      "epoch 57 iter 21 loss=0.09125841408967972\n",
      "epoch 57 iter 22 loss=0.08520648628473282\n",
      "epoch 57 iter 23 loss=0.13080210983753204\n",
      "epoch 57 iter 24 loss=0.08100464195013046\n",
      "epoch 57 iter 25 loss=0.12676620483398438\n",
      "epoch 57 iter 26 loss=0.10499761253595352\n",
      "epoch 57 iter 27 loss=0.11316745728254318\n",
      "epoch 57 iter 28 loss=0.10508967936038971\n",
      "epoch 57 iter 29 loss=0.09144981950521469\n",
      "epoch 57 iter 30 loss=0.11965365707874298\n",
      "epoch 57 iter 31 loss=0.19200752675533295\n",
      "epoch 57 iter 32 loss=0.10909925401210785\n",
      "epoch 57 iter 33 loss=0.10827116668224335\n",
      "epoch 57 iter 34 loss=0.09127584844827652\n",
      "epoch 57 iter 35 loss=0.12037438899278641\n",
      "epoch 57 iter 36 loss=0.14191460609436035\n",
      "epoch 57 iter 37 loss=0.08767517656087875\n",
      "epoch 57 iter 38 loss=0.07675302773714066\n",
      "epoch 57 iter 39 loss=0.11537760496139526\n",
      "epoch 57 iter 40 loss=0.12605969607830048\n",
      "epoch 57 iter 41 loss=0.07597973942756653\n",
      "epoch 57 iter 42 loss=0.10225925594568253\n",
      "epoch 57 iter 43 loss=0.13199616968631744\n",
      "epoch 57 iter 44 loss=0.12011654675006866\n",
      "epoch 57 iter 45 loss=0.10649137198925018\n",
      "epoch 57 iter 46 loss=0.09546424448490143\n",
      "epoch 57 iter 47 loss=0.1044626533985138\n",
      "epoch 57 iter 48 loss=0.14395612478256226\n",
      "epoch 57 iter 49 loss=0.05850907415151596\n",
      "epoch 57 iter 50 loss=0.07327841967344284\n",
      "epoch 57 iter 51 loss=0.15635351836681366\n",
      "epoch 57 iter 52 loss=0.13644734025001526\n",
      "epoch 57 iter 53 loss=0.07517692446708679\n",
      "epoch 57 iter 54 loss=0.1667948067188263\n",
      "epoch 57 iter 55 loss=0.08757919073104858\n",
      "epoch 57 iter 56 loss=0.0958879292011261\n",
      "epoch 57 iter 57 loss=0.06236360967159271\n",
      "epoch 57 iter 58 loss=0.0928887203335762\n",
      "epoch 57 iter 59 loss=0.08606676757335663\n",
      "epoch 57 iter 60 loss=0.08759283274412155\n",
      "epoch 57 iter 61 loss=0.13295242190361023\n",
      "epoch 57 iter 62 loss=0.06927423179149628\n",
      "epoch 57 iter 63 loss=0.11518236249685287\n",
      "epoch 57 iter 64 loss=0.10459895431995392\n",
      "epoch 57 iter 65 loss=0.12706369161605835\n",
      "epoch 57 iter 66 loss=0.11446938663721085\n",
      "epoch 57 iter 67 loss=0.1395311951637268\n",
      "epoch 57 iter 68 loss=0.08461477607488632\n",
      "epoch 57 iter 69 loss=0.10100270807743073\n",
      "epoch 57 iter 70 loss=0.08866974711418152\n",
      "epoch 57 iter 71 loss=0.067771315574646\n",
      "epoch 57 iter 72 loss=0.091383196413517\n",
      "epoch 57 iter 73 loss=0.076927050948143\n",
      "epoch 57 iter 74 loss=0.06389310210943222\n",
      "epoch 58 iter 0 loss=0.1071503534913063\n",
      "epoch 58 iter 1 loss=0.10274197906255722\n",
      "epoch 58 iter 2 loss=0.05581238865852356\n",
      "epoch 58 iter 3 loss=0.089333176612854\n",
      "epoch 58 iter 4 loss=0.13523364067077637\n",
      "epoch 58 iter 5 loss=0.04325771704316139\n",
      "epoch 58 iter 6 loss=0.07928025722503662\n",
      "epoch 58 iter 7 loss=0.09231964498758316\n",
      "epoch 58 iter 8 loss=0.0513325035572052\n",
      "epoch 58 iter 9 loss=0.12906087934970856\n",
      "epoch 58 iter 10 loss=0.11666394770145416\n",
      "epoch 58 iter 11 loss=0.2061499059200287\n",
      "epoch 58 iter 12 loss=0.07830090075731277\n",
      "epoch 58 iter 13 loss=0.08044362813234329\n",
      "epoch 58 iter 14 loss=0.10488888621330261\n",
      "epoch 58 iter 15 loss=0.08561597019433975\n",
      "epoch 58 iter 16 loss=0.15992330014705658\n",
      "epoch 58 iter 17 loss=0.10017558187246323\n",
      "epoch 58 iter 18 loss=0.10482189804315567\n",
      "epoch 58 iter 19 loss=0.07942282408475876\n",
      "epoch 58 iter 20 loss=0.08735870569944382\n",
      "epoch 58 iter 21 loss=0.08508437871932983\n",
      "epoch 58 iter 22 loss=0.09789130091667175\n",
      "epoch 58 iter 23 loss=0.11182887107133865\n",
      "epoch 58 iter 24 loss=0.11735609173774719\n",
      "epoch 58 iter 25 loss=0.1103338971734047\n",
      "epoch 58 iter 26 loss=0.06263018399477005\n",
      "epoch 58 iter 27 loss=0.10086297988891602\n",
      "epoch 58 iter 28 loss=0.09761634469032288\n",
      "epoch 58 iter 29 loss=0.11227051168680191\n",
      "epoch 58 iter 30 loss=0.08229199051856995\n",
      "epoch 58 iter 31 loss=0.09156803786754608\n",
      "epoch 58 iter 32 loss=0.09256204217672348\n",
      "epoch 58 iter 33 loss=0.07916362583637238\n",
      "epoch 58 iter 34 loss=0.07017973810434341\n",
      "epoch 58 iter 35 loss=0.08733740448951721\n",
      "epoch 58 iter 36 loss=0.0715537816286087\n",
      "epoch 58 iter 37 loss=0.09495826065540314\n",
      "epoch 58 iter 38 loss=0.0850682407617569\n",
      "epoch 58 iter 39 loss=0.09791877120733261\n",
      "epoch 58 iter 40 loss=0.10692603886127472\n",
      "epoch 58 iter 41 loss=0.09984742850065231\n",
      "epoch 58 iter 42 loss=0.06484869867563248\n",
      "epoch 58 iter 43 loss=0.09613414853811264\n",
      "epoch 58 iter 44 loss=0.10118705779314041\n",
      "epoch 58 iter 45 loss=0.08751567453145981\n",
      "epoch 58 iter 46 loss=0.08542376011610031\n",
      "epoch 58 iter 47 loss=0.08103374391794205\n",
      "epoch 58 iter 48 loss=0.10524097084999084\n",
      "epoch 58 iter 49 loss=0.15945616364479065\n",
      "epoch 58 iter 50 loss=0.09544414281845093\n",
      "epoch 58 iter 51 loss=0.10052201896905899\n",
      "epoch 58 iter 52 loss=0.11396374553442001\n",
      "epoch 58 iter 53 loss=0.08925063163042068\n",
      "epoch 58 iter 54 loss=0.13228583335876465\n",
      "epoch 58 iter 55 loss=0.08890264481306076\n",
      "epoch 58 iter 56 loss=0.1164025291800499\n",
      "epoch 58 iter 57 loss=0.12503936886787415\n",
      "epoch 58 iter 58 loss=0.08436084538698196\n",
      "epoch 58 iter 59 loss=0.09386081993579865\n",
      "epoch 58 iter 60 loss=0.07986614108085632\n",
      "epoch 58 iter 61 loss=0.11184217035770416\n",
      "epoch 58 iter 62 loss=0.10101927071809769\n",
      "epoch 58 iter 63 loss=0.09977233409881592\n",
      "epoch 58 iter 64 loss=0.06686996668577194\n",
      "epoch 58 iter 65 loss=0.15097253024578094\n",
      "epoch 58 iter 66 loss=0.10234169661998749\n",
      "epoch 58 iter 67 loss=0.059222761541604996\n",
      "epoch 58 iter 68 loss=0.11480706930160522\n",
      "epoch 58 iter 69 loss=0.08691249787807465\n",
      "epoch 58 iter 70 loss=0.0936870276927948\n",
      "epoch 58 iter 71 loss=0.10251229256391525\n",
      "epoch 58 iter 72 loss=0.10260225832462311\n",
      "epoch 58 iter 73 loss=0.18271324038505554\n",
      "epoch 58 iter 74 loss=0.08724427223205566\n",
      "epoch 59 iter 0 loss=0.06693487614393234\n",
      "epoch 59 iter 1 loss=0.055984750390052795\n",
      "epoch 59 iter 2 loss=0.06579484790563583\n",
      "epoch 59 iter 3 loss=0.07564637064933777\n",
      "epoch 59 iter 4 loss=0.09872084110975266\n",
      "epoch 59 iter 5 loss=0.10844964534044266\n",
      "epoch 59 iter 6 loss=0.07101961225271225\n",
      "epoch 59 iter 7 loss=0.07410901039838791\n",
      "epoch 59 iter 8 loss=0.11875906586647034\n",
      "epoch 59 iter 9 loss=0.07569482922554016\n",
      "epoch 59 iter 10 loss=0.10503634810447693\n",
      "epoch 59 iter 11 loss=0.14666008949279785\n",
      "epoch 59 iter 12 loss=0.10211198031902313\n",
      "epoch 59 iter 13 loss=0.09315715730190277\n",
      "epoch 59 iter 14 loss=0.09841535240411758\n",
      "epoch 59 iter 15 loss=0.06740216165781021\n",
      "epoch 59 iter 16 loss=0.09976392984390259\n",
      "epoch 59 iter 17 loss=0.08497096598148346\n",
      "epoch 59 iter 18 loss=0.09127439558506012\n",
      "epoch 59 iter 19 loss=0.12253935635089874\n",
      "epoch 59 iter 20 loss=0.08345162123441696\n",
      "epoch 59 iter 21 loss=0.08209553360939026\n",
      "epoch 59 iter 22 loss=0.07195138186216354\n",
      "epoch 59 iter 23 loss=0.07917798310518265\n",
      "epoch 59 iter 24 loss=0.06855704635381699\n",
      "epoch 59 iter 25 loss=0.08758745342493057\n",
      "epoch 59 iter 26 loss=0.10282876342535019\n",
      "epoch 59 iter 27 loss=0.0968455970287323\n",
      "epoch 59 iter 28 loss=0.10816279798746109\n",
      "epoch 59 iter 29 loss=0.11690933257341385\n",
      "epoch 59 iter 30 loss=0.09611503779888153\n",
      "epoch 59 iter 31 loss=0.14135229587554932\n",
      "epoch 59 iter 32 loss=0.11919063329696655\n",
      "epoch 59 iter 33 loss=0.05749555304646492\n",
      "epoch 59 iter 34 loss=0.09428571164608002\n",
      "epoch 59 iter 35 loss=0.08025582134723663\n",
      "epoch 59 iter 36 loss=0.08697465062141418\n",
      "epoch 59 iter 37 loss=0.1287420392036438\n",
      "epoch 59 iter 38 loss=0.06900966912508011\n",
      "epoch 59 iter 39 loss=0.07788927108049393\n",
      "epoch 59 iter 40 loss=0.12919223308563232\n",
      "epoch 59 iter 41 loss=0.2061612457036972\n",
      "epoch 59 iter 42 loss=0.10311958938837051\n",
      "epoch 59 iter 43 loss=0.06757806986570358\n",
      "epoch 59 iter 44 loss=0.06971833109855652\n",
      "epoch 59 iter 45 loss=0.09610762447118759\n",
      "epoch 59 iter 46 loss=0.07668963074684143\n",
      "epoch 59 iter 47 loss=0.08265848457813263\n",
      "epoch 59 iter 48 loss=0.06622619926929474\n",
      "epoch 59 iter 49 loss=0.10557994991540909\n",
      "epoch 59 iter 50 loss=0.06957708299160004\n",
      "epoch 59 iter 51 loss=0.099708192050457\n",
      "epoch 59 iter 52 loss=0.09843867272138596\n",
      "epoch 59 iter 53 loss=0.0768197551369667\n",
      "epoch 59 iter 54 loss=0.08368491381406784\n",
      "epoch 59 iter 55 loss=0.06926895678043365\n",
      "epoch 59 iter 56 loss=0.10368289798498154\n",
      "epoch 59 iter 57 loss=0.09475836902856827\n",
      "epoch 59 iter 58 loss=0.05178884044289589\n",
      "epoch 59 iter 59 loss=0.061979442834854126\n",
      "epoch 59 iter 60 loss=0.08099804073572159\n",
      "epoch 59 iter 61 loss=0.09178981930017471\n",
      "epoch 59 iter 62 loss=0.14567537605762482\n",
      "epoch 59 iter 63 loss=0.07031825184822083\n",
      "epoch 59 iter 64 loss=0.11888295412063599\n",
      "epoch 59 iter 65 loss=0.09763596206903458\n",
      "epoch 59 iter 66 loss=0.09422257542610168\n",
      "epoch 59 iter 67 loss=0.1035972610116005\n",
      "epoch 59 iter 68 loss=0.13023211061954498\n",
      "epoch 59 iter 69 loss=0.09091918170452118\n",
      "epoch 59 iter 70 loss=0.13096730411052704\n",
      "epoch 59 iter 71 loss=0.11728616058826447\n",
      "epoch 59 iter 72 loss=0.12049203366041183\n",
      "epoch 59 iter 73 loss=0.10905879735946655\n",
      "epoch 59 iter 74 loss=0.13536307215690613\n",
      "epoch 60 iter 0 loss=0.1137392520904541\n",
      "epoch 60 iter 1 loss=0.11772473901510239\n",
      "epoch 60 iter 2 loss=0.13283632695674896\n",
      "epoch 60 iter 3 loss=0.09788774698972702\n",
      "epoch 60 iter 4 loss=0.1057882308959961\n",
      "epoch 60 iter 5 loss=0.08517332375049591\n",
      "epoch 60 iter 6 loss=0.09878256916999817\n",
      "epoch 60 iter 7 loss=0.11345170438289642\n",
      "epoch 60 iter 8 loss=0.06932974606752396\n",
      "epoch 60 iter 9 loss=0.10592935979366302\n",
      "epoch 60 iter 10 loss=0.08197572827339172\n",
      "epoch 60 iter 11 loss=0.15523746609687805\n",
      "epoch 60 iter 12 loss=0.0985814779996872\n",
      "epoch 60 iter 13 loss=0.0903773382306099\n",
      "epoch 60 iter 14 loss=0.06703143566846848\n",
      "epoch 60 iter 15 loss=0.11501850187778473\n",
      "epoch 60 iter 16 loss=0.08428819477558136\n",
      "epoch 60 iter 17 loss=0.10441180318593979\n",
      "epoch 60 iter 18 loss=0.07904515415430069\n",
      "epoch 60 iter 19 loss=0.1120523139834404\n",
      "epoch 60 iter 20 loss=0.11709458380937576\n",
      "epoch 60 iter 21 loss=0.07154261320829391\n",
      "epoch 60 iter 22 loss=0.08614544570446014\n",
      "epoch 60 iter 23 loss=0.07535872608423233\n",
      "epoch 60 iter 24 loss=0.07029472291469574\n",
      "epoch 60 iter 25 loss=0.07802074402570724\n",
      "epoch 60 iter 26 loss=0.15531349182128906\n",
      "epoch 60 iter 27 loss=0.08411401510238647\n",
      "epoch 60 iter 28 loss=0.13896745443344116\n",
      "epoch 60 iter 29 loss=0.08848407119512558\n",
      "epoch 60 iter 30 loss=0.12923844158649445\n",
      "epoch 60 iter 31 loss=0.09220985323190689\n",
      "epoch 60 iter 32 loss=0.06503937393426895\n",
      "epoch 60 iter 33 loss=0.1015704870223999\n",
      "epoch 60 iter 34 loss=0.08177348971366882\n",
      "epoch 60 iter 35 loss=0.1327410638332367\n",
      "epoch 60 iter 36 loss=0.09285483509302139\n",
      "epoch 60 iter 37 loss=0.08305969834327698\n",
      "epoch 60 iter 38 loss=0.08789997547864914\n",
      "epoch 60 iter 39 loss=0.08978845924139023\n",
      "epoch 60 iter 40 loss=0.15511411428451538\n",
      "epoch 60 iter 41 loss=0.06486257165670395\n",
      "epoch 60 iter 42 loss=0.08453622460365295\n",
      "epoch 60 iter 43 loss=0.07992234826087952\n",
      "epoch 60 iter 44 loss=0.06962991505861282\n",
      "epoch 60 iter 45 loss=0.09357782453298569\n",
      "epoch 60 iter 46 loss=0.08155136555433273\n",
      "epoch 60 iter 47 loss=0.08039611577987671\n",
      "epoch 60 iter 48 loss=0.14291250705718994\n",
      "epoch 60 iter 49 loss=0.10321370512247086\n",
      "epoch 60 iter 50 loss=0.11730023473501205\n",
      "epoch 60 iter 51 loss=0.09934362024068832\n",
      "epoch 60 iter 52 loss=0.07141546159982681\n",
      "epoch 60 iter 53 loss=0.07570059597492218\n",
      "epoch 60 iter 54 loss=0.07287981361150742\n",
      "epoch 60 iter 55 loss=0.12998391687870026\n",
      "epoch 60 iter 56 loss=0.07889151573181152\n",
      "epoch 60 iter 57 loss=0.11683960258960724\n",
      "epoch 60 iter 58 loss=0.11425912380218506\n",
      "epoch 60 iter 59 loss=0.09290538728237152\n",
      "epoch 60 iter 60 loss=0.07437775284051895\n",
      "epoch 60 iter 61 loss=0.1223386749625206\n",
      "epoch 60 iter 62 loss=0.13939933478832245\n",
      "epoch 60 iter 63 loss=0.10665187239646912\n",
      "epoch 60 iter 64 loss=0.05289489030838013\n",
      "epoch 60 iter 65 loss=0.05612815544009209\n",
      "epoch 60 iter 66 loss=0.08419058471918106\n",
      "epoch 60 iter 67 loss=0.10661040246486664\n",
      "epoch 60 iter 68 loss=0.10364668816328049\n",
      "epoch 60 iter 69 loss=0.24103844165802002\n",
      "epoch 60 iter 70 loss=0.1231294795870781\n",
      "epoch 60 iter 71 loss=0.10415711253881454\n",
      "epoch 60 iter 72 loss=0.09988968819379807\n",
      "epoch 60 iter 73 loss=0.07109536975622177\n",
      "epoch 60 iter 74 loss=0.09891294687986374\n",
      "epoch 61 iter 0 loss=0.10253710299730301\n",
      "epoch 61 iter 1 loss=0.1410442739725113\n",
      "epoch 61 iter 2 loss=0.10263808816671371\n",
      "epoch 61 iter 3 loss=0.1386929303407669\n",
      "epoch 61 iter 4 loss=0.15727116167545319\n",
      "epoch 61 iter 5 loss=0.08354754745960236\n",
      "epoch 61 iter 6 loss=0.069484643638134\n",
      "epoch 61 iter 7 loss=0.133034810423851\n",
      "epoch 61 iter 8 loss=0.09359350055456161\n",
      "epoch 61 iter 9 loss=0.12257742881774902\n",
      "epoch 61 iter 10 loss=0.08541300147771835\n",
      "epoch 61 iter 11 loss=0.13589219748973846\n",
      "epoch 61 iter 12 loss=0.0855657085776329\n",
      "epoch 61 iter 13 loss=0.06927157938480377\n",
      "epoch 61 iter 14 loss=0.12305908650159836\n",
      "epoch 61 iter 15 loss=0.16778883337974548\n",
      "epoch 61 iter 16 loss=0.11032532900571823\n",
      "epoch 61 iter 17 loss=0.07283923029899597\n",
      "epoch 61 iter 18 loss=0.05818641930818558\n",
      "epoch 61 iter 19 loss=0.15395593643188477\n",
      "epoch 61 iter 20 loss=0.11482386291027069\n",
      "epoch 61 iter 21 loss=0.11236066371202469\n",
      "epoch 61 iter 22 loss=0.08195367455482483\n",
      "epoch 61 iter 23 loss=0.07226219028234482\n",
      "epoch 61 iter 24 loss=0.1007176861166954\n",
      "epoch 61 iter 25 loss=0.09181899577379227\n",
      "epoch 61 iter 26 loss=0.08005945384502411\n",
      "epoch 61 iter 27 loss=0.0812421441078186\n",
      "epoch 61 iter 28 loss=0.08440867066383362\n",
      "epoch 61 iter 29 loss=0.079042449593544\n",
      "epoch 61 iter 30 loss=0.1086084395647049\n",
      "epoch 61 iter 31 loss=0.09482917934656143\n",
      "epoch 61 iter 32 loss=0.1237678974866867\n",
      "epoch 61 iter 33 loss=0.15787304937839508\n",
      "epoch 61 iter 34 loss=0.11177672445774078\n",
      "epoch 61 iter 35 loss=0.12016724795103073\n",
      "epoch 61 iter 36 loss=0.053584299981594086\n",
      "epoch 61 iter 37 loss=0.07247444242238998\n",
      "epoch 61 iter 38 loss=0.1461321860551834\n",
      "epoch 61 iter 39 loss=0.08499552309513092\n",
      "epoch 61 iter 40 loss=0.1232374757528305\n",
      "epoch 61 iter 41 loss=0.17251643538475037\n",
      "epoch 61 iter 42 loss=0.07889105379581451\n",
      "epoch 61 iter 43 loss=0.1540203094482422\n",
      "epoch 61 iter 44 loss=0.12400708347558975\n",
      "epoch 61 iter 45 loss=0.08474717289209366\n",
      "epoch 61 iter 46 loss=0.09898725897073746\n",
      "epoch 61 iter 47 loss=0.11608806997537613\n",
      "epoch 61 iter 48 loss=0.13617704808712006\n",
      "epoch 61 iter 49 loss=0.06624876707792282\n",
      "epoch 61 iter 50 loss=0.08541170507669449\n",
      "epoch 61 iter 51 loss=0.07350819557905197\n",
      "epoch 61 iter 52 loss=0.1023314967751503\n",
      "epoch 61 iter 53 loss=0.09845206141471863\n",
      "epoch 61 iter 54 loss=0.08221353590488434\n",
      "epoch 61 iter 55 loss=0.14372922480106354\n",
      "epoch 61 iter 56 loss=0.14280839264392853\n",
      "epoch 61 iter 57 loss=0.08876001834869385\n",
      "epoch 61 iter 58 loss=0.13819639384746552\n",
      "epoch 61 iter 59 loss=0.07294083386659622\n",
      "epoch 61 iter 60 loss=0.08801396936178207\n",
      "epoch 61 iter 61 loss=0.07157090306282043\n",
      "epoch 61 iter 62 loss=0.09200482815504074\n",
      "epoch 61 iter 63 loss=0.09018171578645706\n",
      "epoch 61 iter 64 loss=0.0944761261343956\n",
      "epoch 61 iter 65 loss=0.09130155295133591\n",
      "epoch 61 iter 66 loss=0.1746366322040558\n",
      "epoch 61 iter 67 loss=0.11194292455911636\n",
      "epoch 61 iter 68 loss=0.06678511202335358\n",
      "epoch 61 iter 69 loss=0.06837073713541031\n",
      "epoch 61 iter 70 loss=0.08134061098098755\n",
      "epoch 61 iter 71 loss=0.1054769977927208\n",
      "epoch 61 iter 72 loss=0.15615719556808472\n",
      "epoch 61 iter 73 loss=0.09057985246181488\n",
      "epoch 61 iter 74 loss=0.12175721675157547\n",
      "epoch 62 iter 0 loss=0.07599515467882156\n",
      "epoch 62 iter 1 loss=0.10340513288974762\n",
      "epoch 62 iter 2 loss=0.09704618901014328\n",
      "epoch 62 iter 3 loss=0.1291457861661911\n",
      "epoch 62 iter 4 loss=0.10052071511745453\n",
      "epoch 62 iter 5 loss=0.13475051522254944\n",
      "epoch 62 iter 6 loss=0.10184357315301895\n",
      "epoch 62 iter 7 loss=0.1061936542391777\n",
      "epoch 62 iter 8 loss=0.0862964540719986\n",
      "epoch 62 iter 9 loss=0.06862850487232208\n",
      "epoch 62 iter 10 loss=0.11747018247842789\n",
      "epoch 62 iter 11 loss=0.08792585134506226\n",
      "epoch 62 iter 12 loss=0.16528218984603882\n",
      "epoch 62 iter 13 loss=0.16840742528438568\n",
      "epoch 62 iter 14 loss=0.055859941989183426\n",
      "epoch 62 iter 15 loss=0.06818819046020508\n",
      "epoch 62 iter 16 loss=0.16738168895244598\n",
      "epoch 62 iter 17 loss=0.07347023487091064\n",
      "epoch 62 iter 18 loss=0.12287292629480362\n",
      "epoch 62 iter 19 loss=0.0991542711853981\n",
      "epoch 62 iter 20 loss=0.10987207293510437\n",
      "epoch 62 iter 21 loss=0.10509072244167328\n",
      "epoch 62 iter 22 loss=0.057497039437294006\n",
      "epoch 62 iter 23 loss=0.10562694072723389\n",
      "epoch 62 iter 24 loss=0.07969258725643158\n",
      "epoch 62 iter 25 loss=0.09969265758991241\n",
      "epoch 62 iter 26 loss=0.09506258368492126\n",
      "epoch 62 iter 27 loss=0.09486991167068481\n",
      "epoch 62 iter 28 loss=0.0665183812379837\n",
      "epoch 62 iter 29 loss=0.07441546022891998\n",
      "epoch 62 iter 30 loss=0.08018799871206284\n",
      "epoch 62 iter 31 loss=0.08856268227100372\n",
      "epoch 62 iter 32 loss=0.18480287492275238\n",
      "epoch 62 iter 33 loss=0.12806493043899536\n",
      "epoch 62 iter 34 loss=0.16789610683918\n",
      "epoch 62 iter 35 loss=0.11625466495752335\n",
      "epoch 62 iter 36 loss=0.10839314013719559\n",
      "epoch 62 iter 37 loss=0.10457514226436615\n",
      "epoch 62 iter 38 loss=0.10952802747488022\n",
      "epoch 62 iter 39 loss=0.08667074888944626\n",
      "epoch 62 iter 40 loss=0.11268026381731033\n",
      "epoch 62 iter 41 loss=0.09213142096996307\n",
      "epoch 62 iter 42 loss=0.08597874641418457\n",
      "epoch 62 iter 43 loss=0.10198681801557541\n",
      "epoch 62 iter 44 loss=0.13376975059509277\n",
      "epoch 62 iter 45 loss=0.07443791627883911\n",
      "epoch 62 iter 46 loss=0.08776651322841644\n",
      "epoch 62 iter 47 loss=0.0810338631272316\n",
      "epoch 62 iter 48 loss=0.15165764093399048\n",
      "epoch 62 iter 49 loss=0.10385223478078842\n",
      "epoch 62 iter 50 loss=0.09327133744955063\n",
      "epoch 62 iter 51 loss=0.12327374517917633\n",
      "epoch 62 iter 52 loss=0.15767811238765717\n",
      "epoch 62 iter 53 loss=0.06267822533845901\n",
      "epoch 62 iter 54 loss=0.07615409791469574\n",
      "epoch 62 iter 55 loss=0.09300245344638824\n",
      "epoch 62 iter 56 loss=0.08523468673229218\n",
      "epoch 62 iter 57 loss=0.07983327656984329\n",
      "epoch 62 iter 58 loss=0.2203318029642105\n",
      "epoch 62 iter 59 loss=0.10629893094301224\n",
      "epoch 62 iter 60 loss=0.0999763011932373\n",
      "epoch 62 iter 61 loss=0.1325283646583557\n",
      "epoch 62 iter 62 loss=0.21996887028217316\n",
      "epoch 62 iter 63 loss=0.07500009983778\n",
      "epoch 62 iter 64 loss=0.1624312698841095\n",
      "epoch 62 iter 65 loss=0.09578468650579453\n",
      "epoch 62 iter 66 loss=0.11716979742050171\n",
      "epoch 62 iter 67 loss=0.09070129692554474\n",
      "epoch 62 iter 68 loss=0.1401587575674057\n",
      "epoch 62 iter 69 loss=0.13511966168880463\n",
      "epoch 62 iter 70 loss=0.09159224480390549\n",
      "epoch 62 iter 71 loss=0.12750421464443207\n",
      "epoch 62 iter 72 loss=0.08884294331073761\n",
      "epoch 62 iter 73 loss=0.08104208111763\n",
      "epoch 62 iter 74 loss=0.1602465957403183\n",
      "epoch 63 iter 0 loss=0.12448065727949142\n",
      "epoch 63 iter 1 loss=0.09816356003284454\n",
      "epoch 63 iter 2 loss=0.09147117286920547\n",
      "epoch 63 iter 3 loss=0.10849969834089279\n",
      "epoch 63 iter 4 loss=0.07149510830640793\n",
      "epoch 63 iter 5 loss=0.09141139686107635\n",
      "epoch 63 iter 6 loss=0.13314653933048248\n",
      "epoch 63 iter 7 loss=0.11863837391138077\n",
      "epoch 63 iter 8 loss=0.11052646487951279\n",
      "epoch 63 iter 9 loss=0.11118294298648834\n",
      "epoch 63 iter 10 loss=0.11133731156587601\n",
      "epoch 63 iter 11 loss=0.11289667338132858\n",
      "epoch 63 iter 12 loss=0.0774158164858818\n",
      "epoch 63 iter 13 loss=0.10797824710607529\n",
      "epoch 63 iter 14 loss=0.12358969449996948\n",
      "epoch 63 iter 15 loss=0.10490237176418304\n",
      "epoch 63 iter 16 loss=0.10182483494281769\n",
      "epoch 63 iter 17 loss=0.09459308534860611\n",
      "epoch 63 iter 18 loss=0.07508357614278793\n",
      "epoch 63 iter 19 loss=0.17486774921417236\n",
      "epoch 63 iter 20 loss=0.0830584466457367\n",
      "epoch 63 iter 21 loss=0.12264933437108994\n",
      "epoch 63 iter 22 loss=0.10888348519802094\n",
      "epoch 63 iter 23 loss=0.09853249788284302\n",
      "epoch 63 iter 24 loss=0.11767976731061935\n",
      "epoch 63 iter 25 loss=0.08078941702842712\n",
      "epoch 63 iter 26 loss=0.10948621481657028\n",
      "epoch 63 iter 27 loss=0.10944748669862747\n",
      "epoch 63 iter 28 loss=0.12050284445285797\n",
      "epoch 63 iter 29 loss=0.09445454925298691\n",
      "epoch 63 iter 30 loss=0.13719692826271057\n",
      "epoch 63 iter 31 loss=0.09179936349391937\n",
      "epoch 63 iter 32 loss=0.204055517911911\n",
      "epoch 63 iter 33 loss=0.11515149474143982\n",
      "epoch 63 iter 34 loss=0.14610867202281952\n",
      "epoch 63 iter 35 loss=0.06192151829600334\n",
      "epoch 63 iter 36 loss=0.09912052750587463\n",
      "epoch 63 iter 37 loss=0.07299129664897919\n",
      "epoch 63 iter 38 loss=0.10081608593463898\n",
      "epoch 63 iter 39 loss=0.07064532488584518\n",
      "epoch 63 iter 40 loss=0.1093827486038208\n",
      "epoch 63 iter 41 loss=0.08867402374744415\n",
      "epoch 63 iter 42 loss=0.09727493673563004\n",
      "epoch 63 iter 43 loss=0.11873351037502289\n",
      "epoch 63 iter 44 loss=0.13982146978378296\n",
      "epoch 63 iter 45 loss=0.08254354447126389\n",
      "epoch 63 iter 46 loss=0.09307270497083664\n",
      "epoch 63 iter 47 loss=0.17766398191452026\n",
      "epoch 63 iter 48 loss=0.08510362356901169\n",
      "epoch 63 iter 49 loss=0.08274313807487488\n",
      "epoch 63 iter 50 loss=0.09891204535961151\n",
      "epoch 63 iter 51 loss=0.09349796921014786\n",
      "epoch 63 iter 52 loss=0.12392512708902359\n",
      "epoch 63 iter 53 loss=0.10573072731494904\n",
      "epoch 63 iter 54 loss=0.13724319636821747\n",
      "epoch 63 iter 55 loss=0.12788595259189606\n",
      "epoch 63 iter 56 loss=0.0710982084274292\n",
      "epoch 63 iter 57 loss=0.07262372225522995\n",
      "epoch 63 iter 58 loss=0.0747351124882698\n",
      "epoch 63 iter 59 loss=0.08168759196996689\n",
      "epoch 63 iter 60 loss=0.09878579527139664\n",
      "epoch 63 iter 61 loss=0.11645238846540451\n",
      "epoch 63 iter 62 loss=0.11958640068769455\n",
      "epoch 63 iter 63 loss=0.08055229485034943\n",
      "epoch 63 iter 64 loss=0.16328276693820953\n",
      "epoch 63 iter 65 loss=0.09691384434700012\n",
      "epoch 63 iter 66 loss=0.07733075320720673\n",
      "epoch 63 iter 67 loss=0.11335378140211105\n",
      "epoch 63 iter 68 loss=0.07916812598705292\n",
      "epoch 63 iter 69 loss=0.10721570253372192\n",
      "epoch 63 iter 70 loss=0.0999808982014656\n",
      "epoch 63 iter 71 loss=0.053372181951999664\n",
      "epoch 63 iter 72 loss=0.085416778922081\n",
      "epoch 63 iter 73 loss=0.07116595655679703\n",
      "epoch 63 iter 74 loss=0.061774998903274536\n",
      "epoch 64 iter 0 loss=0.12589284777641296\n",
      "epoch 64 iter 1 loss=0.04748997092247009\n",
      "epoch 64 iter 2 loss=0.07757266610860825\n",
      "epoch 64 iter 3 loss=0.07192099839448929\n",
      "epoch 64 iter 4 loss=0.08849979937076569\n",
      "epoch 64 iter 5 loss=0.056529633700847626\n",
      "epoch 64 iter 6 loss=0.08950605988502502\n",
      "epoch 64 iter 7 loss=0.07171295583248138\n",
      "epoch 64 iter 8 loss=0.08435999602079391\n",
      "epoch 64 iter 9 loss=0.07131338119506836\n",
      "epoch 64 iter 10 loss=0.06505648046731949\n",
      "epoch 64 iter 11 loss=0.06645529717206955\n",
      "epoch 64 iter 12 loss=0.11762715131044388\n",
      "epoch 64 iter 13 loss=0.09457801282405853\n",
      "epoch 64 iter 14 loss=0.08316894620656967\n",
      "epoch 64 iter 15 loss=0.0938987135887146\n",
      "epoch 64 iter 16 loss=0.08744574338197708\n",
      "epoch 64 iter 17 loss=0.0721801295876503\n",
      "epoch 64 iter 18 loss=0.06208709999918938\n",
      "epoch 64 iter 19 loss=0.11020611226558685\n",
      "epoch 64 iter 20 loss=0.056675657629966736\n",
      "epoch 64 iter 21 loss=0.09472883492708206\n",
      "epoch 64 iter 22 loss=0.12010480463504791\n",
      "epoch 64 iter 23 loss=0.06155835837125778\n",
      "epoch 64 iter 24 loss=0.0828944593667984\n",
      "epoch 64 iter 25 loss=0.09725050628185272\n",
      "epoch 64 iter 26 loss=0.08444860577583313\n",
      "epoch 64 iter 27 loss=0.0625118687748909\n",
      "epoch 64 iter 28 loss=0.06277145445346832\n",
      "epoch 64 iter 29 loss=0.11953763663768768\n",
      "epoch 64 iter 30 loss=0.08160227537155151\n",
      "epoch 64 iter 31 loss=0.082985520362854\n",
      "epoch 64 iter 32 loss=0.060243211686611176\n",
      "epoch 64 iter 33 loss=0.07939554005861282\n",
      "epoch 64 iter 34 loss=0.0950261577963829\n",
      "epoch 64 iter 35 loss=0.09459145367145538\n",
      "epoch 64 iter 36 loss=0.11677338182926178\n",
      "epoch 64 iter 37 loss=0.08325248211622238\n",
      "epoch 64 iter 38 loss=0.10850583761930466\n",
      "epoch 64 iter 39 loss=0.07792061567306519\n",
      "epoch 64 iter 40 loss=0.1703580617904663\n",
      "epoch 64 iter 41 loss=0.05884993448853493\n",
      "epoch 64 iter 42 loss=0.05576803907752037\n",
      "epoch 64 iter 43 loss=0.09070318937301636\n",
      "epoch 64 iter 44 loss=0.10724297165870667\n",
      "epoch 64 iter 45 loss=0.09882067143917084\n",
      "epoch 64 iter 46 loss=0.08010850846767426\n",
      "epoch 64 iter 47 loss=0.11654888838529587\n",
      "epoch 64 iter 48 loss=0.07200279086828232\n",
      "epoch 64 iter 49 loss=0.07149957120418549\n",
      "epoch 64 iter 50 loss=0.10803521424531937\n",
      "epoch 64 iter 51 loss=0.10083013772964478\n",
      "epoch 64 iter 52 loss=0.07824580371379852\n",
      "epoch 64 iter 53 loss=0.07028861343860626\n",
      "epoch 64 iter 54 loss=0.07308230549097061\n",
      "epoch 64 iter 55 loss=0.06491003930568695\n",
      "epoch 64 iter 56 loss=0.05497027561068535\n",
      "epoch 64 iter 57 loss=0.08841801434755325\n",
      "epoch 64 iter 58 loss=0.09999210387468338\n",
      "epoch 64 iter 59 loss=0.06174975633621216\n",
      "epoch 64 iter 60 loss=0.08337026834487915\n",
      "epoch 64 iter 61 loss=0.06373905390501022\n",
      "epoch 64 iter 62 loss=0.07904154807329178\n",
      "epoch 64 iter 63 loss=0.09611956775188446\n",
      "epoch 64 iter 64 loss=0.09645800292491913\n",
      "epoch 64 iter 65 loss=0.06863704323768616\n",
      "epoch 64 iter 66 loss=0.11678381264209747\n",
      "epoch 64 iter 67 loss=0.10281691700220108\n",
      "epoch 64 iter 68 loss=0.1296958327293396\n",
      "epoch 64 iter 69 loss=0.0882265716791153\n",
      "epoch 64 iter 70 loss=0.09786507487297058\n",
      "epoch 64 iter 71 loss=0.04957122728228569\n",
      "epoch 64 iter 72 loss=0.0980023443698883\n",
      "epoch 64 iter 73 loss=0.0983377993106842\n",
      "epoch 64 iter 74 loss=0.13164553046226501\n",
      "epoch 65 iter 0 loss=0.05304628983139992\n",
      "epoch 65 iter 1 loss=0.12218271195888519\n",
      "epoch 65 iter 2 loss=0.07837934046983719\n",
      "epoch 65 iter 3 loss=0.06333483755588531\n",
      "epoch 65 iter 4 loss=0.07883855700492859\n",
      "epoch 65 iter 5 loss=0.08086846768856049\n",
      "epoch 65 iter 6 loss=0.08045536279678345\n",
      "epoch 65 iter 7 loss=0.09070362150669098\n",
      "epoch 65 iter 8 loss=0.08680732548236847\n",
      "epoch 65 iter 9 loss=0.0764969065785408\n",
      "epoch 65 iter 10 loss=0.09727129340171814\n",
      "epoch 65 iter 11 loss=0.07942711561918259\n",
      "epoch 65 iter 12 loss=0.12013906985521317\n",
      "epoch 65 iter 13 loss=0.07763119041919708\n",
      "epoch 65 iter 14 loss=0.0775466337800026\n",
      "epoch 65 iter 15 loss=0.06808387488126755\n",
      "epoch 65 iter 16 loss=0.06342044472694397\n",
      "epoch 65 iter 17 loss=0.07500369101762772\n",
      "epoch 65 iter 18 loss=0.08642368763685226\n",
      "epoch 65 iter 19 loss=0.09340617060661316\n",
      "epoch 65 iter 20 loss=0.0661354809999466\n",
      "epoch 65 iter 21 loss=0.09329642355442047\n",
      "epoch 65 iter 22 loss=0.06025874242186546\n",
      "epoch 65 iter 23 loss=0.07657715678215027\n",
      "epoch 65 iter 24 loss=0.08269909769296646\n",
      "epoch 65 iter 25 loss=0.10297641158103943\n",
      "epoch 65 iter 26 loss=0.07739407569169998\n",
      "epoch 65 iter 27 loss=0.08604539185762405\n",
      "epoch 65 iter 28 loss=0.04205603897571564\n",
      "epoch 65 iter 29 loss=0.10559423267841339\n",
      "epoch 65 iter 30 loss=0.07870442420244217\n",
      "epoch 65 iter 31 loss=0.1281314492225647\n",
      "epoch 65 iter 32 loss=0.07208065688610077\n",
      "epoch 65 iter 33 loss=0.074859119951725\n",
      "epoch 65 iter 34 loss=0.08171733468770981\n",
      "epoch 65 iter 35 loss=0.08619768917560577\n",
      "epoch 65 iter 36 loss=0.08814791589975357\n",
      "epoch 65 iter 37 loss=0.09189684689044952\n",
      "epoch 65 iter 38 loss=0.036377567797899246\n",
      "epoch 65 iter 39 loss=0.10666682571172714\n",
      "epoch 65 iter 40 loss=0.06189802289009094\n",
      "epoch 65 iter 41 loss=0.1251118779182434\n",
      "epoch 65 iter 42 loss=0.11770568042993546\n",
      "epoch 65 iter 43 loss=0.0752105563879013\n",
      "epoch 65 iter 44 loss=0.07589150220155716\n",
      "epoch 65 iter 45 loss=0.05981962010264397\n",
      "epoch 65 iter 46 loss=0.0925452932715416\n",
      "epoch 65 iter 47 loss=0.091965451836586\n",
      "epoch 65 iter 48 loss=0.09118638932704926\n",
      "epoch 65 iter 49 loss=0.0702638030052185\n",
      "epoch 65 iter 50 loss=0.11745624244213104\n",
      "epoch 65 iter 51 loss=0.0700153335928917\n",
      "epoch 65 iter 52 loss=0.09034913778305054\n",
      "epoch 65 iter 53 loss=0.07428759336471558\n",
      "epoch 65 iter 54 loss=0.07830599695444107\n",
      "epoch 65 iter 55 loss=0.1426282525062561\n",
      "epoch 65 iter 56 loss=0.07655815780162811\n",
      "epoch 65 iter 57 loss=0.08540040999650955\n",
      "epoch 65 iter 58 loss=0.0634460523724556\n",
      "epoch 65 iter 59 loss=0.06106237694621086\n",
      "epoch 65 iter 60 loss=0.13453969359397888\n",
      "epoch 65 iter 61 loss=0.12919019162654877\n",
      "epoch 65 iter 62 loss=0.06983621418476105\n",
      "epoch 65 iter 63 loss=0.10514360666275024\n",
      "epoch 65 iter 64 loss=0.05629045516252518\n",
      "epoch 65 iter 65 loss=0.08398601412773132\n",
      "epoch 65 iter 66 loss=0.056634362787008286\n",
      "epoch 65 iter 67 loss=0.08715318888425827\n",
      "epoch 65 iter 68 loss=0.08143970370292664\n",
      "epoch 65 iter 69 loss=0.055115796625614166\n",
      "epoch 65 iter 70 loss=0.11738268285989761\n",
      "epoch 65 iter 71 loss=0.05185309424996376\n",
      "epoch 65 iter 72 loss=0.07006306946277618\n",
      "epoch 65 iter 73 loss=0.1039402186870575\n",
      "epoch 65 iter 74 loss=0.061285294592380524\n",
      "epoch 66 iter 0 loss=0.06579957902431488\n",
      "epoch 66 iter 1 loss=0.09596861898899078\n",
      "epoch 66 iter 2 loss=0.07293855398893356\n",
      "epoch 66 iter 3 loss=0.08368213474750519\n",
      "epoch 66 iter 4 loss=0.06675985455513\n",
      "epoch 66 iter 5 loss=0.09092450886964798\n",
      "epoch 66 iter 6 loss=0.11243792623281479\n",
      "epoch 66 iter 7 loss=0.08331400901079178\n",
      "epoch 66 iter 8 loss=0.06788193434476852\n",
      "epoch 66 iter 9 loss=0.05267483741044998\n",
      "epoch 66 iter 10 loss=0.08766452223062515\n",
      "epoch 66 iter 11 loss=0.07627131044864655\n",
      "epoch 66 iter 12 loss=0.08302368968725204\n",
      "epoch 66 iter 13 loss=0.13694541156291962\n",
      "epoch 66 iter 14 loss=0.052670784294605255\n",
      "epoch 66 iter 15 loss=0.0847744345664978\n",
      "epoch 66 iter 16 loss=0.09275264292955399\n",
      "epoch 66 iter 17 loss=0.13942313194274902\n",
      "epoch 66 iter 18 loss=0.12067355960607529\n",
      "epoch 66 iter 19 loss=0.04336506500840187\n",
      "epoch 66 iter 20 loss=0.08428236842155457\n",
      "epoch 66 iter 21 loss=0.053806111216545105\n",
      "epoch 66 iter 22 loss=0.07317406684160233\n",
      "epoch 66 iter 23 loss=0.10115063190460205\n",
      "epoch 66 iter 24 loss=0.08265125006437302\n",
      "epoch 66 iter 25 loss=0.10837550461292267\n",
      "epoch 66 iter 26 loss=0.09439808130264282\n",
      "epoch 66 iter 27 loss=0.0692986473441124\n",
      "epoch 66 iter 28 loss=0.12547701597213745\n",
      "epoch 66 iter 29 loss=0.08895444124937057\n",
      "epoch 66 iter 30 loss=0.044472821056842804\n",
      "epoch 66 iter 31 loss=0.05083506926894188\n",
      "epoch 66 iter 32 loss=0.09265092015266418\n",
      "epoch 66 iter 33 loss=0.06626350432634354\n",
      "epoch 66 iter 34 loss=0.07143464684486389\n",
      "epoch 66 iter 35 loss=0.11908168345689774\n",
      "epoch 66 iter 36 loss=0.0636206716299057\n",
      "epoch 66 iter 37 loss=0.06374967098236084\n",
      "epoch 66 iter 38 loss=0.07458405196666718\n",
      "epoch 66 iter 39 loss=0.08785782754421234\n",
      "epoch 66 iter 40 loss=0.10178360342979431\n",
      "epoch 66 iter 41 loss=0.0746924951672554\n",
      "epoch 66 iter 42 loss=0.09944639354944229\n",
      "epoch 66 iter 43 loss=0.08709017187356949\n",
      "epoch 66 iter 44 loss=0.06120564043521881\n",
      "epoch 66 iter 45 loss=0.08715948462486267\n",
      "epoch 66 iter 46 loss=0.07556124776601791\n",
      "epoch 66 iter 47 loss=0.08273494988679886\n",
      "epoch 66 iter 48 loss=0.08405350893735886\n",
      "epoch 66 iter 49 loss=0.05833365023136139\n",
      "epoch 66 iter 50 loss=0.07705496996641159\n",
      "epoch 66 iter 51 loss=0.0639776661992073\n",
      "epoch 66 iter 52 loss=0.12715379893779755\n",
      "epoch 66 iter 53 loss=0.05832876265048981\n",
      "epoch 66 iter 54 loss=0.09291167557239532\n",
      "epoch 66 iter 55 loss=0.09683410823345184\n",
      "epoch 66 iter 56 loss=0.0791962593793869\n",
      "epoch 66 iter 57 loss=0.09812052547931671\n",
      "epoch 66 iter 58 loss=0.07385481148958206\n",
      "epoch 66 iter 59 loss=0.08457294851541519\n",
      "epoch 66 iter 60 loss=0.06273587793111801\n",
      "epoch 66 iter 61 loss=0.041809361428022385\n",
      "epoch 66 iter 62 loss=0.061647795140743256\n",
      "epoch 66 iter 63 loss=0.044684700667858124\n",
      "epoch 66 iter 64 loss=0.09682148694992065\n",
      "epoch 66 iter 65 loss=0.07992938160896301\n",
      "epoch 66 iter 66 loss=0.04235333204269409\n",
      "epoch 66 iter 67 loss=0.10456321388483047\n",
      "epoch 66 iter 68 loss=0.06682845205068588\n",
      "epoch 66 iter 69 loss=0.08774437010288239\n",
      "epoch 66 iter 70 loss=0.1341070830821991\n",
      "epoch 66 iter 71 loss=0.05945057049393654\n",
      "epoch 66 iter 72 loss=0.08060167729854584\n",
      "epoch 66 iter 73 loss=0.0860847532749176\n",
      "epoch 66 iter 74 loss=0.12196401506662369\n",
      "epoch 67 iter 0 loss=0.08040794730186462\n",
      "epoch 67 iter 1 loss=0.06744638830423355\n",
      "epoch 67 iter 2 loss=0.09199263900518417\n",
      "epoch 67 iter 3 loss=0.09760221093893051\n",
      "epoch 67 iter 4 loss=0.0761454626917839\n",
      "epoch 67 iter 5 loss=0.09897557646036148\n",
      "epoch 67 iter 6 loss=0.06341012567281723\n",
      "epoch 67 iter 7 loss=0.05629819631576538\n",
      "epoch 67 iter 8 loss=0.07534381747245789\n",
      "epoch 67 iter 9 loss=0.08447903394699097\n",
      "epoch 67 iter 10 loss=0.0627397820353508\n",
      "epoch 67 iter 11 loss=0.06854958087205887\n",
      "epoch 67 iter 12 loss=0.058696649968624115\n",
      "epoch 67 iter 13 loss=0.0946669727563858\n",
      "epoch 67 iter 14 loss=0.0785110592842102\n",
      "epoch 67 iter 15 loss=0.05962544307112694\n",
      "epoch 67 iter 16 loss=0.1290750801563263\n",
      "epoch 67 iter 17 loss=0.08354079723358154\n",
      "epoch 67 iter 18 loss=0.06243397668004036\n",
      "epoch 67 iter 19 loss=0.06508747488260269\n",
      "epoch 67 iter 20 loss=0.0783858597278595\n",
      "epoch 67 iter 21 loss=0.050833750516176224\n",
      "epoch 67 iter 22 loss=0.09185045957565308\n",
      "epoch 67 iter 23 loss=0.0810011625289917\n",
      "epoch 67 iter 24 loss=0.08288374543190002\n",
      "epoch 67 iter 25 loss=0.06568862497806549\n",
      "epoch 67 iter 26 loss=0.08498312532901764\n",
      "epoch 67 iter 27 loss=0.0618666373193264\n",
      "epoch 67 iter 28 loss=0.09089911729097366\n",
      "epoch 67 iter 29 loss=0.0858769491314888\n",
      "epoch 67 iter 30 loss=0.12866859138011932\n",
      "epoch 67 iter 31 loss=0.08975054323673248\n",
      "epoch 67 iter 32 loss=0.05160011723637581\n",
      "epoch 67 iter 33 loss=0.08277799189090729\n",
      "epoch 67 iter 34 loss=0.07877728343009949\n",
      "epoch 67 iter 35 loss=0.06202472746372223\n",
      "epoch 67 iter 36 loss=0.09092815965414047\n",
      "epoch 67 iter 37 loss=0.11893739551305771\n",
      "epoch 67 iter 38 loss=0.040124885737895966\n",
      "epoch 67 iter 39 loss=0.11469026654958725\n",
      "epoch 67 iter 40 loss=0.10826043039560318\n",
      "epoch 67 iter 41 loss=0.11965816468000412\n",
      "epoch 67 iter 42 loss=0.10453470051288605\n",
      "epoch 67 iter 43 loss=0.08963668346405029\n",
      "epoch 67 iter 44 loss=0.09599993377923965\n",
      "epoch 67 iter 45 loss=0.06231824681162834\n",
      "epoch 67 iter 46 loss=0.08536498248577118\n",
      "epoch 67 iter 47 loss=0.11561351269483566\n",
      "epoch 67 iter 48 loss=0.07083162665367126\n",
      "epoch 67 iter 49 loss=0.07277624309062958\n",
      "epoch 67 iter 50 loss=0.10487154871225357\n",
      "epoch 67 iter 51 loss=0.060594066977500916\n",
      "epoch 67 iter 52 loss=0.09229728579521179\n",
      "epoch 67 iter 53 loss=0.05652705207467079\n",
      "epoch 67 iter 54 loss=0.09381802380084991\n",
      "epoch 67 iter 55 loss=0.05985221266746521\n",
      "epoch 67 iter 56 loss=0.13142016530036926\n",
      "epoch 67 iter 57 loss=0.0804402306675911\n",
      "epoch 67 iter 58 loss=0.07222398370504379\n",
      "epoch 67 iter 59 loss=0.07650862634181976\n",
      "epoch 67 iter 60 loss=0.08578795939683914\n",
      "epoch 67 iter 61 loss=0.05897771194577217\n",
      "epoch 67 iter 62 loss=0.11033869534730911\n",
      "epoch 67 iter 63 loss=0.05200301855802536\n",
      "epoch 67 iter 64 loss=0.1205778643488884\n",
      "epoch 67 iter 65 loss=0.08557625859975815\n",
      "epoch 67 iter 66 loss=0.12133654952049255\n",
      "epoch 67 iter 67 loss=0.08623643219470978\n",
      "epoch 67 iter 68 loss=0.07577279210090637\n",
      "epoch 67 iter 69 loss=0.06556370109319687\n",
      "epoch 67 iter 70 loss=0.06625001132488251\n",
      "epoch 67 iter 71 loss=0.06402700394392014\n",
      "epoch 67 iter 72 loss=0.07071531563997269\n",
      "epoch 67 iter 73 loss=0.07626497745513916\n",
      "epoch 67 iter 74 loss=0.10383256524801254\n",
      "epoch 68 iter 0 loss=0.0662364736199379\n",
      "epoch 68 iter 1 loss=0.060632869601249695\n",
      "epoch 68 iter 2 loss=0.06732427328824997\n",
      "epoch 68 iter 3 loss=0.07370279729366302\n",
      "epoch 68 iter 4 loss=0.08870115876197815\n",
      "epoch 68 iter 5 loss=0.09954721480607986\n",
      "epoch 68 iter 6 loss=0.10518839955329895\n",
      "epoch 68 iter 7 loss=0.12256676703691483\n",
      "epoch 68 iter 8 loss=0.11908875405788422\n",
      "epoch 68 iter 9 loss=0.07269377261400223\n",
      "epoch 68 iter 10 loss=0.06561066955327988\n",
      "epoch 68 iter 11 loss=0.08707287162542343\n",
      "epoch 68 iter 12 loss=0.07099688798189163\n",
      "epoch 68 iter 13 loss=0.04972274601459503\n",
      "epoch 68 iter 14 loss=0.10674837976694107\n",
      "epoch 68 iter 15 loss=0.10899579524993896\n",
      "epoch 68 iter 16 loss=0.07918227463960648\n",
      "epoch 68 iter 17 loss=0.07601823657751083\n",
      "epoch 68 iter 18 loss=0.0810869038105011\n",
      "epoch 68 iter 19 loss=0.04962274432182312\n",
      "epoch 68 iter 20 loss=0.053365737199783325\n",
      "epoch 68 iter 21 loss=0.12878502905368805\n",
      "epoch 68 iter 22 loss=0.08227618038654327\n",
      "epoch 68 iter 23 loss=0.08314675092697144\n",
      "epoch 68 iter 24 loss=0.07842089980840683\n",
      "epoch 68 iter 25 loss=0.06806788593530655\n",
      "epoch 68 iter 26 loss=0.05667714774608612\n",
      "epoch 68 iter 27 loss=0.08202549070119858\n",
      "epoch 68 iter 28 loss=0.1388407051563263\n",
      "epoch 68 iter 29 loss=0.08372973650693893\n",
      "epoch 68 iter 30 loss=0.07327720522880554\n",
      "epoch 68 iter 31 loss=0.08440279960632324\n",
      "epoch 68 iter 32 loss=0.061199914664030075\n",
      "epoch 68 iter 33 loss=0.06876055896282196\n",
      "epoch 68 iter 34 loss=0.07751841098070145\n",
      "epoch 68 iter 35 loss=0.08527497202157974\n",
      "epoch 68 iter 36 loss=0.03922223299741745\n",
      "epoch 68 iter 37 loss=0.08043687045574188\n",
      "epoch 68 iter 38 loss=0.11284787952899933\n",
      "epoch 68 iter 39 loss=0.06212424114346504\n",
      "epoch 68 iter 40 loss=0.09201176464557648\n",
      "epoch 68 iter 41 loss=0.0732593908905983\n",
      "epoch 68 iter 42 loss=0.09884125739336014\n",
      "epoch 68 iter 43 loss=0.07619015127420425\n",
      "epoch 68 iter 44 loss=0.08341430127620697\n",
      "epoch 68 iter 45 loss=0.04002730920910835\n",
      "epoch 68 iter 46 loss=0.060343701392412186\n",
      "epoch 68 iter 47 loss=0.09582789987325668\n",
      "epoch 68 iter 48 loss=0.07391806691884995\n",
      "epoch 68 iter 49 loss=0.11805367469787598\n",
      "epoch 68 iter 50 loss=0.06511181592941284\n",
      "epoch 68 iter 51 loss=0.08027712255716324\n",
      "epoch 68 iter 52 loss=0.09857628494501114\n",
      "epoch 68 iter 53 loss=0.0457850806415081\n",
      "epoch 68 iter 54 loss=0.10033612698316574\n",
      "epoch 68 iter 55 loss=0.08126978576183319\n",
      "epoch 68 iter 56 loss=0.08326885849237442\n",
      "epoch 68 iter 57 loss=0.07580220699310303\n",
      "epoch 68 iter 58 loss=0.037173911929130554\n",
      "epoch 68 iter 59 loss=0.07403184473514557\n",
      "epoch 68 iter 60 loss=0.07165706902742386\n",
      "epoch 68 iter 61 loss=0.07062593847513199\n",
      "epoch 68 iter 62 loss=0.1084425076842308\n",
      "epoch 68 iter 63 loss=0.1443302184343338\n",
      "epoch 68 iter 64 loss=0.07448349893093109\n",
      "epoch 68 iter 65 loss=0.056613050401210785\n",
      "epoch 68 iter 66 loss=0.04699498414993286\n",
      "epoch 68 iter 67 loss=0.07266175001859665\n",
      "epoch 68 iter 68 loss=0.09830828756093979\n",
      "epoch 68 iter 69 loss=0.0686289444565773\n",
      "epoch 68 iter 70 loss=0.07278819382190704\n",
      "epoch 68 iter 71 loss=0.08406831324100494\n",
      "epoch 68 iter 72 loss=0.056032367050647736\n",
      "epoch 68 iter 73 loss=0.07851099222898483\n",
      "epoch 68 iter 74 loss=0.08115503937005997\n",
      "epoch 69 iter 0 loss=0.07676248997449875\n",
      "epoch 69 iter 1 loss=0.09235310554504395\n",
      "epoch 69 iter 2 loss=0.06941454112529755\n",
      "epoch 69 iter 3 loss=0.048957087099552155\n",
      "epoch 69 iter 4 loss=0.12535695731639862\n",
      "epoch 69 iter 5 loss=0.04684322327375412\n",
      "epoch 69 iter 6 loss=0.09401167929172516\n",
      "epoch 69 iter 7 loss=0.09420064836740494\n",
      "epoch 69 iter 8 loss=0.11312639713287354\n",
      "epoch 69 iter 9 loss=0.07464254647493362\n",
      "epoch 69 iter 10 loss=0.09124444425106049\n",
      "epoch 69 iter 11 loss=0.06188661605119705\n",
      "epoch 69 iter 12 loss=0.05704273283481598\n",
      "epoch 69 iter 13 loss=0.06610860675573349\n",
      "epoch 69 iter 14 loss=0.05097736418247223\n",
      "epoch 69 iter 15 loss=0.10499922931194305\n",
      "epoch 69 iter 16 loss=0.07598207890987396\n",
      "epoch 69 iter 17 loss=0.05285387486219406\n",
      "epoch 69 iter 18 loss=0.052017103880643845\n",
      "epoch 69 iter 19 loss=0.0782279446721077\n",
      "epoch 69 iter 20 loss=0.07823193818330765\n",
      "epoch 69 iter 21 loss=0.057857196778059006\n",
      "epoch 69 iter 22 loss=0.11067948490381241\n",
      "epoch 69 iter 23 loss=0.06994342803955078\n",
      "epoch 69 iter 24 loss=0.061239369213581085\n",
      "epoch 69 iter 25 loss=0.09565376490354538\n",
      "epoch 69 iter 26 loss=0.059199608862400055\n",
      "epoch 69 iter 27 loss=0.050006695091724396\n",
      "epoch 69 iter 28 loss=0.04144987836480141\n",
      "epoch 69 iter 29 loss=0.07448801398277283\n",
      "epoch 69 iter 30 loss=0.0749957337975502\n",
      "epoch 69 iter 31 loss=0.07688701897859573\n",
      "epoch 69 iter 32 loss=0.08127531409263611\n",
      "epoch 69 iter 33 loss=0.12358386814594269\n",
      "epoch 69 iter 34 loss=0.07311590015888214\n",
      "epoch 69 iter 35 loss=0.09723266959190369\n",
      "epoch 69 iter 36 loss=0.06828390061855316\n",
      "epoch 69 iter 37 loss=0.0756312757730484\n",
      "epoch 69 iter 38 loss=0.0725075900554657\n",
      "epoch 69 iter 39 loss=0.08838093280792236\n",
      "epoch 69 iter 40 loss=0.053738661110401154\n",
      "epoch 69 iter 41 loss=0.08784602582454681\n",
      "epoch 69 iter 42 loss=0.10923374444246292\n",
      "epoch 69 iter 43 loss=0.08459991216659546\n",
      "epoch 69 iter 44 loss=0.06672681123018265\n",
      "epoch 69 iter 45 loss=0.05326016992330551\n",
      "epoch 69 iter 46 loss=0.060162365436553955\n",
      "epoch 69 iter 47 loss=0.07652967423200607\n",
      "epoch 69 iter 48 loss=0.06658156216144562\n",
      "epoch 69 iter 49 loss=0.09084513783454895\n",
      "epoch 69 iter 50 loss=0.05043164640665054\n",
      "epoch 69 iter 51 loss=0.04473278298974037\n",
      "epoch 69 iter 52 loss=0.11923577636480331\n",
      "epoch 69 iter 53 loss=0.055562540888786316\n",
      "epoch 69 iter 54 loss=0.06391371786594391\n",
      "epoch 69 iter 55 loss=0.142165407538414\n",
      "epoch 69 iter 56 loss=0.051545072346925735\n",
      "epoch 69 iter 57 loss=0.12047337740659714\n",
      "epoch 69 iter 58 loss=0.10479485243558884\n",
      "epoch 69 iter 59 loss=0.09497258812189102\n",
      "epoch 69 iter 60 loss=0.08752110600471497\n",
      "epoch 69 iter 61 loss=0.04984552413225174\n",
      "epoch 69 iter 62 loss=0.06354741752147675\n",
      "epoch 69 iter 63 loss=0.0791468545794487\n",
      "epoch 69 iter 64 loss=0.07766182720661163\n",
      "epoch 69 iter 65 loss=0.042032912373542786\n",
      "epoch 69 iter 66 loss=0.0872981920838356\n",
      "epoch 69 iter 67 loss=0.06093889847397804\n",
      "epoch 69 iter 68 loss=0.09060477465391159\n",
      "epoch 69 iter 69 loss=0.048550158739089966\n",
      "epoch 69 iter 70 loss=0.08515019714832306\n",
      "epoch 69 iter 71 loss=0.092642642557621\n",
      "epoch 69 iter 72 loss=0.09550169855356216\n",
      "epoch 69 iter 73 loss=0.08028393983840942\n",
      "epoch 69 iter 74 loss=0.08946989476680756\n",
      "epoch 70 iter 0 loss=0.06626914441585541\n",
      "epoch 70 iter 1 loss=0.07634719461202621\n",
      "epoch 70 iter 2 loss=0.07789663225412369\n",
      "epoch 70 iter 3 loss=0.06016317382454872\n",
      "epoch 70 iter 4 loss=0.10222012549638748\n",
      "epoch 70 iter 5 loss=0.10112941265106201\n",
      "epoch 70 iter 6 loss=0.1289331316947937\n",
      "epoch 70 iter 7 loss=0.08881941437721252\n",
      "epoch 70 iter 8 loss=0.08837813138961792\n",
      "epoch 70 iter 9 loss=0.05121178552508354\n",
      "epoch 70 iter 10 loss=0.07210215926170349\n",
      "epoch 70 iter 11 loss=0.05894927307963371\n",
      "epoch 70 iter 12 loss=0.08970705419778824\n",
      "epoch 70 iter 13 loss=0.054959483444690704\n",
      "epoch 70 iter 14 loss=0.10990703105926514\n",
      "epoch 70 iter 15 loss=0.11342675238847733\n",
      "epoch 70 iter 16 loss=0.07695262134075165\n",
      "epoch 70 iter 17 loss=0.0629548653960228\n",
      "epoch 70 iter 18 loss=0.06781819462776184\n",
      "epoch 70 iter 19 loss=0.04307633265852928\n",
      "epoch 70 iter 20 loss=0.07494909316301346\n",
      "epoch 70 iter 21 loss=0.05077100545167923\n",
      "epoch 70 iter 22 loss=0.07264561206102371\n",
      "epoch 70 iter 23 loss=0.09767503291368484\n",
      "epoch 70 iter 24 loss=0.06980744749307632\n",
      "epoch 70 iter 25 loss=0.13898393511772156\n",
      "epoch 70 iter 26 loss=0.06811076402664185\n",
      "epoch 70 iter 27 loss=0.09435517340898514\n",
      "epoch 70 iter 28 loss=0.08144520968198776\n",
      "epoch 70 iter 29 loss=0.052830781787633896\n",
      "epoch 70 iter 30 loss=0.0949699729681015\n",
      "epoch 70 iter 31 loss=0.08068802207708359\n",
      "epoch 70 iter 32 loss=0.07867589592933655\n",
      "epoch 70 iter 33 loss=0.10687121748924255\n",
      "epoch 70 iter 34 loss=0.09635243564844131\n",
      "epoch 70 iter 35 loss=0.1060444638133049\n",
      "epoch 70 iter 36 loss=0.053991083055734634\n",
      "epoch 70 iter 37 loss=0.07335217297077179\n",
      "epoch 70 iter 38 loss=0.09250666201114655\n",
      "epoch 70 iter 39 loss=0.07934103906154633\n",
      "epoch 70 iter 40 loss=0.08250219374895096\n",
      "epoch 70 iter 41 loss=0.043499916791915894\n",
      "epoch 70 iter 42 loss=0.08784323185682297\n",
      "epoch 70 iter 43 loss=0.09963846206665039\n",
      "epoch 70 iter 44 loss=0.06390547007322311\n",
      "epoch 70 iter 45 loss=0.06754753738641739\n",
      "epoch 70 iter 46 loss=0.08911800384521484\n",
      "epoch 70 iter 47 loss=0.07039780914783478\n",
      "epoch 70 iter 48 loss=0.10475943982601166\n",
      "epoch 70 iter 49 loss=0.07317348569631577\n",
      "epoch 70 iter 50 loss=0.06721565127372742\n",
      "epoch 70 iter 51 loss=0.07774563878774643\n",
      "epoch 70 iter 52 loss=0.10036374628543854\n",
      "epoch 70 iter 53 loss=0.09460984915494919\n",
      "epoch 70 iter 54 loss=0.07391895353794098\n",
      "epoch 70 iter 55 loss=0.08784735947847366\n",
      "epoch 70 iter 56 loss=0.05441538989543915\n",
      "epoch 70 iter 57 loss=0.07861200720071793\n",
      "epoch 70 iter 58 loss=0.06126784160733223\n",
      "epoch 70 iter 59 loss=0.05468883737921715\n",
      "epoch 70 iter 60 loss=0.05679238960146904\n",
      "epoch 70 iter 61 loss=0.04148669168353081\n",
      "epoch 70 iter 62 loss=0.07469797134399414\n",
      "epoch 70 iter 63 loss=0.06147392466664314\n",
      "epoch 70 iter 64 loss=0.05077407881617546\n",
      "epoch 70 iter 65 loss=0.08477257937192917\n",
      "epoch 70 iter 66 loss=0.048131298273801804\n",
      "epoch 70 iter 67 loss=0.07784345746040344\n",
      "epoch 70 iter 68 loss=0.06695673614740372\n",
      "epoch 70 iter 69 loss=0.10537931323051453\n",
      "epoch 70 iter 70 loss=0.05559064447879791\n",
      "epoch 70 iter 71 loss=0.10619151592254639\n",
      "epoch 70 iter 72 loss=0.10140585154294968\n",
      "epoch 70 iter 73 loss=0.05142714083194733\n",
      "epoch 70 iter 74 loss=0.07522277534008026\n",
      "epoch 71 iter 0 loss=0.05636156350374222\n",
      "epoch 71 iter 1 loss=0.07119137793779373\n",
      "epoch 71 iter 2 loss=0.052373256534338\n",
      "epoch 71 iter 3 loss=0.08237304538488388\n",
      "epoch 71 iter 4 loss=0.08200953155755997\n",
      "epoch 71 iter 5 loss=0.10178005695343018\n",
      "epoch 71 iter 6 loss=0.09203382581472397\n",
      "epoch 71 iter 7 loss=0.08070988208055496\n",
      "epoch 71 iter 8 loss=0.06630534678697586\n",
      "epoch 71 iter 9 loss=0.06523162126541138\n",
      "epoch 71 iter 10 loss=0.09137968719005585\n",
      "epoch 71 iter 11 loss=0.06000687927007675\n",
      "epoch 71 iter 12 loss=0.05567585676908493\n",
      "epoch 71 iter 13 loss=0.06731032580137253\n",
      "epoch 71 iter 14 loss=0.0526103712618351\n",
      "epoch 71 iter 15 loss=0.0830225795507431\n",
      "epoch 71 iter 16 loss=0.0891159251332283\n",
      "epoch 71 iter 17 loss=0.08308955281972885\n",
      "epoch 71 iter 18 loss=0.08968232572078705\n",
      "epoch 71 iter 19 loss=0.13048022985458374\n",
      "epoch 71 iter 20 loss=0.11816971004009247\n",
      "epoch 71 iter 21 loss=0.07256948202848434\n",
      "epoch 71 iter 22 loss=0.07598802447319031\n",
      "epoch 71 iter 23 loss=0.08237827569246292\n",
      "epoch 71 iter 24 loss=0.08897841721773148\n",
      "epoch 71 iter 25 loss=0.10799948126077652\n",
      "epoch 71 iter 26 loss=0.08392586559057236\n",
      "epoch 71 iter 27 loss=0.07133908569812775\n",
      "epoch 71 iter 28 loss=0.07189305126667023\n",
      "epoch 71 iter 29 loss=0.08495522290468216\n",
      "epoch 71 iter 30 loss=0.058148857206106186\n",
      "epoch 71 iter 31 loss=0.059229474514722824\n",
      "epoch 71 iter 32 loss=0.07537399232387543\n",
      "epoch 71 iter 33 loss=0.0592283234000206\n",
      "epoch 71 iter 34 loss=0.08671223372220993\n",
      "epoch 71 iter 35 loss=0.11799004673957825\n",
      "epoch 71 iter 36 loss=0.059478357434272766\n",
      "epoch 71 iter 37 loss=0.06400184333324432\n",
      "epoch 71 iter 38 loss=0.06966545432806015\n",
      "epoch 71 iter 39 loss=0.0661291629076004\n",
      "epoch 71 iter 40 loss=0.08033803105354309\n",
      "epoch 71 iter 41 loss=0.09417673200368881\n",
      "epoch 71 iter 42 loss=0.05617261677980423\n",
      "epoch 71 iter 43 loss=0.07405533641576767\n",
      "epoch 71 iter 44 loss=0.053914494812488556\n",
      "epoch 71 iter 45 loss=0.0861353650689125\n",
      "epoch 71 iter 46 loss=0.07694365829229355\n",
      "epoch 71 iter 47 loss=0.08586887270212173\n",
      "epoch 71 iter 48 loss=0.06996220350265503\n",
      "epoch 71 iter 49 loss=0.057272251695394516\n",
      "epoch 71 iter 50 loss=0.11129043996334076\n",
      "epoch 71 iter 51 loss=0.07932974398136139\n",
      "epoch 71 iter 52 loss=0.060260478407144547\n",
      "epoch 71 iter 53 loss=0.057425640523433685\n",
      "epoch 71 iter 54 loss=0.06495936959981918\n",
      "epoch 71 iter 55 loss=0.07824226468801498\n",
      "epoch 71 iter 56 loss=0.07256165146827698\n",
      "epoch 71 iter 57 loss=0.04972074180841446\n",
      "epoch 71 iter 58 loss=0.11098509281873703\n",
      "epoch 71 iter 59 loss=0.09469408541917801\n",
      "epoch 71 iter 60 loss=0.060884565114974976\n",
      "epoch 71 iter 61 loss=0.09028152376413345\n",
      "epoch 71 iter 62 loss=0.047077056020498276\n",
      "epoch 71 iter 63 loss=0.16756571829319\n",
      "epoch 71 iter 64 loss=0.06380046904087067\n",
      "epoch 71 iter 65 loss=0.07629823684692383\n",
      "epoch 71 iter 66 loss=0.06930041313171387\n",
      "epoch 71 iter 67 loss=0.07908818870782852\n",
      "epoch 71 iter 68 loss=0.08367414027452469\n",
      "epoch 71 iter 69 loss=0.06485791504383087\n",
      "epoch 71 iter 70 loss=0.0775078609585762\n",
      "epoch 71 iter 71 loss=0.1255308836698532\n",
      "epoch 71 iter 72 loss=0.06134863570332527\n",
      "epoch 71 iter 73 loss=0.08448349684476852\n",
      "epoch 71 iter 74 loss=0.07328301668167114\n",
      "epoch 72 iter 0 loss=0.08904915302991867\n",
      "epoch 72 iter 1 loss=0.05726732686161995\n",
      "epoch 72 iter 2 loss=0.06510566174983978\n",
      "epoch 72 iter 3 loss=0.04952697455883026\n",
      "epoch 72 iter 4 loss=0.08642476052045822\n",
      "epoch 72 iter 5 loss=0.06916841119527817\n",
      "epoch 72 iter 6 loss=0.08578013628721237\n",
      "epoch 72 iter 7 loss=0.09332650899887085\n",
      "epoch 72 iter 8 loss=0.07526613026857376\n",
      "epoch 72 iter 9 loss=0.08270388096570969\n",
      "epoch 72 iter 10 loss=0.11826381087303162\n",
      "epoch 72 iter 11 loss=0.06169247627258301\n",
      "epoch 72 iter 12 loss=0.103078193962574\n",
      "epoch 72 iter 13 loss=0.07769658416509628\n",
      "epoch 72 iter 14 loss=0.07696761190891266\n",
      "epoch 72 iter 15 loss=0.07420111447572708\n",
      "epoch 72 iter 16 loss=0.07504388689994812\n",
      "epoch 72 iter 17 loss=0.06650460511445999\n",
      "epoch 72 iter 18 loss=0.07546661049127579\n",
      "epoch 72 iter 19 loss=0.05114840716123581\n",
      "epoch 72 iter 20 loss=0.07112019509077072\n",
      "epoch 72 iter 21 loss=0.07632102817296982\n",
      "epoch 72 iter 22 loss=0.07069703191518784\n",
      "epoch 72 iter 23 loss=0.060279492288827896\n",
      "epoch 72 iter 24 loss=0.048075903207063675\n",
      "epoch 72 iter 25 loss=0.05062587559223175\n",
      "epoch 72 iter 26 loss=0.06436602771282196\n",
      "epoch 72 iter 27 loss=0.06621913611888885\n",
      "epoch 72 iter 28 loss=0.09264619648456573\n",
      "epoch 72 iter 29 loss=0.08762776851654053\n",
      "epoch 72 iter 30 loss=0.0711871087551117\n",
      "epoch 72 iter 31 loss=0.06818052381277084\n",
      "epoch 72 iter 32 loss=0.1139487475156784\n",
      "epoch 72 iter 33 loss=0.11669342964887619\n",
      "epoch 72 iter 34 loss=0.06019648164510727\n",
      "epoch 72 iter 35 loss=0.057307399809360504\n",
      "epoch 72 iter 36 loss=0.0652596578001976\n",
      "epoch 72 iter 37 loss=0.07808501273393631\n",
      "epoch 72 iter 38 loss=0.04558325931429863\n",
      "epoch 72 iter 39 loss=0.06016415357589722\n",
      "epoch 72 iter 40 loss=0.06635946035385132\n",
      "epoch 72 iter 41 loss=0.06424331665039062\n",
      "epoch 72 iter 42 loss=0.07858181744813919\n",
      "epoch 72 iter 43 loss=0.08004092425107956\n",
      "epoch 72 iter 44 loss=0.0971946194767952\n",
      "epoch 72 iter 45 loss=0.0651610940694809\n",
      "epoch 72 iter 46 loss=0.04755881056189537\n",
      "epoch 72 iter 47 loss=0.08605902642011642\n",
      "epoch 72 iter 48 loss=0.07537685334682465\n",
      "epoch 72 iter 49 loss=0.058842726051807404\n",
      "epoch 72 iter 50 loss=0.088833287358284\n",
      "epoch 72 iter 51 loss=0.04802611842751503\n",
      "epoch 72 iter 52 loss=0.07227015495300293\n",
      "epoch 72 iter 53 loss=0.04391370341181755\n",
      "epoch 72 iter 54 loss=0.08833804726600647\n",
      "epoch 72 iter 55 loss=0.09083276987075806\n",
      "epoch 72 iter 56 loss=0.3983044922351837\n",
      "epoch 72 iter 57 loss=0.08548220992088318\n",
      "epoch 72 iter 58 loss=0.12136993557214737\n",
      "epoch 72 iter 59 loss=0.21344967186450958\n",
      "epoch 72 iter 60 loss=0.1361107975244522\n",
      "epoch 72 iter 61 loss=0.1803579330444336\n",
      "epoch 72 iter 62 loss=0.09704264253377914\n",
      "epoch 72 iter 63 loss=0.119936004281044\n",
      "epoch 72 iter 64 loss=0.15537095069885254\n",
      "epoch 72 iter 65 loss=0.23751798272132874\n",
      "epoch 72 iter 66 loss=0.30330660939216614\n",
      "epoch 72 iter 67 loss=0.14670872688293457\n",
      "epoch 72 iter 68 loss=0.1440373808145523\n",
      "epoch 72 iter 69 loss=0.2329476922750473\n",
      "epoch 72 iter 70 loss=0.23443825542926788\n",
      "epoch 72 iter 71 loss=0.1879688948392868\n",
      "epoch 72 iter 72 loss=0.16981419920921326\n",
      "epoch 72 iter 73 loss=0.1529938280582428\n",
      "epoch 72 iter 74 loss=0.195696160197258\n",
      "epoch 73 iter 0 loss=0.12884864211082458\n",
      "epoch 73 iter 1 loss=0.2577601969242096\n",
      "epoch 73 iter 2 loss=0.15081211924552917\n",
      "epoch 73 iter 3 loss=0.16828615963459015\n",
      "epoch 73 iter 4 loss=0.2017364203929901\n",
      "epoch 73 iter 5 loss=0.19869334995746613\n",
      "epoch 73 iter 6 loss=0.23361851274967194\n",
      "epoch 73 iter 7 loss=0.22038356959819794\n",
      "epoch 73 iter 8 loss=0.08881499618291855\n",
      "epoch 73 iter 9 loss=0.29137730598449707\n",
      "epoch 73 iter 10 loss=0.16928865015506744\n",
      "epoch 73 iter 11 loss=0.10579478740692139\n",
      "epoch 73 iter 12 loss=0.13579565286636353\n",
      "epoch 73 iter 13 loss=0.1841810643672943\n",
      "epoch 73 iter 14 loss=0.16957195103168488\n",
      "epoch 73 iter 15 loss=0.14476829767227173\n",
      "epoch 73 iter 16 loss=0.11767259985208511\n",
      "epoch 73 iter 17 loss=0.23154360055923462\n",
      "epoch 73 iter 18 loss=0.14639556407928467\n",
      "epoch 73 iter 19 loss=0.12048765271902084\n",
      "epoch 73 iter 20 loss=0.1478976011276245\n",
      "epoch 73 iter 21 loss=0.11763539165258408\n",
      "epoch 73 iter 22 loss=0.4293076694011688\n",
      "epoch 73 iter 23 loss=0.15289351344108582\n",
      "epoch 73 iter 24 loss=0.1930791288614273\n",
      "epoch 73 iter 25 loss=0.33361515402793884\n",
      "epoch 73 iter 26 loss=0.23116375505924225\n",
      "epoch 73 iter 27 loss=0.17990483343601227\n",
      "epoch 73 iter 28 loss=0.1524304449558258\n",
      "epoch 73 iter 29 loss=0.2793336510658264\n",
      "epoch 73 iter 30 loss=0.12016461044549942\n",
      "epoch 73 iter 31 loss=0.1927376091480255\n",
      "epoch 73 iter 32 loss=0.1759081482887268\n",
      "epoch 73 iter 33 loss=0.13552142679691315\n",
      "epoch 73 iter 34 loss=0.2731803357601166\n",
      "epoch 73 iter 35 loss=0.18083995580673218\n",
      "epoch 73 iter 36 loss=0.2698475420475006\n",
      "epoch 73 iter 37 loss=0.1317901760339737\n",
      "epoch 73 iter 38 loss=0.16066664457321167\n",
      "epoch 73 iter 39 loss=0.14926913380622864\n",
      "epoch 73 iter 40 loss=0.3700595796108246\n",
      "epoch 73 iter 41 loss=0.11645402014255524\n",
      "epoch 73 iter 42 loss=0.15742036700248718\n",
      "epoch 73 iter 43 loss=0.12534283101558685\n",
      "epoch 73 iter 44 loss=0.1454002410173416\n",
      "epoch 73 iter 45 loss=0.18889181315898895\n",
      "epoch 73 iter 46 loss=0.12714490294456482\n",
      "epoch 73 iter 47 loss=0.13122887909412384\n",
      "epoch 73 iter 48 loss=0.1264904886484146\n",
      "epoch 73 iter 49 loss=0.15770366787910461\n",
      "epoch 73 iter 50 loss=0.1034012883901596\n",
      "epoch 73 iter 51 loss=0.09062711149454117\n",
      "epoch 73 iter 52 loss=0.267897367477417\n",
      "epoch 73 iter 53 loss=0.185345858335495\n",
      "epoch 73 iter 54 loss=0.13495033979415894\n",
      "epoch 73 iter 55 loss=0.11550629138946533\n",
      "epoch 73 iter 56 loss=0.19582201540470123\n",
      "epoch 73 iter 57 loss=0.21039460599422455\n",
      "epoch 73 iter 58 loss=0.17399755120277405\n",
      "epoch 73 iter 59 loss=0.2845819592475891\n",
      "epoch 73 iter 60 loss=0.12770862877368927\n",
      "epoch 73 iter 61 loss=0.1882212609052658\n",
      "epoch 73 iter 62 loss=0.16524900496006012\n",
      "epoch 73 iter 63 loss=0.12012732028961182\n",
      "epoch 73 iter 64 loss=0.1917620748281479\n",
      "epoch 73 iter 65 loss=0.17768347263336182\n",
      "epoch 73 iter 66 loss=0.12604717910289764\n",
      "epoch 73 iter 67 loss=0.13532106578350067\n",
      "epoch 73 iter 68 loss=0.2229524701833725\n",
      "epoch 73 iter 69 loss=0.09257270395755768\n",
      "epoch 73 iter 70 loss=0.16022339463233948\n",
      "epoch 73 iter 71 loss=0.0924990326166153\n",
      "epoch 73 iter 72 loss=0.07946634292602539\n",
      "epoch 73 iter 73 loss=0.31041449308395386\n",
      "epoch 73 iter 74 loss=0.08380626887083054\n",
      "epoch 74 iter 0 loss=0.13445426523685455\n",
      "epoch 74 iter 1 loss=0.08685605973005295\n",
      "epoch 74 iter 2 loss=0.2036091387271881\n",
      "epoch 74 iter 3 loss=0.10677556693553925\n",
      "epoch 74 iter 4 loss=0.17278341948986053\n",
      "epoch 74 iter 5 loss=0.13210374116897583\n",
      "epoch 74 iter 6 loss=0.09995579719543457\n",
      "epoch 74 iter 7 loss=0.14119385182857513\n",
      "epoch 74 iter 8 loss=0.1716475784778595\n",
      "epoch 74 iter 9 loss=0.09921479225158691\n",
      "epoch 74 iter 10 loss=0.1155804768204689\n",
      "epoch 74 iter 11 loss=0.1104486733675003\n",
      "epoch 74 iter 12 loss=0.09772911667823792\n",
      "epoch 74 iter 13 loss=0.09271056950092316\n",
      "epoch 74 iter 14 loss=0.07978076487779617\n",
      "epoch 74 iter 15 loss=0.12544521689414978\n",
      "epoch 74 iter 16 loss=0.06545379757881165\n",
      "epoch 74 iter 17 loss=0.11137863993644714\n",
      "epoch 74 iter 18 loss=0.09592954069375992\n",
      "epoch 74 iter 19 loss=0.19789698719978333\n",
      "epoch 74 iter 20 loss=0.09634071588516235\n",
      "epoch 74 iter 21 loss=0.10183937102556229\n",
      "epoch 74 iter 22 loss=0.09620073437690735\n",
      "epoch 74 iter 23 loss=0.07515943795442581\n",
      "epoch 74 iter 24 loss=0.11830197274684906\n",
      "epoch 74 iter 25 loss=0.14980125427246094\n",
      "epoch 74 iter 26 loss=0.13573764264583588\n",
      "epoch 74 iter 27 loss=0.1321825534105301\n",
      "epoch 74 iter 28 loss=0.13276581466197968\n",
      "epoch 74 iter 29 loss=0.09161555022001266\n",
      "epoch 74 iter 30 loss=0.08242780715227127\n",
      "epoch 74 iter 31 loss=0.09277599304914474\n",
      "epoch 74 iter 32 loss=0.12227781862020493\n",
      "epoch 74 iter 33 loss=0.09991765767335892\n",
      "epoch 74 iter 34 loss=0.10767532140016556\n",
      "epoch 74 iter 35 loss=0.0960962325334549\n",
      "epoch 74 iter 36 loss=0.14856603741645813\n",
      "epoch 74 iter 37 loss=0.10822267830371857\n",
      "epoch 74 iter 38 loss=0.09303467720746994\n",
      "epoch 74 iter 39 loss=0.08831063657999039\n",
      "epoch 74 iter 40 loss=0.07239697873592377\n",
      "epoch 74 iter 41 loss=0.1285845935344696\n",
      "epoch 74 iter 42 loss=0.10816190391778946\n",
      "epoch 74 iter 43 loss=0.07729461789131165\n",
      "epoch 74 iter 44 loss=0.08580420166254044\n",
      "epoch 74 iter 45 loss=0.31325578689575195\n",
      "epoch 74 iter 46 loss=0.2194606065750122\n",
      "epoch 74 iter 47 loss=0.10164982825517654\n",
      "epoch 74 iter 48 loss=0.09440199285745621\n",
      "epoch 74 iter 49 loss=0.17291930317878723\n",
      "epoch 74 iter 50 loss=0.10900837928056717\n",
      "epoch 74 iter 51 loss=0.13672447204589844\n",
      "epoch 74 iter 52 loss=0.08792388439178467\n",
      "epoch 74 iter 53 loss=0.05095034837722778\n",
      "epoch 74 iter 54 loss=0.10886461287736893\n",
      "epoch 74 iter 55 loss=0.09165787696838379\n",
      "epoch 74 iter 56 loss=0.0976228266954422\n",
      "epoch 74 iter 57 loss=0.11950448155403137\n",
      "epoch 74 iter 58 loss=0.10632745176553726\n",
      "epoch 74 iter 59 loss=0.17121323943138123\n",
      "epoch 74 iter 60 loss=0.09553125500679016\n",
      "epoch 74 iter 61 loss=0.07343190908432007\n",
      "epoch 74 iter 62 loss=0.06754996627569199\n",
      "epoch 74 iter 63 loss=0.08583671599626541\n",
      "epoch 74 iter 64 loss=0.09652773290872574\n",
      "epoch 74 iter 65 loss=0.16302891075611115\n",
      "epoch 74 iter 66 loss=0.1175350472331047\n",
      "epoch 74 iter 67 loss=0.12065520882606506\n",
      "epoch 74 iter 68 loss=0.09534180164337158\n",
      "epoch 74 iter 69 loss=0.08547020703554153\n",
      "epoch 74 iter 70 loss=0.08901821821928024\n",
      "epoch 74 iter 71 loss=0.07575207203626633\n",
      "epoch 74 iter 72 loss=0.1066678985953331\n",
      "epoch 74 iter 73 loss=0.10465475916862488\n",
      "epoch 74 iter 74 loss=0.0674896314740181\n",
      "epoch 75 iter 0 loss=0.09872454404830933\n",
      "epoch 75 iter 1 loss=0.09772465378046036\n",
      "epoch 75 iter 2 loss=0.08400049805641174\n",
      "epoch 75 iter 3 loss=0.18367134034633636\n",
      "epoch 75 iter 4 loss=0.0899553969502449\n",
      "epoch 75 iter 5 loss=0.09378642588853836\n",
      "epoch 75 iter 6 loss=0.10156430304050446\n",
      "epoch 75 iter 7 loss=0.07243042439222336\n",
      "epoch 75 iter 8 loss=0.05709780380129814\n",
      "epoch 75 iter 9 loss=0.07201043516397476\n",
      "epoch 75 iter 10 loss=0.0696132481098175\n",
      "epoch 75 iter 11 loss=0.07546865195035934\n",
      "epoch 75 iter 12 loss=0.10244046896696091\n",
      "epoch 75 iter 13 loss=0.08456268161535263\n",
      "epoch 75 iter 14 loss=0.09908802807331085\n",
      "epoch 75 iter 15 loss=0.09497988969087601\n",
      "epoch 75 iter 16 loss=0.10071206092834473\n",
      "epoch 75 iter 17 loss=0.06103667989373207\n",
      "epoch 75 iter 18 loss=0.22418084740638733\n",
      "epoch 75 iter 19 loss=0.12969736754894257\n",
      "epoch 75 iter 20 loss=0.11432205885648727\n",
      "epoch 75 iter 21 loss=0.0701165646314621\n",
      "epoch 75 iter 22 loss=0.08370567858219147\n",
      "epoch 75 iter 23 loss=0.0684066042304039\n",
      "epoch 75 iter 24 loss=0.05393865704536438\n",
      "epoch 75 iter 25 loss=0.08253718912601471\n",
      "epoch 75 iter 26 loss=0.08206599205732346\n",
      "epoch 75 iter 27 loss=0.04489670321345329\n",
      "epoch 75 iter 28 loss=0.10050205141305923\n",
      "epoch 75 iter 29 loss=0.08796420693397522\n",
      "epoch 75 iter 30 loss=0.10710737854242325\n",
      "epoch 75 iter 31 loss=0.062182120978832245\n",
      "epoch 75 iter 32 loss=0.1700376719236374\n",
      "epoch 75 iter 33 loss=0.10198891907930374\n",
      "epoch 75 iter 34 loss=0.09293901920318604\n",
      "epoch 75 iter 35 loss=0.08068503439426422\n",
      "epoch 75 iter 36 loss=0.0969436839222908\n",
      "epoch 75 iter 37 loss=0.05046297237277031\n",
      "epoch 75 iter 38 loss=0.05823677033185959\n",
      "epoch 75 iter 39 loss=0.07563391327857971\n",
      "epoch 75 iter 40 loss=0.12977787852287292\n",
      "epoch 75 iter 41 loss=0.11339833587408066\n",
      "epoch 75 iter 42 loss=0.10970733314752579\n",
      "epoch 75 iter 43 loss=0.14608542621135712\n",
      "epoch 75 iter 44 loss=0.08029872179031372\n",
      "epoch 75 iter 45 loss=0.0657874047756195\n",
      "epoch 75 iter 46 loss=0.132830411195755\n",
      "epoch 75 iter 47 loss=0.09622229635715485\n",
      "epoch 75 iter 48 loss=0.09979565441608429\n",
      "epoch 75 iter 49 loss=0.058577388525009155\n",
      "epoch 75 iter 50 loss=0.09279807657003403\n",
      "epoch 75 iter 51 loss=0.09733082354068756\n",
      "epoch 75 iter 52 loss=0.11687463521957397\n",
      "epoch 75 iter 53 loss=0.12549500167369843\n",
      "epoch 75 iter 54 loss=0.09586207568645477\n",
      "epoch 75 iter 55 loss=0.06540848314762115\n",
      "epoch 75 iter 56 loss=0.07161704450845718\n",
      "epoch 75 iter 57 loss=0.09071964025497437\n",
      "epoch 75 iter 58 loss=0.07557611167430878\n",
      "epoch 75 iter 59 loss=0.06854402273893356\n",
      "epoch 75 iter 60 loss=0.11915478855371475\n",
      "epoch 75 iter 61 loss=0.06811346113681793\n",
      "epoch 75 iter 62 loss=0.04583365470170975\n",
      "epoch 75 iter 63 loss=0.058471743017435074\n",
      "epoch 75 iter 64 loss=0.0700792670249939\n",
      "epoch 75 iter 65 loss=0.0753040462732315\n",
      "epoch 75 iter 66 loss=0.1183990091085434\n",
      "epoch 75 iter 67 loss=0.07898291945457458\n",
      "epoch 75 iter 68 loss=0.06371892988681793\n",
      "epoch 75 iter 69 loss=0.095639169216156\n",
      "epoch 75 iter 70 loss=0.09857252985239029\n",
      "epoch 75 iter 71 loss=0.09350387752056122\n",
      "epoch 75 iter 72 loss=0.06260498613119125\n",
      "epoch 75 iter 73 loss=0.09998898953199387\n",
      "epoch 75 iter 74 loss=0.07107264548540115\n",
      "epoch 76 iter 0 loss=0.10447937250137329\n",
      "epoch 76 iter 1 loss=0.08934742212295532\n",
      "epoch 76 iter 2 loss=0.07279854267835617\n",
      "epoch 76 iter 3 loss=0.055297575891017914\n",
      "epoch 76 iter 4 loss=0.10587818175554276\n",
      "epoch 76 iter 5 loss=0.04624191299080849\n",
      "epoch 76 iter 6 loss=0.057076167315244675\n",
      "epoch 76 iter 7 loss=0.0712580606341362\n",
      "epoch 76 iter 8 loss=0.08361741155385971\n",
      "epoch 76 iter 9 loss=0.04668596386909485\n",
      "epoch 76 iter 10 loss=0.09124176949262619\n",
      "epoch 76 iter 11 loss=0.043260760605335236\n",
      "epoch 76 iter 12 loss=0.09432341903448105\n",
      "epoch 76 iter 13 loss=0.06367944180965424\n",
      "epoch 76 iter 14 loss=0.0935051441192627\n",
      "epoch 76 iter 15 loss=0.0655721127986908\n",
      "epoch 76 iter 16 loss=0.11795175820589066\n",
      "epoch 76 iter 17 loss=0.08406756818294525\n",
      "epoch 76 iter 18 loss=0.10384000837802887\n",
      "epoch 76 iter 19 loss=0.04713577404618263\n",
      "epoch 76 iter 20 loss=0.07931315898895264\n",
      "epoch 76 iter 21 loss=0.08547366410493851\n",
      "epoch 76 iter 22 loss=0.10555316507816315\n",
      "epoch 76 iter 23 loss=0.0862518846988678\n",
      "epoch 76 iter 24 loss=0.052669115364551544\n",
      "epoch 76 iter 25 loss=0.09976080060005188\n",
      "epoch 76 iter 26 loss=0.06374029815196991\n",
      "epoch 76 iter 27 loss=0.07245036214590073\n",
      "epoch 76 iter 28 loss=0.11338372528553009\n",
      "epoch 76 iter 29 loss=0.07404345273971558\n",
      "epoch 76 iter 30 loss=0.05759924277663231\n",
      "epoch 76 iter 31 loss=0.09033932536840439\n",
      "epoch 76 iter 32 loss=0.07778158038854599\n",
      "epoch 76 iter 33 loss=0.14453819394111633\n",
      "epoch 76 iter 34 loss=0.07450264692306519\n",
      "epoch 76 iter 35 loss=0.055045340210199356\n",
      "epoch 76 iter 36 loss=0.051556654274463654\n",
      "epoch 76 iter 37 loss=0.06297729909420013\n",
      "epoch 76 iter 38 loss=0.11267755180597305\n",
      "epoch 76 iter 39 loss=0.07261542975902557\n",
      "epoch 76 iter 40 loss=0.08560694009065628\n",
      "epoch 76 iter 41 loss=0.08442744612693787\n",
      "epoch 76 iter 42 loss=0.06078289449214935\n",
      "epoch 76 iter 43 loss=0.06032155826687813\n",
      "epoch 76 iter 44 loss=0.09522180259227753\n",
      "epoch 76 iter 45 loss=0.1047501340508461\n",
      "epoch 76 iter 46 loss=0.08243165910243988\n",
      "epoch 76 iter 47 loss=0.052685219794511795\n",
      "epoch 76 iter 48 loss=0.055978886783123016\n",
      "epoch 76 iter 49 loss=0.10868541896343231\n",
      "epoch 76 iter 50 loss=0.08880749344825745\n",
      "epoch 76 iter 51 loss=0.08752963691949844\n",
      "epoch 76 iter 52 loss=0.0920935571193695\n",
      "epoch 76 iter 53 loss=0.07195665687322617\n",
      "epoch 76 iter 54 loss=0.10052160173654556\n",
      "epoch 76 iter 55 loss=0.05657649412751198\n",
      "epoch 76 iter 56 loss=0.08024697005748749\n",
      "epoch 76 iter 57 loss=0.08396460860967636\n",
      "epoch 76 iter 58 loss=0.06444519013166428\n",
      "epoch 76 iter 59 loss=0.09023743122816086\n",
      "epoch 76 iter 60 loss=0.09569466859102249\n",
      "epoch 76 iter 61 loss=0.10282867401838303\n",
      "epoch 76 iter 62 loss=0.07017915695905685\n",
      "epoch 76 iter 63 loss=0.11768002063035965\n",
      "epoch 76 iter 64 loss=0.05614928528666496\n",
      "epoch 76 iter 65 loss=0.06011103838682175\n",
      "epoch 76 iter 66 loss=0.05954345315694809\n",
      "epoch 76 iter 67 loss=0.12307779490947723\n",
      "epoch 76 iter 68 loss=0.12832464277744293\n",
      "epoch 76 iter 69 loss=0.07300955057144165\n",
      "epoch 76 iter 70 loss=0.07336414605379105\n",
      "epoch 76 iter 71 loss=0.11507990956306458\n",
      "epoch 76 iter 72 loss=0.0627463236451149\n",
      "epoch 76 iter 73 loss=0.06186710298061371\n",
      "epoch 76 iter 74 loss=0.09202148765325546\n",
      "epoch 77 iter 0 loss=0.06722189486026764\n",
      "epoch 77 iter 1 loss=0.0958499014377594\n",
      "epoch 77 iter 2 loss=0.08414901047945023\n",
      "epoch 77 iter 3 loss=0.0663468986749649\n",
      "epoch 77 iter 4 loss=0.08978141099214554\n",
      "epoch 77 iter 5 loss=0.07418382167816162\n",
      "epoch 77 iter 6 loss=0.05202019587159157\n",
      "epoch 77 iter 7 loss=0.06791644543409348\n",
      "epoch 77 iter 8 loss=0.08487395942211151\n",
      "epoch 77 iter 9 loss=0.1378474086523056\n",
      "epoch 77 iter 10 loss=0.06059490889310837\n",
      "epoch 77 iter 11 loss=0.08519060164690018\n",
      "epoch 77 iter 12 loss=0.09338383376598358\n",
      "epoch 77 iter 13 loss=0.050357867032289505\n",
      "epoch 77 iter 14 loss=0.05892309173941612\n",
      "epoch 77 iter 15 loss=0.0605890154838562\n",
      "epoch 77 iter 16 loss=0.07660253345966339\n",
      "epoch 77 iter 17 loss=0.058411456644535065\n",
      "epoch 77 iter 18 loss=0.10020294040441513\n",
      "epoch 77 iter 19 loss=0.10907883197069168\n",
      "epoch 77 iter 20 loss=0.10061249881982803\n",
      "epoch 77 iter 21 loss=0.07567266374826431\n",
      "epoch 77 iter 22 loss=0.07581278681755066\n",
      "epoch 77 iter 23 loss=0.0825662910938263\n",
      "epoch 77 iter 24 loss=0.060078177601099014\n",
      "epoch 77 iter 25 loss=0.06686914712190628\n",
      "epoch 77 iter 26 loss=0.08250538259744644\n",
      "epoch 77 iter 27 loss=0.08145222067832947\n",
      "epoch 77 iter 28 loss=0.05184222385287285\n",
      "epoch 77 iter 29 loss=0.10760769993066788\n",
      "epoch 77 iter 30 loss=0.07498352974653244\n",
      "epoch 77 iter 31 loss=0.11328891664743423\n",
      "epoch 77 iter 32 loss=0.07312153279781342\n",
      "epoch 77 iter 33 loss=0.055484604090452194\n",
      "epoch 77 iter 34 loss=0.08076923340559006\n",
      "epoch 77 iter 35 loss=0.07615689188241959\n",
      "epoch 77 iter 36 loss=0.09186891466379166\n",
      "epoch 77 iter 37 loss=0.04644860327243805\n",
      "epoch 77 iter 38 loss=0.07397731393575668\n",
      "epoch 77 iter 39 loss=0.05339131876826286\n",
      "epoch 77 iter 40 loss=0.09408701956272125\n",
      "epoch 77 iter 41 loss=0.04643296077847481\n",
      "epoch 77 iter 42 loss=0.07299789786338806\n",
      "epoch 77 iter 43 loss=0.10075298696756363\n",
      "epoch 77 iter 44 loss=0.05734750255942345\n",
      "epoch 77 iter 45 loss=0.03535417467355728\n",
      "epoch 77 iter 46 loss=0.0653194785118103\n",
      "epoch 77 iter 47 loss=0.06670094281435013\n",
      "epoch 77 iter 48 loss=0.07545892894268036\n",
      "epoch 77 iter 49 loss=0.09314866364002228\n",
      "epoch 77 iter 50 loss=0.07439185678958893\n",
      "epoch 77 iter 51 loss=0.04360225051641464\n",
      "epoch 77 iter 52 loss=0.07416590303182602\n",
      "epoch 77 iter 53 loss=0.10936243087053299\n",
      "epoch 77 iter 54 loss=0.0668000802397728\n",
      "epoch 77 iter 55 loss=0.07446108013391495\n",
      "epoch 77 iter 56 loss=0.06903961300849915\n",
      "epoch 77 iter 57 loss=0.08884225785732269\n",
      "epoch 77 iter 58 loss=0.04186759144067764\n",
      "epoch 77 iter 59 loss=0.06558631360530853\n",
      "epoch 77 iter 60 loss=0.09570067375898361\n",
      "epoch 77 iter 61 loss=0.08070443570613861\n",
      "epoch 77 iter 62 loss=0.04768148809671402\n",
      "epoch 77 iter 63 loss=0.08169545978307724\n",
      "epoch 77 iter 64 loss=0.04860483109951019\n",
      "epoch 77 iter 65 loss=0.09167022258043289\n",
      "epoch 77 iter 66 loss=0.07554450631141663\n",
      "epoch 77 iter 67 loss=0.07854752987623215\n",
      "epoch 77 iter 68 loss=0.09746425598859787\n",
      "epoch 77 iter 69 loss=0.08500344306230545\n",
      "epoch 77 iter 70 loss=0.05923748388886452\n",
      "epoch 77 iter 71 loss=0.06781298667192459\n",
      "epoch 77 iter 72 loss=0.06056354567408562\n",
      "epoch 77 iter 73 loss=0.1111505851149559\n",
      "epoch 77 iter 74 loss=0.07247625291347504\n",
      "epoch 78 iter 0 loss=0.0599481500685215\n",
      "epoch 78 iter 1 loss=0.05838242545723915\n",
      "epoch 78 iter 2 loss=0.06196843087673187\n",
      "epoch 78 iter 3 loss=0.05256480723619461\n",
      "epoch 78 iter 4 loss=0.09223029017448425\n",
      "epoch 78 iter 5 loss=0.044130150228738785\n",
      "epoch 78 iter 6 loss=0.06909722834825516\n",
      "epoch 78 iter 7 loss=0.08715970069169998\n",
      "epoch 78 iter 8 loss=0.0711003914475441\n",
      "epoch 78 iter 9 loss=0.08008425682783127\n",
      "epoch 78 iter 10 loss=0.06314919143915176\n",
      "epoch 78 iter 11 loss=0.06341131776571274\n",
      "epoch 78 iter 12 loss=0.05147501081228256\n",
      "epoch 78 iter 13 loss=0.060473065823316574\n",
      "epoch 78 iter 14 loss=0.035877179354429245\n",
      "epoch 78 iter 15 loss=0.08641673624515533\n",
      "epoch 78 iter 16 loss=0.08939005434513092\n",
      "epoch 78 iter 17 loss=0.10422927886247635\n",
      "epoch 78 iter 18 loss=0.10219453275203705\n",
      "epoch 78 iter 19 loss=0.05898776277899742\n",
      "epoch 78 iter 20 loss=0.08728811144828796\n",
      "epoch 78 iter 21 loss=0.055404409766197205\n",
      "epoch 78 iter 22 loss=0.06302942335605621\n",
      "epoch 78 iter 23 loss=0.059479884803295135\n",
      "epoch 78 iter 24 loss=0.04572548344731331\n",
      "epoch 78 iter 25 loss=0.05146903172135353\n",
      "epoch 78 iter 26 loss=0.10021940618753433\n",
      "epoch 78 iter 27 loss=0.06857974082231522\n",
      "epoch 78 iter 28 loss=0.09992562979459763\n",
      "epoch 78 iter 29 loss=0.06550393998622894\n",
      "epoch 78 iter 30 loss=0.05085296928882599\n",
      "epoch 78 iter 31 loss=0.08548533171415329\n",
      "epoch 78 iter 32 loss=0.03829270973801613\n",
      "epoch 78 iter 33 loss=0.06931957602500916\n",
      "epoch 78 iter 34 loss=0.08103743195533752\n",
      "epoch 78 iter 35 loss=0.06797999888658524\n",
      "epoch 78 iter 36 loss=0.07932836562395096\n",
      "epoch 78 iter 37 loss=0.11526254564523697\n",
      "epoch 78 iter 38 loss=0.05043209344148636\n",
      "epoch 78 iter 39 loss=0.07334820926189423\n",
      "epoch 78 iter 40 loss=0.1096729263663292\n",
      "epoch 78 iter 41 loss=0.12104098498821259\n",
      "epoch 78 iter 42 loss=0.06214520335197449\n",
      "epoch 78 iter 43 loss=0.05849628895521164\n",
      "epoch 78 iter 44 loss=0.056294333189725876\n",
      "epoch 78 iter 45 loss=0.14938069880008698\n",
      "epoch 78 iter 46 loss=0.06280140578746796\n",
      "epoch 78 iter 47 loss=0.08299343287944794\n",
      "epoch 78 iter 48 loss=0.10906556993722916\n",
      "epoch 78 iter 49 loss=0.08336883783340454\n",
      "epoch 78 iter 50 loss=0.07116219401359558\n",
      "epoch 78 iter 51 loss=0.1382860392332077\n",
      "epoch 78 iter 52 loss=0.08771749585866928\n",
      "epoch 78 iter 53 loss=0.09528885036706924\n",
      "epoch 78 iter 54 loss=0.08157844096422195\n",
      "epoch 78 iter 55 loss=0.07784295827150345\n",
      "epoch 78 iter 56 loss=0.08473148196935654\n",
      "epoch 78 iter 57 loss=0.1690966784954071\n",
      "epoch 78 iter 58 loss=0.10307520627975464\n",
      "epoch 78 iter 59 loss=0.07251111418008804\n",
      "epoch 78 iter 60 loss=0.12055917829275131\n",
      "epoch 78 iter 61 loss=0.05653614178299904\n",
      "epoch 78 iter 62 loss=0.09009841829538345\n",
      "epoch 78 iter 63 loss=0.13101385533809662\n",
      "epoch 78 iter 64 loss=0.04497246444225311\n",
      "epoch 78 iter 65 loss=0.06374034285545349\n",
      "epoch 78 iter 66 loss=0.08074066042900085\n",
      "epoch 78 iter 67 loss=0.08077811449766159\n",
      "epoch 78 iter 68 loss=0.06920143216848373\n",
      "epoch 78 iter 69 loss=0.05961969494819641\n",
      "epoch 78 iter 70 loss=0.05244876444339752\n",
      "epoch 78 iter 71 loss=0.15652798116207123\n",
      "epoch 78 iter 72 loss=0.07168208807706833\n",
      "epoch 78 iter 73 loss=0.04755572974681854\n",
      "epoch 78 iter 74 loss=0.060569800436496735\n",
      "epoch 79 iter 0 loss=0.05430484935641289\n",
      "epoch 79 iter 1 loss=0.06651104986667633\n",
      "epoch 79 iter 2 loss=0.05224646255373955\n",
      "epoch 79 iter 3 loss=0.09905063360929489\n",
      "epoch 79 iter 4 loss=0.09949110448360443\n",
      "epoch 79 iter 5 loss=0.04745497927069664\n",
      "epoch 79 iter 6 loss=0.11137217283248901\n",
      "epoch 79 iter 7 loss=0.062386780977249146\n",
      "epoch 79 iter 8 loss=0.0928795263171196\n",
      "epoch 79 iter 9 loss=0.08164725452661514\n",
      "epoch 79 iter 10 loss=0.10896069556474686\n",
      "epoch 79 iter 11 loss=0.07842348515987396\n",
      "epoch 79 iter 12 loss=0.16414926946163177\n",
      "epoch 79 iter 13 loss=0.0820607841014862\n",
      "epoch 79 iter 14 loss=0.062049709260463715\n",
      "epoch 79 iter 15 loss=0.0700439065694809\n",
      "epoch 79 iter 16 loss=0.04338589310646057\n",
      "epoch 79 iter 17 loss=0.10152751952409744\n",
      "epoch 79 iter 18 loss=0.08654031157493591\n",
      "epoch 79 iter 19 loss=0.06535718590021133\n",
      "epoch 79 iter 20 loss=0.06867682933807373\n",
      "epoch 79 iter 21 loss=0.05042752996087074\n",
      "epoch 79 iter 22 loss=0.12557339668273926\n",
      "epoch 79 iter 23 loss=0.060563478618860245\n",
      "epoch 79 iter 24 loss=0.07553090155124664\n",
      "epoch 79 iter 25 loss=0.053685709834098816\n",
      "epoch 79 iter 26 loss=0.05764113366603851\n",
      "epoch 79 iter 27 loss=0.0805339589715004\n",
      "epoch 79 iter 28 loss=0.0768476203083992\n",
      "epoch 79 iter 29 loss=0.09426802396774292\n",
      "epoch 79 iter 30 loss=0.06276702135801315\n",
      "epoch 79 iter 31 loss=0.09307310730218887\n",
      "epoch 79 iter 32 loss=0.07361077517271042\n",
      "epoch 79 iter 33 loss=0.07230623066425323\n",
      "epoch 79 iter 34 loss=0.15053176879882812\n",
      "epoch 79 iter 35 loss=0.08840782940387726\n",
      "epoch 79 iter 36 loss=0.0802823156118393\n",
      "epoch 79 iter 37 loss=0.10754147171974182\n",
      "epoch 79 iter 38 loss=0.07935605943202972\n",
      "epoch 79 iter 39 loss=0.0820622518658638\n",
      "epoch 79 iter 40 loss=0.06059675291180611\n",
      "epoch 79 iter 41 loss=0.08605837821960449\n",
      "epoch 79 iter 42 loss=0.16024337708950043\n",
      "epoch 79 iter 43 loss=0.10632583498954773\n",
      "epoch 79 iter 44 loss=0.08658537268638611\n",
      "epoch 79 iter 45 loss=0.10458122193813324\n",
      "epoch 79 iter 46 loss=0.07580380886793137\n",
      "epoch 79 iter 47 loss=0.06608188152313232\n",
      "epoch 79 iter 48 loss=0.06094448268413544\n",
      "epoch 79 iter 49 loss=0.06341250985860825\n",
      "epoch 79 iter 50 loss=0.06480363011360168\n",
      "epoch 79 iter 51 loss=0.07473351061344147\n",
      "epoch 79 iter 52 loss=0.09181126952171326\n",
      "epoch 79 iter 53 loss=0.10421641170978546\n",
      "epoch 79 iter 54 loss=0.06350009888410568\n",
      "epoch 79 iter 55 loss=0.04959733411669731\n",
      "epoch 79 iter 56 loss=0.10547617077827454\n",
      "epoch 79 iter 57 loss=0.14274370670318604\n",
      "epoch 79 iter 58 loss=0.08088438957929611\n",
      "epoch 79 iter 59 loss=0.0739373043179512\n",
      "epoch 79 iter 60 loss=0.08834262937307358\n",
      "epoch 79 iter 61 loss=0.06536341458559036\n",
      "epoch 79 iter 62 loss=0.07674504071474075\n",
      "epoch 79 iter 63 loss=0.06416776776313782\n",
      "epoch 79 iter 64 loss=0.046608299016952515\n",
      "epoch 79 iter 65 loss=0.06370784342288971\n",
      "epoch 79 iter 66 loss=0.06632021814584732\n",
      "epoch 79 iter 67 loss=0.05036864057183266\n",
      "epoch 79 iter 68 loss=0.08929639309644699\n",
      "epoch 79 iter 69 loss=0.06434527039527893\n",
      "epoch 79 iter 70 loss=0.08475309610366821\n",
      "epoch 79 iter 71 loss=0.10452155768871307\n",
      "epoch 79 iter 72 loss=0.1095045879483223\n",
      "epoch 79 iter 73 loss=0.12345266342163086\n",
      "epoch 79 iter 74 loss=0.06296861171722412\n",
      "epoch 80 iter 0 loss=0.0783889889717102\n",
      "epoch 80 iter 1 loss=0.08844685554504395\n",
      "epoch 80 iter 2 loss=0.09000924229621887\n",
      "epoch 80 iter 3 loss=0.04771352931857109\n",
      "epoch 80 iter 4 loss=0.06432744860649109\n",
      "epoch 80 iter 5 loss=0.07825655490159988\n",
      "epoch 80 iter 6 loss=0.052046872675418854\n",
      "epoch 80 iter 7 loss=0.09752243757247925\n",
      "epoch 80 iter 8 loss=0.09835933148860931\n",
      "epoch 80 iter 9 loss=0.09756073355674744\n",
      "epoch 80 iter 10 loss=0.07119699567556381\n",
      "epoch 80 iter 11 loss=0.07204151898622513\n",
      "epoch 80 iter 12 loss=0.03507934510707855\n",
      "epoch 80 iter 13 loss=0.07113522291183472\n",
      "epoch 80 iter 14 loss=0.05839256942272186\n",
      "epoch 80 iter 15 loss=0.05613284558057785\n",
      "epoch 80 iter 16 loss=0.04370034113526344\n",
      "epoch 80 iter 17 loss=0.04292556643486023\n",
      "epoch 80 iter 18 loss=0.07140202075242996\n",
      "epoch 80 iter 19 loss=0.07724927365779877\n",
      "epoch 80 iter 20 loss=0.0601842887699604\n",
      "epoch 80 iter 21 loss=0.043782785534858704\n",
      "epoch 80 iter 22 loss=0.06505464762449265\n",
      "epoch 80 iter 23 loss=0.05562320351600647\n",
      "epoch 80 iter 24 loss=0.050626449286937714\n",
      "epoch 80 iter 25 loss=0.07370956987142563\n",
      "epoch 80 iter 26 loss=0.07906889915466309\n",
      "epoch 80 iter 27 loss=0.0928950384259224\n",
      "epoch 80 iter 28 loss=0.10072053968906403\n",
      "epoch 80 iter 29 loss=0.0978713184595108\n",
      "epoch 80 iter 30 loss=0.050287507474422455\n",
      "epoch 80 iter 31 loss=0.052899427711963654\n",
      "epoch 80 iter 32 loss=0.05834328383207321\n",
      "epoch 80 iter 33 loss=0.09393031895160675\n",
      "epoch 80 iter 34 loss=0.06895560026168823\n",
      "epoch 80 iter 35 loss=0.029089778661727905\n",
      "epoch 80 iter 36 loss=0.07262006402015686\n",
      "epoch 80 iter 37 loss=0.04583658650517464\n",
      "epoch 80 iter 38 loss=0.0720171332359314\n",
      "epoch 80 iter 39 loss=0.1000145748257637\n",
      "epoch 80 iter 40 loss=0.07572733610868454\n",
      "epoch 80 iter 41 loss=0.05478408932685852\n",
      "epoch 80 iter 42 loss=0.10064264386892319\n",
      "epoch 80 iter 43 loss=0.07526874542236328\n",
      "epoch 80 iter 44 loss=0.09028945118188858\n",
      "epoch 80 iter 45 loss=0.076645128428936\n",
      "epoch 80 iter 46 loss=0.08372688293457031\n",
      "epoch 80 iter 47 loss=0.04615682363510132\n",
      "epoch 80 iter 48 loss=0.06074507534503937\n",
      "epoch 80 iter 49 loss=0.05070958659052849\n",
      "epoch 80 iter 50 loss=0.10946574807167053\n",
      "epoch 80 iter 51 loss=0.09365703910589218\n",
      "epoch 80 iter 52 loss=0.08678585290908813\n",
      "epoch 80 iter 53 loss=0.04653593897819519\n",
      "epoch 80 iter 54 loss=0.05430116876959801\n",
      "epoch 80 iter 55 loss=0.05216117575764656\n",
      "epoch 80 iter 56 loss=0.13128584623336792\n",
      "epoch 80 iter 57 loss=0.09012992680072784\n",
      "epoch 80 iter 58 loss=0.0631084218621254\n",
      "epoch 80 iter 59 loss=0.06969036161899567\n",
      "epoch 80 iter 60 loss=0.06522057950496674\n",
      "epoch 80 iter 61 loss=0.07084039598703384\n",
      "epoch 80 iter 62 loss=0.06625008583068848\n",
      "epoch 80 iter 63 loss=0.0632501095533371\n",
      "epoch 80 iter 64 loss=0.09638714790344238\n",
      "epoch 80 iter 65 loss=0.0532858781516552\n",
      "epoch 80 iter 66 loss=0.0505390428006649\n",
      "epoch 80 iter 67 loss=0.07702599465847015\n",
      "epoch 80 iter 68 loss=0.08235160261392593\n",
      "epoch 80 iter 69 loss=0.07329144328832626\n",
      "epoch 80 iter 70 loss=0.07106587290763855\n",
      "epoch 80 iter 71 loss=0.07206329703330994\n",
      "epoch 80 iter 72 loss=0.06668152660131454\n",
      "epoch 80 iter 73 loss=0.056422796100378036\n",
      "epoch 80 iter 74 loss=0.09794798493385315\n",
      "epoch 81 iter 0 loss=0.043752990663051605\n",
      "epoch 81 iter 1 loss=0.04463815689086914\n",
      "epoch 81 iter 2 loss=0.08312568068504333\n",
      "epoch 81 iter 3 loss=0.06764241307973862\n",
      "epoch 81 iter 4 loss=0.08035778254270554\n",
      "epoch 81 iter 5 loss=0.062182482331991196\n",
      "epoch 81 iter 6 loss=0.04487595707178116\n",
      "epoch 81 iter 7 loss=0.07156873494386673\n",
      "epoch 81 iter 8 loss=0.11416328698396683\n",
      "epoch 81 iter 9 loss=0.07477561384439468\n",
      "epoch 81 iter 10 loss=0.058493856340646744\n",
      "epoch 81 iter 11 loss=0.05240970477461815\n",
      "epoch 81 iter 12 loss=0.08235763758420944\n",
      "epoch 81 iter 13 loss=0.060023676604032516\n",
      "epoch 81 iter 14 loss=0.06062937155365944\n",
      "epoch 81 iter 15 loss=0.06658709049224854\n",
      "epoch 81 iter 16 loss=0.04677290841937065\n",
      "epoch 81 iter 17 loss=0.11499923467636108\n",
      "epoch 81 iter 18 loss=0.06776409596204758\n",
      "epoch 81 iter 19 loss=0.08992709219455719\n",
      "epoch 81 iter 20 loss=0.07942726463079453\n",
      "epoch 81 iter 21 loss=0.06755232065916061\n",
      "epoch 81 iter 22 loss=0.0531662292778492\n",
      "epoch 81 iter 23 loss=0.065858855843544\n",
      "epoch 81 iter 24 loss=0.0425533652305603\n",
      "epoch 81 iter 25 loss=0.06932590901851654\n",
      "epoch 81 iter 26 loss=0.08397538959980011\n",
      "epoch 81 iter 27 loss=0.07735709100961685\n",
      "epoch 81 iter 28 loss=0.041978444904088974\n",
      "epoch 81 iter 29 loss=0.08053738623857498\n",
      "epoch 81 iter 30 loss=0.07182913273572922\n",
      "epoch 81 iter 31 loss=0.05078764259815216\n",
      "epoch 81 iter 32 loss=0.06611823290586472\n",
      "epoch 81 iter 33 loss=0.06625648587942123\n",
      "epoch 81 iter 34 loss=0.04098191484808922\n",
      "epoch 81 iter 35 loss=0.0689074695110321\n",
      "epoch 81 iter 36 loss=0.07978109270334244\n",
      "epoch 81 iter 37 loss=0.030048631131649017\n",
      "epoch 81 iter 38 loss=0.07858633995056152\n",
      "epoch 81 iter 39 loss=0.07506178319454193\n",
      "epoch 81 iter 40 loss=0.0680091381072998\n",
      "epoch 81 iter 41 loss=0.05997521057724953\n",
      "epoch 81 iter 42 loss=0.06261447072029114\n",
      "epoch 81 iter 43 loss=0.05725444480776787\n",
      "epoch 81 iter 44 loss=0.06304565072059631\n",
      "epoch 81 iter 45 loss=0.09367020428180695\n",
      "epoch 81 iter 46 loss=0.04013073071837425\n",
      "epoch 81 iter 47 loss=0.06787969917058945\n",
      "epoch 81 iter 48 loss=0.09002841264009476\n",
      "epoch 81 iter 49 loss=0.07301007956266403\n",
      "epoch 81 iter 50 loss=0.07555125653743744\n",
      "epoch 81 iter 51 loss=0.06825178116559982\n",
      "epoch 81 iter 52 loss=0.07253769040107727\n",
      "epoch 81 iter 53 loss=0.0795670598745346\n",
      "epoch 81 iter 54 loss=0.04707445204257965\n",
      "epoch 81 iter 55 loss=0.05948606878519058\n",
      "epoch 81 iter 56 loss=0.07810704410076141\n",
      "epoch 81 iter 57 loss=0.07600656896829605\n",
      "epoch 81 iter 58 loss=0.048968829214572906\n",
      "epoch 81 iter 59 loss=0.08443984389305115\n",
      "epoch 81 iter 60 loss=0.06227479502558708\n",
      "epoch 81 iter 61 loss=0.09404772520065308\n",
      "epoch 81 iter 62 loss=0.0814599096775055\n",
      "epoch 81 iter 63 loss=0.09634224325418472\n",
      "epoch 81 iter 64 loss=0.053882088512182236\n",
      "epoch 81 iter 65 loss=0.03916054219007492\n",
      "epoch 81 iter 66 loss=0.041430775076150894\n",
      "epoch 81 iter 67 loss=0.04604502394795418\n",
      "epoch 81 iter 68 loss=0.10825863480567932\n",
      "epoch 81 iter 69 loss=0.0855911448597908\n",
      "epoch 81 iter 70 loss=0.0718102976679802\n",
      "epoch 81 iter 71 loss=0.05162607133388519\n",
      "epoch 81 iter 72 loss=0.07649290561676025\n",
      "epoch 81 iter 73 loss=0.11130645126104355\n",
      "epoch 81 iter 74 loss=0.13788166642189026\n",
      "epoch 82 iter 0 loss=0.1379614919424057\n",
      "epoch 82 iter 1 loss=0.08548184484243393\n",
      "epoch 82 iter 2 loss=0.08818688988685608\n",
      "epoch 82 iter 3 loss=0.04755066707730293\n",
      "epoch 82 iter 4 loss=0.07996392250061035\n",
      "epoch 82 iter 5 loss=0.09404966235160828\n",
      "epoch 82 iter 6 loss=0.08390430361032486\n",
      "epoch 82 iter 7 loss=0.06406643241643906\n",
      "epoch 82 iter 8 loss=0.08013802766799927\n",
      "epoch 82 iter 9 loss=0.09173782169818878\n",
      "epoch 82 iter 10 loss=0.10206194967031479\n",
      "epoch 82 iter 11 loss=0.08939836174249649\n",
      "epoch 82 iter 12 loss=0.09567002952098846\n",
      "epoch 82 iter 13 loss=0.0588579997420311\n",
      "epoch 82 iter 14 loss=0.08543512225151062\n",
      "epoch 82 iter 15 loss=0.11860526353120804\n",
      "epoch 82 iter 16 loss=0.04722462221980095\n",
      "epoch 82 iter 17 loss=0.0715632513165474\n",
      "epoch 82 iter 18 loss=0.07681133598089218\n",
      "epoch 82 iter 19 loss=0.044761475175619125\n",
      "epoch 82 iter 20 loss=0.10704907029867172\n",
      "epoch 82 iter 21 loss=0.05362634360790253\n",
      "epoch 82 iter 22 loss=0.08360018581151962\n",
      "epoch 82 iter 23 loss=0.0439472459256649\n",
      "epoch 82 iter 24 loss=0.1318848580121994\n",
      "epoch 82 iter 25 loss=0.06704284250736237\n",
      "epoch 82 iter 26 loss=0.06488555669784546\n",
      "epoch 82 iter 27 loss=0.08491531014442444\n",
      "epoch 82 iter 28 loss=0.06380195915699005\n",
      "epoch 82 iter 29 loss=0.07234657555818558\n",
      "epoch 82 iter 30 loss=0.15175393223762512\n",
      "epoch 82 iter 31 loss=0.08104247599840164\n",
      "epoch 82 iter 32 loss=0.07447899878025055\n",
      "epoch 82 iter 33 loss=0.05828779935836792\n",
      "epoch 82 iter 34 loss=0.07684028148651123\n",
      "epoch 82 iter 35 loss=0.08766210079193115\n",
      "epoch 82 iter 36 loss=0.10150161385536194\n",
      "epoch 82 iter 37 loss=0.0837402492761612\n",
      "epoch 82 iter 38 loss=0.06669911742210388\n",
      "epoch 82 iter 39 loss=0.06900490820407867\n",
      "epoch 82 iter 40 loss=0.0633898675441742\n",
      "epoch 82 iter 41 loss=0.060539670288562775\n",
      "epoch 82 iter 42 loss=0.08750470727682114\n",
      "epoch 82 iter 43 loss=0.043757081031799316\n",
      "epoch 82 iter 44 loss=0.06207256764173508\n",
      "epoch 82 iter 45 loss=0.059336379170417786\n",
      "epoch 82 iter 46 loss=0.06023239716887474\n",
      "epoch 82 iter 47 loss=0.07574418932199478\n",
      "epoch 82 iter 48 loss=0.08684805780649185\n",
      "epoch 82 iter 49 loss=0.07740418612957001\n",
      "epoch 82 iter 50 loss=0.05741678178310394\n",
      "epoch 82 iter 51 loss=0.060066383332014084\n",
      "epoch 82 iter 52 loss=0.058063484728336334\n",
      "epoch 82 iter 53 loss=0.07886666059494019\n",
      "epoch 82 iter 54 loss=0.08722814172506332\n",
      "epoch 82 iter 55 loss=0.0875641256570816\n",
      "epoch 82 iter 56 loss=0.10235147923231125\n",
      "epoch 82 iter 57 loss=0.04712141677737236\n",
      "epoch 82 iter 58 loss=0.05087306722998619\n",
      "epoch 82 iter 59 loss=0.10497331619262695\n",
      "epoch 82 iter 60 loss=0.09489163756370544\n",
      "epoch 82 iter 61 loss=0.08778993785381317\n",
      "epoch 82 iter 62 loss=0.07581636309623718\n",
      "epoch 82 iter 63 loss=0.08758100122213364\n",
      "epoch 82 iter 64 loss=0.11547373235225677\n",
      "epoch 82 iter 65 loss=0.07505422830581665\n",
      "epoch 82 iter 66 loss=0.10366462171077728\n",
      "epoch 82 iter 67 loss=0.06712791323661804\n",
      "epoch 82 iter 68 loss=0.06284290552139282\n",
      "epoch 82 iter 69 loss=0.07567539811134338\n",
      "epoch 82 iter 70 loss=0.07882631570100784\n",
      "epoch 82 iter 71 loss=0.05790427327156067\n",
      "epoch 82 iter 72 loss=0.08263679593801498\n",
      "epoch 82 iter 73 loss=0.09144791215658188\n",
      "epoch 82 iter 74 loss=0.08123794198036194\n",
      "epoch 83 iter 0 loss=0.08744268119335175\n",
      "epoch 83 iter 1 loss=0.05648130923509598\n",
      "epoch 83 iter 2 loss=0.06852167099714279\n",
      "epoch 83 iter 3 loss=0.0922083780169487\n",
      "epoch 83 iter 4 loss=0.07824451476335526\n",
      "epoch 83 iter 5 loss=0.08882921934127808\n",
      "epoch 83 iter 6 loss=0.10065038502216339\n",
      "epoch 83 iter 7 loss=0.06316600739955902\n",
      "epoch 83 iter 8 loss=0.06374567747116089\n",
      "epoch 83 iter 9 loss=0.10124532878398895\n",
      "epoch 83 iter 10 loss=0.07308311760425568\n",
      "epoch 83 iter 11 loss=0.06763347238302231\n",
      "epoch 83 iter 12 loss=0.04597916081547737\n",
      "epoch 83 iter 13 loss=0.07318884879350662\n",
      "epoch 83 iter 14 loss=0.06145908683538437\n",
      "epoch 83 iter 15 loss=0.06712602078914642\n",
      "epoch 83 iter 16 loss=0.10463619232177734\n",
      "epoch 83 iter 17 loss=0.0810980498790741\n",
      "epoch 83 iter 18 loss=0.08826430886983871\n",
      "epoch 83 iter 19 loss=0.06238178163766861\n",
      "epoch 83 iter 20 loss=0.09496420621871948\n",
      "epoch 83 iter 21 loss=0.06182456016540527\n",
      "epoch 83 iter 22 loss=0.07149693369865417\n",
      "epoch 83 iter 23 loss=0.07335567474365234\n",
      "epoch 83 iter 24 loss=0.05146219581365585\n",
      "epoch 83 iter 25 loss=0.06080344691872597\n",
      "epoch 83 iter 26 loss=0.07224442064762115\n",
      "epoch 83 iter 27 loss=0.07397525757551193\n",
      "epoch 83 iter 28 loss=0.06753475964069366\n",
      "epoch 83 iter 29 loss=0.09900803864002228\n",
      "epoch 83 iter 30 loss=0.06233656406402588\n",
      "epoch 83 iter 31 loss=0.06178009510040283\n",
      "epoch 83 iter 32 loss=0.0818265900015831\n",
      "epoch 83 iter 33 loss=0.09999807178974152\n",
      "epoch 83 iter 34 loss=0.07173363119363785\n",
      "epoch 83 iter 35 loss=0.0882122665643692\n",
      "epoch 83 iter 36 loss=0.06370395421981812\n",
      "epoch 83 iter 37 loss=0.04956328123807907\n",
      "epoch 83 iter 38 loss=0.06775163114070892\n",
      "epoch 83 iter 39 loss=0.06594966351985931\n",
      "epoch 83 iter 40 loss=0.06865506619215012\n",
      "epoch 83 iter 41 loss=0.08290029317140579\n",
      "epoch 83 iter 42 loss=0.07461627572774887\n",
      "epoch 83 iter 43 loss=0.05350833386182785\n",
      "epoch 83 iter 44 loss=0.0537697970867157\n",
      "epoch 83 iter 45 loss=0.10255996137857437\n",
      "epoch 83 iter 46 loss=0.0527479387819767\n",
      "epoch 83 iter 47 loss=0.055657073855400085\n",
      "epoch 83 iter 48 loss=0.11832047998905182\n",
      "epoch 83 iter 49 loss=0.11839502304792404\n",
      "epoch 83 iter 50 loss=0.05457419157028198\n",
      "epoch 83 iter 51 loss=0.1265379637479782\n",
      "epoch 83 iter 52 loss=0.07503214478492737\n",
      "epoch 83 iter 53 loss=0.04988120123744011\n",
      "epoch 83 iter 54 loss=0.08783339709043503\n",
      "epoch 83 iter 55 loss=0.08439107239246368\n",
      "epoch 83 iter 56 loss=0.07708308100700378\n",
      "epoch 83 iter 57 loss=0.10685250908136368\n",
      "epoch 83 iter 58 loss=0.0707414299249649\n",
      "epoch 83 iter 59 loss=0.06697717308998108\n",
      "epoch 83 iter 60 loss=0.05595291778445244\n",
      "epoch 83 iter 61 loss=0.06772705167531967\n",
      "epoch 83 iter 62 loss=0.053581155836582184\n",
      "epoch 83 iter 63 loss=0.06734388321638107\n",
      "epoch 83 iter 64 loss=0.06692627817392349\n",
      "epoch 83 iter 65 loss=0.06745891273021698\n",
      "epoch 83 iter 66 loss=0.0355805903673172\n",
      "epoch 83 iter 67 loss=0.09821023792028427\n",
      "epoch 83 iter 68 loss=0.05700363591313362\n",
      "epoch 83 iter 69 loss=0.05643297731876373\n",
      "epoch 83 iter 70 loss=0.07294762134552002\n",
      "epoch 83 iter 71 loss=0.0565483383834362\n",
      "epoch 83 iter 72 loss=0.07566142827272415\n",
      "epoch 83 iter 73 loss=0.053214848041534424\n",
      "epoch 83 iter 74 loss=0.10296878218650818\n",
      "epoch 84 iter 0 loss=0.07371924072504044\n",
      "epoch 84 iter 1 loss=0.03977416828274727\n",
      "epoch 84 iter 2 loss=0.08074454218149185\n",
      "epoch 84 iter 3 loss=0.03235171362757683\n",
      "epoch 84 iter 4 loss=0.047979824244976044\n",
      "epoch 84 iter 5 loss=0.05946319177746773\n",
      "epoch 84 iter 6 loss=0.08605482429265976\n",
      "epoch 84 iter 7 loss=0.050042860209941864\n",
      "epoch 84 iter 8 loss=0.07055061310529709\n",
      "epoch 84 iter 9 loss=0.11902906000614166\n",
      "epoch 84 iter 10 loss=0.07403149455785751\n",
      "epoch 84 iter 11 loss=0.05633829906582832\n",
      "epoch 84 iter 12 loss=0.04339602589607239\n",
      "epoch 84 iter 13 loss=0.05024421215057373\n",
      "epoch 84 iter 14 loss=0.04186788946390152\n",
      "epoch 84 iter 15 loss=0.10612676292657852\n",
      "epoch 84 iter 16 loss=0.07405342906713486\n",
      "epoch 84 iter 17 loss=0.059152405709028244\n",
      "epoch 84 iter 18 loss=0.05843959003686905\n",
      "epoch 84 iter 19 loss=0.039483752101659775\n",
      "epoch 84 iter 20 loss=0.10053914040327072\n",
      "epoch 84 iter 21 loss=0.06555817276239395\n",
      "epoch 84 iter 22 loss=0.05830797553062439\n",
      "epoch 84 iter 23 loss=0.08321420848369598\n",
      "epoch 84 iter 24 loss=0.06059558317065239\n",
      "epoch 84 iter 25 loss=0.07518379390239716\n",
      "epoch 84 iter 26 loss=0.06415457278490067\n",
      "epoch 84 iter 27 loss=0.06806547194719315\n",
      "epoch 84 iter 28 loss=0.06582136452198029\n",
      "epoch 84 iter 29 loss=0.07032811641693115\n",
      "epoch 84 iter 30 loss=0.054224222898483276\n",
      "epoch 84 iter 31 loss=0.06345845013856888\n",
      "epoch 84 iter 32 loss=0.048075780272483826\n",
      "epoch 84 iter 33 loss=0.0628327801823616\n",
      "epoch 84 iter 34 loss=0.10230479389429092\n",
      "epoch 84 iter 35 loss=0.07485245913267136\n",
      "epoch 84 iter 36 loss=0.09408991038799286\n",
      "epoch 84 iter 37 loss=0.11858557164669037\n",
      "epoch 84 iter 38 loss=0.07374343276023865\n",
      "epoch 84 iter 39 loss=0.059842489659786224\n",
      "epoch 84 iter 40 loss=0.04756557568907738\n",
      "epoch 84 iter 41 loss=0.15749424695968628\n",
      "epoch 84 iter 42 loss=0.08645091205835342\n",
      "epoch 84 iter 43 loss=0.05249827355146408\n",
      "epoch 84 iter 44 loss=0.08849804848432541\n",
      "epoch 84 iter 45 loss=0.06788671016693115\n",
      "epoch 84 iter 46 loss=0.05085865035653114\n",
      "epoch 84 iter 47 loss=0.0944371372461319\n",
      "epoch 84 iter 48 loss=0.04005928710103035\n",
      "epoch 84 iter 49 loss=0.13221503794193268\n",
      "epoch 84 iter 50 loss=0.056701090186834335\n",
      "epoch 84 iter 51 loss=0.07481832802295685\n",
      "epoch 84 iter 52 loss=0.10397959500551224\n",
      "epoch 84 iter 53 loss=0.07348998636007309\n",
      "epoch 84 iter 54 loss=0.0436997190117836\n",
      "epoch 84 iter 55 loss=0.12234716862440109\n",
      "epoch 84 iter 56 loss=0.07379205524921417\n",
      "epoch 84 iter 57 loss=0.06958547979593277\n",
      "epoch 84 iter 58 loss=0.04181002825498581\n",
      "epoch 84 iter 59 loss=0.11025715619325638\n",
      "epoch 84 iter 60 loss=0.09147267043590546\n",
      "epoch 84 iter 61 loss=0.06746036559343338\n",
      "epoch 84 iter 62 loss=0.04183140769600868\n",
      "epoch 84 iter 63 loss=0.07167251408100128\n",
      "epoch 84 iter 64 loss=0.07895707339048386\n",
      "epoch 84 iter 65 loss=0.07076514512300491\n",
      "epoch 84 iter 66 loss=0.04910137876868248\n",
      "epoch 84 iter 67 loss=0.06531447172164917\n",
      "epoch 84 iter 68 loss=0.07757627218961716\n",
      "epoch 84 iter 69 loss=0.05893829092383385\n",
      "epoch 84 iter 70 loss=0.07609959691762924\n",
      "epoch 84 iter 71 loss=0.05922114476561546\n",
      "epoch 84 iter 72 loss=0.04438243433833122\n",
      "epoch 84 iter 73 loss=0.06828190386295319\n",
      "epoch 84 iter 74 loss=0.07671184092760086\n",
      "epoch 85 iter 0 loss=0.04700475558638573\n",
      "epoch 85 iter 1 loss=0.038335785269737244\n",
      "epoch 85 iter 2 loss=0.07137731462717056\n",
      "epoch 85 iter 3 loss=0.07509560137987137\n",
      "epoch 85 iter 4 loss=0.04184920713305473\n",
      "epoch 85 iter 5 loss=0.06713031977415085\n",
      "epoch 85 iter 6 loss=0.07845355570316315\n",
      "epoch 85 iter 7 loss=0.07420942932367325\n",
      "epoch 85 iter 8 loss=0.07624693959951401\n",
      "epoch 85 iter 9 loss=0.05024706944823265\n",
      "epoch 85 iter 10 loss=0.05189088359475136\n",
      "epoch 85 iter 11 loss=0.05882587656378746\n",
      "epoch 85 iter 12 loss=0.09331737458705902\n",
      "epoch 85 iter 13 loss=0.06486393511295319\n",
      "epoch 85 iter 14 loss=0.06237461417913437\n",
      "epoch 85 iter 15 loss=0.06669681519269943\n",
      "epoch 85 iter 16 loss=0.08296776562929153\n",
      "epoch 85 iter 17 loss=0.09256882965564728\n",
      "epoch 85 iter 18 loss=0.11757407337427139\n",
      "epoch 85 iter 19 loss=0.059314314275979996\n",
      "epoch 85 iter 20 loss=0.06308158487081528\n",
      "epoch 85 iter 21 loss=0.051754169166088104\n",
      "epoch 85 iter 22 loss=0.04447869211435318\n",
      "epoch 85 iter 23 loss=0.10114214569330215\n",
      "epoch 85 iter 24 loss=0.08907722681760788\n",
      "epoch 85 iter 25 loss=0.054396212100982666\n",
      "epoch 85 iter 26 loss=0.05014127492904663\n",
      "epoch 85 iter 27 loss=0.05796659737825394\n",
      "epoch 85 iter 28 loss=0.0554136261343956\n",
      "epoch 85 iter 29 loss=0.07019010186195374\n",
      "epoch 85 iter 30 loss=0.0977289080619812\n",
      "epoch 85 iter 31 loss=0.10071873664855957\n",
      "epoch 85 iter 32 loss=0.0788210853934288\n",
      "epoch 85 iter 33 loss=0.08509352058172226\n",
      "epoch 85 iter 34 loss=0.03790872171521187\n",
      "epoch 85 iter 35 loss=0.13409170508384705\n",
      "epoch 85 iter 36 loss=0.056426942348480225\n",
      "epoch 85 iter 37 loss=0.08292979747056961\n",
      "epoch 85 iter 38 loss=0.06089252978563309\n",
      "epoch 85 iter 39 loss=0.07308423519134521\n",
      "epoch 85 iter 40 loss=0.03819946199655533\n",
      "epoch 85 iter 41 loss=0.06481433659791946\n",
      "epoch 85 iter 42 loss=0.0651831105351448\n",
      "epoch 85 iter 43 loss=0.061899539083242416\n",
      "epoch 85 iter 44 loss=0.09033111482858658\n",
      "epoch 85 iter 45 loss=0.052631523460149765\n",
      "epoch 85 iter 46 loss=0.09055718779563904\n",
      "epoch 85 iter 47 loss=0.05072729289531708\n",
      "epoch 85 iter 48 loss=0.07524196058511734\n",
      "epoch 85 iter 49 loss=0.061799537390470505\n",
      "epoch 85 iter 50 loss=0.03539559617638588\n",
      "epoch 85 iter 51 loss=0.06014035642147064\n",
      "epoch 85 iter 52 loss=0.05144716054201126\n",
      "epoch 85 iter 53 loss=0.0823870301246643\n",
      "epoch 85 iter 54 loss=0.08059392869472504\n",
      "epoch 85 iter 55 loss=0.060622770339250565\n",
      "epoch 85 iter 56 loss=0.10815099626779556\n",
      "epoch 85 iter 57 loss=0.0662974938750267\n",
      "epoch 85 iter 58 loss=0.06179427728056908\n",
      "epoch 85 iter 59 loss=0.046607404947280884\n",
      "epoch 85 iter 60 loss=0.059550926089286804\n",
      "epoch 85 iter 61 loss=0.06675934046506882\n",
      "epoch 85 iter 62 loss=0.05300814285874367\n",
      "epoch 85 iter 63 loss=0.07879079878330231\n",
      "epoch 85 iter 64 loss=0.045603834092617035\n",
      "epoch 85 iter 65 loss=0.051888372749090195\n",
      "epoch 85 iter 66 loss=0.06604175269603729\n",
      "epoch 85 iter 67 loss=0.06417518109083176\n",
      "epoch 85 iter 68 loss=0.056094877421855927\n",
      "epoch 85 iter 69 loss=0.048169296234846115\n",
      "epoch 85 iter 70 loss=0.051989614963531494\n",
      "epoch 85 iter 71 loss=0.05006685107946396\n",
      "epoch 85 iter 72 loss=0.052990540862083435\n",
      "epoch 85 iter 73 loss=0.03297967463731766\n",
      "epoch 85 iter 74 loss=0.05387119948863983\n",
      "epoch 86 iter 0 loss=0.07462187856435776\n",
      "epoch 86 iter 1 loss=0.05960015580058098\n",
      "epoch 86 iter 2 loss=0.04671617969870567\n",
      "epoch 86 iter 3 loss=0.0657883733510971\n",
      "epoch 86 iter 4 loss=0.058290861546993256\n",
      "epoch 86 iter 5 loss=0.07335135340690613\n",
      "epoch 86 iter 6 loss=0.08964969217777252\n",
      "epoch 86 iter 7 loss=0.07281822711229324\n",
      "epoch 86 iter 8 loss=0.05834101513028145\n",
      "epoch 86 iter 9 loss=0.10340051352977753\n",
      "epoch 86 iter 10 loss=0.06398499757051468\n",
      "epoch 86 iter 11 loss=0.08208249509334564\n",
      "epoch 86 iter 12 loss=0.04178644344210625\n",
      "epoch 86 iter 13 loss=0.07717698812484741\n",
      "epoch 86 iter 14 loss=0.07866199314594269\n",
      "epoch 86 iter 15 loss=0.04173317179083824\n",
      "epoch 86 iter 16 loss=0.06927810609340668\n",
      "epoch 86 iter 17 loss=0.05441580340266228\n",
      "epoch 86 iter 18 loss=0.0558524988591671\n",
      "epoch 86 iter 19 loss=0.06361643224954605\n",
      "epoch 86 iter 20 loss=0.0653591901063919\n",
      "epoch 86 iter 21 loss=0.06664688140153885\n",
      "epoch 86 iter 22 loss=0.05614910274744034\n",
      "epoch 86 iter 23 loss=0.046594321727752686\n",
      "epoch 86 iter 24 loss=0.07557453215122223\n",
      "epoch 86 iter 25 loss=0.06302758306264877\n",
      "epoch 86 iter 26 loss=0.06674979627132416\n",
      "epoch 86 iter 27 loss=0.06233596429228783\n",
      "epoch 86 iter 28 loss=0.05916088819503784\n",
      "epoch 86 iter 29 loss=0.10209088027477264\n",
      "epoch 86 iter 30 loss=0.049749843776226044\n",
      "epoch 86 iter 31 loss=0.04570101946592331\n",
      "epoch 86 iter 32 loss=0.06687013059854507\n",
      "epoch 86 iter 33 loss=0.0924195721745491\n",
      "epoch 86 iter 34 loss=0.08811137080192566\n",
      "epoch 86 iter 35 loss=0.05037063732743263\n",
      "epoch 86 iter 36 loss=0.05039011314511299\n",
      "epoch 86 iter 37 loss=0.03813409060239792\n",
      "epoch 86 iter 38 loss=0.07424324005842209\n",
      "epoch 86 iter 39 loss=0.07048845291137695\n",
      "epoch 86 iter 40 loss=0.07912056148052216\n",
      "epoch 86 iter 41 loss=0.07814181596040726\n",
      "epoch 86 iter 42 loss=0.046314604580402374\n",
      "epoch 86 iter 43 loss=0.08628381043672562\n",
      "epoch 86 iter 44 loss=0.06766960024833679\n",
      "epoch 86 iter 45 loss=0.04044411703944206\n",
      "epoch 86 iter 46 loss=0.03549307957291603\n",
      "epoch 86 iter 47 loss=0.05362347140908241\n",
      "epoch 86 iter 48 loss=0.059837717562913895\n",
      "epoch 86 iter 49 loss=0.048889707773923874\n",
      "epoch 86 iter 50 loss=0.04626862332224846\n",
      "epoch 86 iter 51 loss=0.07499348372220993\n",
      "epoch 86 iter 52 loss=0.06825576722621918\n",
      "epoch 86 iter 53 loss=0.0836421400308609\n",
      "epoch 86 iter 54 loss=0.05824494734406471\n",
      "epoch 86 iter 55 loss=0.054156456142663956\n",
      "epoch 86 iter 56 loss=0.07256720215082169\n",
      "epoch 86 iter 57 loss=0.042988914996385574\n",
      "epoch 86 iter 58 loss=0.05142377316951752\n",
      "epoch 86 iter 59 loss=0.0850164145231247\n",
      "epoch 86 iter 60 loss=0.05028664693236351\n",
      "epoch 86 iter 61 loss=0.03520583361387253\n",
      "epoch 86 iter 62 loss=0.07462282478809357\n",
      "epoch 86 iter 63 loss=0.06253396719694138\n",
      "epoch 86 iter 64 loss=0.04124270752072334\n",
      "epoch 86 iter 65 loss=0.06844960898160934\n",
      "epoch 86 iter 66 loss=0.0529601015150547\n",
      "epoch 86 iter 67 loss=0.0894116535782814\n",
      "epoch 86 iter 68 loss=0.04399387538433075\n",
      "epoch 86 iter 69 loss=0.07705694437026978\n",
      "epoch 86 iter 70 loss=0.06056186184287071\n",
      "epoch 86 iter 71 loss=0.05428628996014595\n",
      "epoch 86 iter 72 loss=0.06917194277048111\n",
      "epoch 86 iter 73 loss=0.08360745012760162\n",
      "epoch 86 iter 74 loss=0.052046939730644226\n",
      "epoch 87 iter 0 loss=0.047339990735054016\n",
      "epoch 87 iter 1 loss=0.07185272872447968\n",
      "epoch 87 iter 2 loss=0.05444817245006561\n",
      "epoch 87 iter 3 loss=0.08644594997167587\n",
      "epoch 87 iter 4 loss=0.05914681777358055\n",
      "epoch 87 iter 5 loss=0.06285559386014938\n",
      "epoch 87 iter 6 loss=0.05012720823287964\n",
      "epoch 87 iter 7 loss=0.03677189350128174\n",
      "epoch 87 iter 8 loss=0.04574378952383995\n",
      "epoch 87 iter 9 loss=0.09069423377513885\n",
      "epoch 87 iter 10 loss=0.05540755018591881\n",
      "epoch 87 iter 11 loss=0.053814925253391266\n",
      "epoch 87 iter 12 loss=0.05043677240610123\n",
      "epoch 87 iter 13 loss=0.047504670917987823\n",
      "epoch 87 iter 14 loss=0.045536212623119354\n",
      "epoch 87 iter 15 loss=0.10657653957605362\n",
      "epoch 87 iter 16 loss=0.08924764394760132\n",
      "epoch 87 iter 17 loss=0.0755649283528328\n",
      "epoch 87 iter 18 loss=0.04420904442667961\n",
      "epoch 87 iter 19 loss=0.061086565256118774\n",
      "epoch 87 iter 20 loss=0.045189592987298965\n",
      "epoch 87 iter 21 loss=0.05210825800895691\n",
      "epoch 87 iter 22 loss=0.04724394530057907\n",
      "epoch 87 iter 23 loss=0.05334513261914253\n",
      "epoch 87 iter 24 loss=0.10046457499265671\n",
      "epoch 87 iter 25 loss=0.07092664390802383\n",
      "epoch 87 iter 26 loss=0.06263075023889542\n",
      "epoch 87 iter 27 loss=0.043337639421224594\n",
      "epoch 87 iter 28 loss=0.07332509756088257\n",
      "epoch 87 iter 29 loss=0.047884501516819\n",
      "epoch 87 iter 30 loss=0.04669617488980293\n",
      "epoch 87 iter 31 loss=0.04190697893500328\n",
      "epoch 87 iter 32 loss=0.05890289694070816\n",
      "epoch 87 iter 33 loss=0.0733659565448761\n",
      "epoch 87 iter 34 loss=0.04070879518985748\n",
      "epoch 87 iter 35 loss=0.062448497861623764\n",
      "epoch 87 iter 36 loss=0.05977394059300423\n",
      "epoch 87 iter 37 loss=0.039187416434288025\n",
      "epoch 87 iter 38 loss=0.06999985128641129\n",
      "epoch 87 iter 39 loss=0.07350441813468933\n",
      "epoch 87 iter 40 loss=0.05833643674850464\n",
      "epoch 87 iter 41 loss=0.05476272851228714\n",
      "epoch 87 iter 42 loss=0.050073739141225815\n",
      "epoch 87 iter 43 loss=0.05359094589948654\n",
      "epoch 87 iter 44 loss=0.05636271834373474\n",
      "epoch 87 iter 45 loss=0.10561025887727737\n",
      "epoch 87 iter 46 loss=0.05564612150192261\n",
      "epoch 87 iter 47 loss=0.0622822605073452\n",
      "epoch 87 iter 48 loss=0.06356783211231232\n",
      "epoch 87 iter 49 loss=0.07265570759773254\n",
      "epoch 87 iter 50 loss=0.06705063581466675\n",
      "epoch 87 iter 51 loss=0.08506076782941818\n",
      "epoch 87 iter 52 loss=0.06750065833330154\n",
      "epoch 87 iter 53 loss=0.08643872290849686\n",
      "epoch 87 iter 54 loss=0.08966518938541412\n",
      "epoch 87 iter 55 loss=0.09115629643201828\n",
      "epoch 87 iter 56 loss=0.07733497023582458\n",
      "epoch 87 iter 57 loss=0.09902200847864151\n",
      "epoch 87 iter 58 loss=0.04763759672641754\n",
      "epoch 87 iter 59 loss=0.0548354908823967\n",
      "epoch 87 iter 60 loss=0.05881230905652046\n",
      "epoch 87 iter 61 loss=0.05043649300932884\n",
      "epoch 87 iter 62 loss=0.06411196291446686\n",
      "epoch 87 iter 63 loss=0.04977871850132942\n",
      "epoch 87 iter 64 loss=0.05271642655134201\n",
      "epoch 87 iter 65 loss=0.07885908335447311\n",
      "epoch 87 iter 66 loss=0.052221328020095825\n",
      "epoch 87 iter 67 loss=0.06632855534553528\n",
      "epoch 87 iter 68 loss=0.08528820425271988\n",
      "epoch 87 iter 69 loss=0.0720863789319992\n",
      "epoch 87 iter 70 loss=0.06826117634773254\n",
      "epoch 87 iter 71 loss=0.03715904429554939\n",
      "epoch 87 iter 72 loss=0.07114427536725998\n",
      "epoch 87 iter 73 loss=0.05907253548502922\n",
      "epoch 87 iter 74 loss=0.07345521450042725\n",
      "epoch 88 iter 0 loss=0.07193965464830399\n",
      "epoch 88 iter 1 loss=0.08595570176839828\n",
      "epoch 88 iter 2 loss=0.05281111225485802\n",
      "epoch 88 iter 3 loss=0.0634634718298912\n",
      "epoch 88 iter 4 loss=0.07106750458478928\n",
      "epoch 88 iter 5 loss=0.04016881063580513\n",
      "epoch 88 iter 6 loss=0.06579913198947906\n",
      "epoch 88 iter 7 loss=0.0489836148917675\n",
      "epoch 88 iter 8 loss=0.08853426575660706\n",
      "epoch 88 iter 9 loss=0.04859574884176254\n",
      "epoch 88 iter 10 loss=0.05827774852514267\n",
      "epoch 88 iter 11 loss=0.04249846935272217\n",
      "epoch 88 iter 12 loss=0.06912413984537125\n",
      "epoch 88 iter 13 loss=0.049635451287031174\n",
      "epoch 88 iter 14 loss=0.05136522278189659\n",
      "epoch 88 iter 15 loss=0.05365530028939247\n",
      "epoch 88 iter 16 loss=0.03834395483136177\n",
      "epoch 88 iter 17 loss=0.043811868876218796\n",
      "epoch 88 iter 18 loss=0.042754996567964554\n",
      "epoch 88 iter 19 loss=0.03463321551680565\n",
      "epoch 88 iter 20 loss=0.06796197593212128\n",
      "epoch 88 iter 21 loss=0.03686759993433952\n",
      "epoch 88 iter 22 loss=0.06032203882932663\n",
      "epoch 88 iter 23 loss=0.0673062801361084\n",
      "epoch 88 iter 24 loss=0.04260287433862686\n",
      "epoch 88 iter 25 loss=0.04914252087473869\n",
      "epoch 88 iter 26 loss=0.06763700395822525\n",
      "epoch 88 iter 27 loss=0.06872864067554474\n",
      "epoch 88 iter 28 loss=0.05309204384684563\n",
      "epoch 88 iter 29 loss=0.07082556188106537\n",
      "epoch 88 iter 30 loss=0.049956176429986954\n",
      "epoch 88 iter 31 loss=0.06613938510417938\n",
      "epoch 88 iter 32 loss=0.06212727725505829\n",
      "epoch 88 iter 33 loss=0.0711086317896843\n",
      "epoch 88 iter 34 loss=0.04927436262369156\n",
      "epoch 88 iter 35 loss=0.06805314123630524\n",
      "epoch 88 iter 36 loss=0.08115394413471222\n",
      "epoch 88 iter 37 loss=0.07255292683839798\n",
      "epoch 88 iter 38 loss=0.07813026010990143\n",
      "epoch 88 iter 39 loss=0.08066863566637039\n",
      "epoch 88 iter 40 loss=0.06881831586360931\n",
      "epoch 88 iter 41 loss=0.10518407821655273\n",
      "epoch 88 iter 42 loss=0.06790342926979065\n",
      "epoch 88 iter 43 loss=0.06880933791399002\n",
      "epoch 88 iter 44 loss=0.06651154160499573\n",
      "epoch 88 iter 45 loss=0.08050941675901413\n",
      "epoch 88 iter 46 loss=0.0401662178337574\n",
      "epoch 88 iter 47 loss=0.05948040634393692\n",
      "epoch 88 iter 48 loss=0.07523492723703384\n",
      "epoch 88 iter 49 loss=0.06872262060642242\n",
      "epoch 88 iter 50 loss=0.044562920928001404\n",
      "epoch 88 iter 51 loss=0.05110844969749451\n",
      "epoch 88 iter 52 loss=0.05729018524289131\n",
      "epoch 88 iter 53 loss=0.0525173582136631\n",
      "epoch 88 iter 54 loss=0.06360147148370743\n",
      "epoch 88 iter 55 loss=0.03860262408852577\n",
      "epoch 88 iter 56 loss=0.06308415532112122\n",
      "epoch 88 iter 57 loss=0.03856940194964409\n",
      "epoch 88 iter 58 loss=0.07853250950574875\n",
      "epoch 88 iter 59 loss=0.06319204717874527\n",
      "epoch 88 iter 60 loss=0.06748327612876892\n",
      "epoch 88 iter 61 loss=0.08053935319185257\n",
      "epoch 88 iter 62 loss=0.041662342846393585\n",
      "epoch 88 iter 63 loss=0.05574851110577583\n",
      "epoch 88 iter 64 loss=0.048528365790843964\n",
      "epoch 88 iter 65 loss=0.0679141953587532\n",
      "epoch 88 iter 66 loss=0.05468752607703209\n",
      "epoch 88 iter 67 loss=0.07154804468154907\n",
      "epoch 88 iter 68 loss=0.106712207198143\n",
      "epoch 88 iter 69 loss=0.04722272604703903\n",
      "epoch 88 iter 70 loss=0.09202034771442413\n",
      "epoch 88 iter 71 loss=0.06288506090641022\n",
      "epoch 88 iter 72 loss=0.03206354379653931\n",
      "epoch 88 iter 73 loss=0.0390680767595768\n",
      "epoch 88 iter 74 loss=0.08517434448003769\n",
      "epoch 89 iter 0 loss=0.051182400435209274\n",
      "epoch 89 iter 1 loss=0.06836789101362228\n",
      "epoch 89 iter 2 loss=0.03814644366502762\n",
      "epoch 89 iter 3 loss=0.06650304049253464\n",
      "epoch 89 iter 4 loss=0.048805318772792816\n",
      "epoch 89 iter 5 loss=0.09258037805557251\n",
      "epoch 89 iter 6 loss=0.06692668050527573\n",
      "epoch 89 iter 7 loss=0.06260102242231369\n",
      "epoch 89 iter 8 loss=0.0777340903878212\n",
      "epoch 89 iter 9 loss=0.06633789092302322\n",
      "epoch 89 iter 10 loss=0.07248590141534805\n",
      "epoch 89 iter 11 loss=0.04389575868844986\n",
      "epoch 89 iter 12 loss=0.06526286154985428\n",
      "epoch 89 iter 13 loss=0.0837879553437233\n",
      "epoch 89 iter 14 loss=0.0872795507311821\n",
      "epoch 89 iter 15 loss=0.06196792796254158\n",
      "epoch 89 iter 16 loss=0.03991401568055153\n",
      "epoch 89 iter 17 loss=0.06398028880357742\n",
      "epoch 89 iter 18 loss=0.043279677629470825\n",
      "epoch 89 iter 19 loss=0.12159367650747299\n",
      "epoch 89 iter 20 loss=0.06586417555809021\n",
      "epoch 89 iter 21 loss=0.05305836722254753\n",
      "epoch 89 iter 22 loss=0.08483222872018814\n",
      "epoch 89 iter 23 loss=0.057792577892541885\n",
      "epoch 89 iter 24 loss=0.06964937597513199\n",
      "epoch 89 iter 25 loss=0.045958712697029114\n",
      "epoch 89 iter 26 loss=0.05777376517653465\n",
      "epoch 89 iter 27 loss=0.05257447063922882\n",
      "epoch 89 iter 28 loss=0.036113131791353226\n",
      "epoch 89 iter 29 loss=0.032898347824811935\n",
      "epoch 89 iter 30 loss=0.06203731521964073\n",
      "epoch 89 iter 31 loss=0.05903410539031029\n",
      "epoch 89 iter 32 loss=0.032303933054208755\n",
      "epoch 89 iter 33 loss=0.04138968884944916\n",
      "epoch 89 iter 34 loss=0.04671941697597504\n",
      "epoch 89 iter 35 loss=0.04529602825641632\n",
      "epoch 89 iter 36 loss=0.06654111295938492\n",
      "epoch 89 iter 37 loss=0.04609785974025726\n",
      "epoch 89 iter 38 loss=0.04181036725640297\n",
      "epoch 89 iter 39 loss=0.045238129794597626\n",
      "epoch 89 iter 40 loss=0.04881471022963524\n",
      "epoch 89 iter 41 loss=0.07903455197811127\n",
      "epoch 89 iter 42 loss=0.07252661138772964\n",
      "epoch 89 iter 43 loss=0.06842676550149918\n",
      "epoch 89 iter 44 loss=0.07571402937173843\n",
      "epoch 89 iter 45 loss=0.0908251702785492\n",
      "epoch 89 iter 46 loss=0.048592180013656616\n",
      "epoch 89 iter 47 loss=0.05106665566563606\n",
      "epoch 89 iter 48 loss=0.0412006676197052\n",
      "epoch 89 iter 49 loss=0.07166629284620285\n",
      "epoch 89 iter 50 loss=0.0889538824558258\n",
      "epoch 89 iter 51 loss=0.05598369985818863\n",
      "epoch 89 iter 52 loss=0.07234726846218109\n",
      "epoch 89 iter 53 loss=0.07365874201059341\n",
      "epoch 89 iter 54 loss=0.05424465984106064\n",
      "epoch 89 iter 55 loss=0.10693666338920593\n",
      "epoch 89 iter 56 loss=0.044705163687467575\n",
      "epoch 89 iter 57 loss=0.055300306528806686\n",
      "epoch 89 iter 58 loss=0.08138412237167358\n",
      "epoch 89 iter 59 loss=0.05756517872214317\n",
      "epoch 89 iter 60 loss=0.05250316485762596\n",
      "epoch 89 iter 61 loss=0.09443040937185287\n",
      "epoch 89 iter 62 loss=0.06157289817929268\n",
      "epoch 89 iter 63 loss=0.03358079865574837\n",
      "epoch 89 iter 64 loss=0.07696820795536041\n",
      "epoch 89 iter 65 loss=0.05178653821349144\n",
      "epoch 89 iter 66 loss=0.06475990265607834\n",
      "epoch 89 iter 67 loss=0.09049536287784576\n",
      "epoch 89 iter 68 loss=0.06833982467651367\n",
      "epoch 89 iter 69 loss=0.04512477666139603\n",
      "epoch 89 iter 70 loss=0.08492782711982727\n",
      "epoch 89 iter 71 loss=0.06319412589073181\n",
      "epoch 89 iter 72 loss=0.057494860142469406\n",
      "epoch 89 iter 73 loss=0.05757230892777443\n",
      "epoch 89 iter 74 loss=0.047368373721838\n",
      "epoch 90 iter 0 loss=0.06986035406589508\n",
      "epoch 90 iter 1 loss=0.0703139677643776\n",
      "epoch 90 iter 2 loss=0.03690725564956665\n",
      "epoch 90 iter 3 loss=0.048405952751636505\n",
      "epoch 90 iter 4 loss=0.06867092102766037\n",
      "epoch 90 iter 5 loss=0.06466130167245865\n",
      "epoch 90 iter 6 loss=0.04453671723604202\n",
      "epoch 90 iter 7 loss=0.09223353117704391\n",
      "epoch 90 iter 8 loss=0.09109259396791458\n",
      "epoch 90 iter 9 loss=0.06421034783124924\n",
      "epoch 90 iter 10 loss=0.0556257888674736\n",
      "epoch 90 iter 11 loss=0.04798618331551552\n",
      "epoch 90 iter 12 loss=0.05652150511741638\n",
      "epoch 90 iter 13 loss=0.12256083637475967\n",
      "epoch 90 iter 14 loss=0.04671391472220421\n",
      "epoch 90 iter 15 loss=0.07183833420276642\n",
      "epoch 90 iter 16 loss=0.07918942719697952\n",
      "epoch 90 iter 17 loss=0.07719019800424576\n",
      "epoch 90 iter 18 loss=0.06367179751396179\n",
      "epoch 90 iter 19 loss=0.05269094184041023\n",
      "epoch 90 iter 20 loss=0.10348426550626755\n",
      "epoch 90 iter 21 loss=0.07302914559841156\n",
      "epoch 90 iter 22 loss=0.11882137507200241\n",
      "epoch 90 iter 23 loss=0.048584531992673874\n",
      "epoch 90 iter 24 loss=0.03498179093003273\n",
      "epoch 90 iter 25 loss=0.0677332654595375\n",
      "epoch 90 iter 26 loss=0.09187020361423492\n",
      "epoch 90 iter 27 loss=0.07213802635669708\n",
      "epoch 90 iter 28 loss=0.053363122045993805\n",
      "epoch 90 iter 29 loss=0.07434870302677155\n",
      "epoch 90 iter 30 loss=0.11432649195194244\n",
      "epoch 90 iter 31 loss=0.06315070390701294\n",
      "epoch 90 iter 32 loss=0.06394016742706299\n",
      "epoch 90 iter 33 loss=0.05518903583288193\n",
      "epoch 90 iter 34 loss=0.05320236459374428\n",
      "epoch 90 iter 35 loss=0.05315868929028511\n",
      "epoch 90 iter 36 loss=0.0632886290550232\n",
      "epoch 90 iter 37 loss=0.0650695338845253\n",
      "epoch 90 iter 38 loss=0.06309188902378082\n",
      "epoch 90 iter 39 loss=0.0863809734582901\n",
      "epoch 90 iter 40 loss=0.029753029346466064\n",
      "epoch 90 iter 41 loss=0.04079565405845642\n",
      "epoch 90 iter 42 loss=0.04618107154965401\n",
      "epoch 90 iter 43 loss=0.06556306779384613\n",
      "epoch 90 iter 44 loss=0.03589339181780815\n",
      "epoch 90 iter 45 loss=0.04479904845356941\n",
      "epoch 90 iter 46 loss=0.04809741675853729\n",
      "epoch 90 iter 47 loss=0.0445825420320034\n",
      "epoch 90 iter 48 loss=0.06812474131584167\n",
      "epoch 90 iter 49 loss=0.10371752828359604\n",
      "epoch 90 iter 50 loss=0.057158391922712326\n",
      "epoch 90 iter 51 loss=0.06964485347270966\n",
      "epoch 90 iter 52 loss=0.07702378183603287\n",
      "epoch 90 iter 53 loss=0.05856672301888466\n",
      "epoch 90 iter 54 loss=0.08743608742952347\n",
      "epoch 90 iter 55 loss=0.05661111697554588\n",
      "epoch 90 iter 56 loss=0.07448769360780716\n",
      "epoch 90 iter 57 loss=0.0420849472284317\n",
      "epoch 90 iter 58 loss=0.08120401948690414\n",
      "epoch 90 iter 59 loss=0.06527315080165863\n",
      "epoch 90 iter 60 loss=0.07341032475233078\n",
      "epoch 90 iter 61 loss=0.06585176289081573\n",
      "epoch 90 iter 62 loss=0.04431682825088501\n",
      "epoch 90 iter 63 loss=0.0680723786354065\n",
      "epoch 90 iter 64 loss=0.0895988941192627\n",
      "epoch 90 iter 65 loss=0.047214604914188385\n",
      "epoch 90 iter 66 loss=0.040862876921892166\n",
      "epoch 90 iter 67 loss=0.06622359156608582\n",
      "epoch 90 iter 68 loss=0.06049344688653946\n",
      "epoch 90 iter 69 loss=0.06432005017995834\n",
      "epoch 90 iter 70 loss=0.039547257125377655\n",
      "epoch 90 iter 71 loss=0.09218735992908478\n",
      "epoch 90 iter 72 loss=0.052127379924058914\n",
      "epoch 90 iter 73 loss=0.06210746616125107\n",
      "epoch 90 iter 74 loss=0.08326829969882965\n",
      "epoch 91 iter 0 loss=0.10291364789009094\n",
      "epoch 91 iter 1 loss=0.04236159101128578\n",
      "epoch 91 iter 2 loss=0.06861306726932526\n",
      "epoch 91 iter 3 loss=0.08962168544530869\n",
      "epoch 91 iter 4 loss=0.062000323086977005\n",
      "epoch 91 iter 5 loss=0.10923697054386139\n",
      "epoch 91 iter 6 loss=0.07168740779161453\n",
      "epoch 91 iter 7 loss=0.05522914603352547\n",
      "epoch 91 iter 8 loss=0.0757790058851242\n",
      "epoch 91 iter 9 loss=0.0324556827545166\n",
      "epoch 91 iter 10 loss=0.048066396266222\n",
      "epoch 91 iter 11 loss=0.06318778544664383\n",
      "epoch 91 iter 12 loss=0.06077352538704872\n",
      "epoch 91 iter 13 loss=0.05434666574001312\n",
      "epoch 91 iter 14 loss=0.08124792575836182\n",
      "epoch 91 iter 15 loss=0.06863098591566086\n",
      "epoch 91 iter 16 loss=0.05830754339694977\n",
      "epoch 91 iter 17 loss=0.03826799616217613\n",
      "epoch 91 iter 18 loss=0.05736264958977699\n",
      "epoch 91 iter 19 loss=0.07323566824197769\n",
      "epoch 91 iter 20 loss=0.032700251787900925\n",
      "epoch 91 iter 21 loss=0.04564328491687775\n",
      "epoch 91 iter 22 loss=0.06629061698913574\n",
      "epoch 91 iter 23 loss=0.07609803974628448\n",
      "epoch 91 iter 24 loss=0.05349449813365936\n",
      "epoch 91 iter 25 loss=0.05994076654314995\n",
      "epoch 91 iter 26 loss=0.0798817127943039\n",
      "epoch 91 iter 27 loss=0.06715575605630875\n",
      "epoch 91 iter 28 loss=0.04815283790230751\n",
      "epoch 91 iter 29 loss=0.07069792598485947\n",
      "epoch 91 iter 30 loss=0.08451748639345169\n",
      "epoch 91 iter 31 loss=0.04179507866501808\n",
      "epoch 91 iter 32 loss=0.04005863890051842\n",
      "epoch 91 iter 33 loss=0.06444448232650757\n",
      "epoch 91 iter 34 loss=0.053496669977903366\n",
      "epoch 91 iter 35 loss=0.058138150721788406\n",
      "epoch 91 iter 36 loss=0.0576116144657135\n",
      "epoch 91 iter 37 loss=0.03870103135704994\n",
      "epoch 91 iter 38 loss=0.08218929171562195\n",
      "epoch 91 iter 39 loss=0.06705398112535477\n",
      "epoch 91 iter 40 loss=0.06877391785383224\n",
      "epoch 91 iter 41 loss=0.057593803852796555\n",
      "epoch 91 iter 42 loss=0.05944199487566948\n",
      "epoch 91 iter 43 loss=0.04967876151204109\n",
      "epoch 91 iter 44 loss=0.06237585097551346\n",
      "epoch 91 iter 45 loss=0.09448661655187607\n",
      "epoch 91 iter 46 loss=0.07838736474514008\n",
      "epoch 91 iter 47 loss=0.045640163123607635\n",
      "epoch 91 iter 48 loss=0.06062227115035057\n",
      "epoch 91 iter 49 loss=0.08560238778591156\n",
      "epoch 91 iter 50 loss=0.06681196391582489\n",
      "epoch 91 iter 51 loss=0.09144660085439682\n",
      "epoch 91 iter 52 loss=0.02874978817999363\n",
      "epoch 91 iter 53 loss=0.05957591533660889\n",
      "epoch 91 iter 54 loss=0.080565445125103\n",
      "epoch 91 iter 55 loss=0.04501831904053688\n",
      "epoch 91 iter 56 loss=0.05123978853225708\n",
      "epoch 91 iter 57 loss=0.05899136886000633\n",
      "epoch 91 iter 58 loss=0.06460555642843246\n",
      "epoch 91 iter 59 loss=0.06003366410732269\n",
      "epoch 91 iter 60 loss=0.05447406694293022\n",
      "epoch 91 iter 61 loss=0.036327678710222244\n",
      "epoch 91 iter 62 loss=0.06768985092639923\n",
      "epoch 91 iter 63 loss=0.053139105439186096\n",
      "epoch 91 iter 64 loss=0.047404635697603226\n",
      "epoch 91 iter 65 loss=0.0516536645591259\n",
      "epoch 91 iter 66 loss=0.07539809495210648\n",
      "epoch 91 iter 67 loss=0.04149630293250084\n",
      "epoch 91 iter 68 loss=0.05434417352080345\n",
      "epoch 91 iter 69 loss=0.05032343044877052\n",
      "epoch 91 iter 70 loss=0.06677863746881485\n",
      "epoch 91 iter 71 loss=0.05169619992375374\n",
      "epoch 91 iter 72 loss=0.06522474437952042\n",
      "epoch 91 iter 73 loss=0.06795677542686462\n",
      "epoch 91 iter 74 loss=0.05033345893025398\n",
      "epoch 92 iter 0 loss=0.03926094248890877\n",
      "epoch 92 iter 1 loss=0.03236865997314453\n",
      "epoch 92 iter 2 loss=0.05066116526722908\n",
      "epoch 92 iter 3 loss=0.07446742057800293\n",
      "epoch 92 iter 4 loss=0.08313685655593872\n",
      "epoch 92 iter 5 loss=0.06268453598022461\n",
      "epoch 92 iter 6 loss=0.03227832168340683\n",
      "epoch 92 iter 7 loss=0.046485479921102524\n",
      "epoch 92 iter 8 loss=0.07374979555606842\n",
      "epoch 92 iter 9 loss=0.036312513053417206\n",
      "epoch 92 iter 10 loss=0.05176114663481712\n",
      "epoch 92 iter 11 loss=0.053264014422893524\n",
      "epoch 92 iter 12 loss=0.06257041543722153\n",
      "epoch 92 iter 13 loss=0.07947324961423874\n",
      "epoch 92 iter 14 loss=0.08295568078756332\n",
      "epoch 92 iter 15 loss=0.04945601895451546\n",
      "epoch 92 iter 16 loss=0.0944090485572815\n",
      "epoch 92 iter 17 loss=0.04393131658434868\n",
      "epoch 92 iter 18 loss=0.07869084924459457\n",
      "epoch 92 iter 19 loss=0.04669073969125748\n",
      "epoch 92 iter 20 loss=0.07367584109306335\n",
      "epoch 92 iter 21 loss=0.056624215096235275\n",
      "epoch 92 iter 22 loss=0.027518514543771744\n",
      "epoch 92 iter 23 loss=0.07476252317428589\n",
      "epoch 92 iter 24 loss=0.07403095066547394\n",
      "epoch 92 iter 25 loss=0.06877453625202179\n",
      "epoch 92 iter 26 loss=0.05948564410209656\n",
      "epoch 92 iter 27 loss=0.0567830465734005\n",
      "epoch 92 iter 28 loss=0.04318738728761673\n",
      "epoch 92 iter 29 loss=0.07604127377271652\n",
      "epoch 92 iter 30 loss=0.04155503213405609\n",
      "epoch 92 iter 31 loss=0.07711594551801682\n",
      "epoch 92 iter 32 loss=0.0354970283806324\n",
      "epoch 92 iter 33 loss=0.04779951274394989\n",
      "epoch 92 iter 34 loss=0.057554978877305984\n",
      "epoch 92 iter 35 loss=0.055334050208330154\n",
      "epoch 92 iter 36 loss=0.051515668630599976\n",
      "epoch 92 iter 37 loss=0.057736825197935104\n",
      "epoch 92 iter 38 loss=0.045607663691043854\n",
      "epoch 92 iter 39 loss=0.04784739762544632\n",
      "epoch 92 iter 40 loss=0.07808180153369904\n",
      "epoch 92 iter 41 loss=0.04298313707113266\n",
      "epoch 92 iter 42 loss=0.05114134028553963\n",
      "epoch 92 iter 43 loss=0.07799157500267029\n",
      "epoch 92 iter 44 loss=0.07228076457977295\n",
      "epoch 92 iter 45 loss=0.08692900836467743\n",
      "epoch 92 iter 46 loss=0.052427392452955246\n",
      "epoch 92 iter 47 loss=0.03724342957139015\n",
      "epoch 92 iter 48 loss=0.04057098552584648\n",
      "epoch 92 iter 49 loss=0.0792360007762909\n",
      "epoch 92 iter 50 loss=0.06619540601968765\n",
      "epoch 92 iter 51 loss=0.06980691850185394\n",
      "epoch 92 iter 52 loss=0.0919577106833458\n",
      "epoch 92 iter 53 loss=0.04359618201851845\n",
      "epoch 92 iter 54 loss=0.08469508588314056\n",
      "epoch 92 iter 55 loss=0.04257524758577347\n",
      "epoch 92 iter 56 loss=0.0659940242767334\n",
      "epoch 92 iter 57 loss=0.03768566995859146\n",
      "epoch 92 iter 58 loss=0.07114429771900177\n",
      "epoch 92 iter 59 loss=0.056347474455833435\n",
      "epoch 92 iter 60 loss=0.08501379191875458\n",
      "epoch 92 iter 61 loss=0.07866683602333069\n",
      "epoch 92 iter 62 loss=0.05473104119300842\n",
      "epoch 92 iter 63 loss=0.06589540839195251\n",
      "epoch 92 iter 64 loss=0.04906662926077843\n",
      "epoch 92 iter 65 loss=0.06053277477622032\n",
      "epoch 92 iter 66 loss=0.04959782212972641\n",
      "epoch 92 iter 67 loss=0.03957952558994293\n",
      "epoch 92 iter 68 loss=0.06991926580667496\n",
      "epoch 92 iter 69 loss=0.060074158012866974\n",
      "epoch 92 iter 70 loss=0.07798881828784943\n",
      "epoch 92 iter 71 loss=0.05095739662647247\n",
      "epoch 92 iter 72 loss=0.060981906950473785\n",
      "epoch 92 iter 73 loss=0.04646890237927437\n",
      "epoch 92 iter 74 loss=0.06828273087739944\n",
      "epoch 93 iter 0 loss=0.04450548440217972\n",
      "epoch 93 iter 1 loss=0.0714336410164833\n",
      "epoch 93 iter 2 loss=0.05765261501073837\n",
      "epoch 93 iter 3 loss=0.0824798196554184\n",
      "epoch 93 iter 4 loss=0.05322713032364845\n",
      "epoch 93 iter 5 loss=0.055627577006816864\n",
      "epoch 93 iter 6 loss=0.08603864908218384\n",
      "epoch 93 iter 7 loss=0.050361521542072296\n",
      "epoch 93 iter 8 loss=0.03504403680562973\n",
      "epoch 93 iter 9 loss=0.04388722404837608\n",
      "epoch 93 iter 10 loss=0.04119594767689705\n",
      "epoch 93 iter 11 loss=0.07742849737405777\n",
      "epoch 93 iter 12 loss=0.04874740168452263\n",
      "epoch 93 iter 13 loss=0.07020571082830429\n",
      "epoch 93 iter 14 loss=0.07033240050077438\n",
      "epoch 93 iter 15 loss=0.05400491878390312\n",
      "epoch 93 iter 16 loss=0.06378173828125\n",
      "epoch 93 iter 17 loss=0.05312802642583847\n",
      "epoch 93 iter 18 loss=0.1034596636891365\n",
      "epoch 93 iter 19 loss=0.0826893299818039\n",
      "epoch 93 iter 20 loss=0.03698766231536865\n",
      "epoch 93 iter 21 loss=0.030062029138207436\n",
      "epoch 93 iter 22 loss=0.04801582917571068\n",
      "epoch 93 iter 23 loss=0.04901951551437378\n",
      "epoch 93 iter 24 loss=0.03978246822953224\n",
      "epoch 93 iter 25 loss=0.08180394023656845\n",
      "epoch 93 iter 26 loss=0.046826306730508804\n",
      "epoch 93 iter 27 loss=0.04156506434082985\n",
      "epoch 93 iter 28 loss=0.08882154524326324\n",
      "epoch 93 iter 29 loss=0.06780529767274857\n",
      "epoch 93 iter 30 loss=0.03938748687505722\n",
      "epoch 93 iter 31 loss=0.07761780917644501\n",
      "epoch 93 iter 32 loss=0.07461946457624435\n",
      "epoch 93 iter 33 loss=0.050366368144750595\n",
      "epoch 93 iter 34 loss=0.05512421950697899\n",
      "epoch 93 iter 35 loss=0.0450434572994709\n",
      "epoch 93 iter 36 loss=0.056096337735652924\n",
      "epoch 93 iter 37 loss=0.07643784582614899\n",
      "epoch 93 iter 38 loss=0.0687200054526329\n",
      "epoch 93 iter 39 loss=0.07011570036411285\n",
      "epoch 93 iter 40 loss=0.06837233901023865\n",
      "epoch 93 iter 41 loss=0.05420916900038719\n",
      "epoch 93 iter 42 loss=0.0696990117430687\n",
      "epoch 93 iter 43 loss=0.07223942130804062\n",
      "epoch 93 iter 44 loss=0.06145805865526199\n",
      "epoch 93 iter 45 loss=0.05475004389882088\n",
      "epoch 93 iter 46 loss=0.08983252197504044\n",
      "epoch 93 iter 47 loss=0.03421252965927124\n",
      "epoch 93 iter 48 loss=0.034601639956235886\n",
      "epoch 93 iter 49 loss=0.03533940017223358\n",
      "epoch 93 iter 50 loss=0.054935235530138016\n",
      "epoch 93 iter 51 loss=0.07187735289335251\n",
      "epoch 93 iter 52 loss=0.032181207090616226\n",
      "epoch 93 iter 53 loss=0.1042485237121582\n",
      "epoch 93 iter 54 loss=0.06842783838510513\n",
      "epoch 93 iter 55 loss=0.06444824486970901\n",
      "epoch 93 iter 56 loss=0.06699518114328384\n",
      "epoch 93 iter 57 loss=0.06264655292034149\n",
      "epoch 93 iter 58 loss=0.04429143667221069\n",
      "epoch 93 iter 59 loss=0.03575993329286575\n",
      "epoch 93 iter 60 loss=0.061464425176382065\n",
      "epoch 93 iter 61 loss=0.04436400160193443\n",
      "epoch 93 iter 62 loss=0.07530789077281952\n",
      "epoch 93 iter 63 loss=0.056693270802497864\n",
      "epoch 93 iter 64 loss=0.05420297011733055\n",
      "epoch 93 iter 65 loss=0.07252747565507889\n",
      "epoch 93 iter 66 loss=0.04786558076739311\n",
      "epoch 93 iter 67 loss=0.04597343131899834\n",
      "epoch 93 iter 68 loss=0.0516662523150444\n",
      "epoch 93 iter 69 loss=0.034814197570085526\n",
      "epoch 93 iter 70 loss=0.05741178244352341\n",
      "epoch 93 iter 71 loss=0.06896919012069702\n",
      "epoch 93 iter 72 loss=0.050288230180740356\n",
      "epoch 93 iter 73 loss=0.04610700532793999\n",
      "epoch 93 iter 74 loss=0.05399145558476448\n",
      "epoch 94 iter 0 loss=0.043542057275772095\n",
      "epoch 94 iter 1 loss=0.07898716628551483\n",
      "epoch 94 iter 2 loss=0.04617416113615036\n",
      "epoch 94 iter 3 loss=0.0489811971783638\n",
      "epoch 94 iter 4 loss=0.07686137408018112\n",
      "epoch 94 iter 5 loss=0.06465540081262589\n",
      "epoch 94 iter 6 loss=0.0657159686088562\n",
      "epoch 94 iter 7 loss=0.03416815772652626\n",
      "epoch 94 iter 8 loss=0.0525704026222229\n",
      "epoch 94 iter 9 loss=0.06605740636587143\n",
      "epoch 94 iter 10 loss=0.07308033108711243\n",
      "epoch 94 iter 11 loss=0.06095494329929352\n",
      "epoch 94 iter 12 loss=0.031607016921043396\n",
      "epoch 94 iter 13 loss=0.061329081654548645\n",
      "epoch 94 iter 14 loss=0.05588191747665405\n",
      "epoch 94 iter 15 loss=0.054612915962934494\n",
      "epoch 94 iter 16 loss=0.051140185445547104\n",
      "epoch 94 iter 17 loss=0.05146751552820206\n",
      "epoch 94 iter 18 loss=0.08511707931756973\n",
      "epoch 94 iter 19 loss=0.05166767165064812\n",
      "epoch 94 iter 20 loss=0.04552190750837326\n",
      "epoch 94 iter 21 loss=0.06905725598335266\n",
      "epoch 94 iter 22 loss=0.10994414240121841\n",
      "epoch 94 iter 23 loss=0.04824339970946312\n",
      "epoch 94 iter 24 loss=0.08175033330917358\n",
      "epoch 94 iter 25 loss=0.06930412352085114\n",
      "epoch 94 iter 26 loss=0.09121637791395187\n",
      "epoch 94 iter 27 loss=0.054600074887275696\n",
      "epoch 94 iter 28 loss=0.054407864809036255\n",
      "epoch 94 iter 29 loss=0.04244410619139671\n",
      "epoch 94 iter 30 loss=0.10552006959915161\n",
      "epoch 94 iter 31 loss=0.04402890056371689\n",
      "epoch 94 iter 32 loss=0.08584421128034592\n",
      "epoch 94 iter 33 loss=0.03985455259680748\n",
      "epoch 94 iter 34 loss=0.06333735585212708\n",
      "epoch 94 iter 35 loss=0.045985810458660126\n",
      "epoch 94 iter 36 loss=0.0366416871547699\n",
      "epoch 94 iter 37 loss=0.05354048311710358\n",
      "epoch 94 iter 38 loss=0.056282080709934235\n",
      "epoch 94 iter 39 loss=0.05532181262969971\n",
      "epoch 94 iter 40 loss=0.02770748920738697\n",
      "epoch 94 iter 41 loss=0.038655199110507965\n",
      "epoch 94 iter 42 loss=0.05353934317827225\n",
      "epoch 94 iter 43 loss=0.06002623960375786\n",
      "epoch 94 iter 44 loss=0.053265318274497986\n",
      "epoch 94 iter 45 loss=0.08378396928310394\n",
      "epoch 94 iter 46 loss=0.09290292114019394\n",
      "epoch 94 iter 47 loss=0.053919415920972824\n",
      "epoch 94 iter 48 loss=0.041091807186603546\n",
      "epoch 94 iter 49 loss=0.08006039261817932\n",
      "epoch 94 iter 50 loss=0.06619838625192642\n",
      "epoch 94 iter 51 loss=0.035187799483537674\n",
      "epoch 94 iter 52 loss=0.06175096705555916\n",
      "epoch 94 iter 53 loss=0.05202851444482803\n",
      "epoch 94 iter 54 loss=0.07581222057342529\n",
      "epoch 94 iter 55 loss=0.044721659272909164\n",
      "epoch 94 iter 56 loss=0.053212642669677734\n",
      "epoch 94 iter 57 loss=0.061452288180589676\n",
      "epoch 94 iter 58 loss=0.08267513662576675\n",
      "epoch 94 iter 59 loss=0.05308321863412857\n",
      "epoch 94 iter 60 loss=0.07837171852588654\n",
      "epoch 94 iter 61 loss=0.04422540217638016\n",
      "epoch 94 iter 62 loss=0.06204593926668167\n",
      "epoch 94 iter 63 loss=0.04927758127450943\n",
      "epoch 94 iter 64 loss=0.05601329356431961\n",
      "epoch 94 iter 65 loss=0.09612725675106049\n",
      "epoch 94 iter 66 loss=0.06705624610185623\n",
      "epoch 94 iter 67 loss=0.07268112897872925\n",
      "epoch 94 iter 68 loss=0.07017811387777328\n",
      "epoch 94 iter 69 loss=0.04801870882511139\n",
      "epoch 94 iter 70 loss=0.059408869594335556\n",
      "epoch 94 iter 71 loss=0.07843019813299179\n",
      "epoch 94 iter 72 loss=0.05734262987971306\n",
      "epoch 94 iter 73 loss=0.0525711290538311\n",
      "epoch 94 iter 74 loss=0.030729271471500397\n",
      "epoch 95 iter 0 loss=0.059735264629125595\n",
      "epoch 95 iter 1 loss=0.06078034266829491\n",
      "epoch 95 iter 2 loss=0.03859679400920868\n",
      "epoch 95 iter 3 loss=0.047884657979011536\n",
      "epoch 95 iter 4 loss=0.03733548894524574\n",
      "epoch 95 iter 5 loss=0.051564786583185196\n",
      "epoch 95 iter 6 loss=0.07165131717920303\n",
      "epoch 95 iter 7 loss=0.0756772831082344\n",
      "epoch 95 iter 8 loss=0.09906195849180222\n",
      "epoch 95 iter 9 loss=0.057324349880218506\n",
      "epoch 95 iter 10 loss=0.05332675203680992\n",
      "epoch 95 iter 11 loss=0.03617079183459282\n",
      "epoch 95 iter 12 loss=0.05728156864643097\n",
      "epoch 95 iter 13 loss=0.0832095667719841\n",
      "epoch 95 iter 14 loss=0.06379491835832596\n",
      "epoch 95 iter 15 loss=0.03411865234375\n",
      "epoch 95 iter 16 loss=0.05956895276904106\n",
      "epoch 95 iter 17 loss=0.02824409492313862\n",
      "epoch 95 iter 18 loss=0.04707413166761398\n",
      "epoch 95 iter 19 loss=0.08677690476179123\n",
      "epoch 95 iter 20 loss=0.0629272535443306\n",
      "epoch 95 iter 21 loss=0.04064847528934479\n",
      "epoch 95 iter 22 loss=0.06598614901304245\n",
      "epoch 95 iter 23 loss=0.055714402347803116\n",
      "epoch 95 iter 24 loss=0.0647895336151123\n",
      "epoch 95 iter 25 loss=0.04802759736776352\n",
      "epoch 95 iter 26 loss=0.05809254199266434\n",
      "epoch 95 iter 27 loss=0.05671010538935661\n",
      "epoch 95 iter 28 loss=0.07584132999181747\n",
      "epoch 95 iter 29 loss=0.048976004123687744\n",
      "epoch 95 iter 30 loss=0.05279839411377907\n",
      "epoch 95 iter 31 loss=0.06220841407775879\n",
      "epoch 95 iter 32 loss=0.05694885179400444\n",
      "epoch 95 iter 33 loss=0.03420836851000786\n",
      "epoch 95 iter 34 loss=0.0743880644440651\n",
      "epoch 95 iter 35 loss=0.05033745616674423\n",
      "epoch 95 iter 36 loss=0.05070280656218529\n",
      "epoch 95 iter 37 loss=0.0391661562025547\n",
      "epoch 95 iter 38 loss=0.07036426663398743\n",
      "epoch 95 iter 39 loss=0.04225907474756241\n",
      "epoch 95 iter 40 loss=0.03623221442103386\n",
      "epoch 95 iter 41 loss=0.047691114246845245\n",
      "epoch 95 iter 42 loss=0.0991622731089592\n",
      "epoch 95 iter 43 loss=0.04415206238627434\n",
      "epoch 95 iter 44 loss=0.0676746666431427\n",
      "epoch 95 iter 45 loss=0.05566627159714699\n",
      "epoch 95 iter 46 loss=0.029414303600788116\n",
      "epoch 95 iter 47 loss=0.05097925662994385\n",
      "epoch 95 iter 48 loss=0.06975066661834717\n",
      "epoch 95 iter 49 loss=0.03853936493396759\n",
      "epoch 95 iter 50 loss=0.07073106616735458\n",
      "epoch 95 iter 51 loss=0.06193358078598976\n",
      "epoch 95 iter 52 loss=0.03901216387748718\n",
      "epoch 95 iter 53 loss=0.036470092833042145\n",
      "epoch 95 iter 54 loss=0.04810136929154396\n",
      "epoch 95 iter 55 loss=0.05495193600654602\n",
      "epoch 95 iter 56 loss=0.05899856612086296\n",
      "epoch 95 iter 57 loss=0.07996239513158798\n",
      "epoch 95 iter 58 loss=0.07618112117052078\n",
      "epoch 95 iter 59 loss=0.08612479269504547\n",
      "epoch 95 iter 60 loss=0.06913124769926071\n",
      "epoch 95 iter 61 loss=0.055734243243932724\n",
      "epoch 95 iter 62 loss=0.05019853264093399\n",
      "epoch 95 iter 63 loss=0.09607008099555969\n",
      "epoch 95 iter 64 loss=0.07620520889759064\n",
      "epoch 95 iter 65 loss=0.05210307985544205\n",
      "epoch 95 iter 66 loss=0.032816722989082336\n",
      "epoch 95 iter 67 loss=0.06951570510864258\n",
      "epoch 95 iter 68 loss=0.03424624353647232\n",
      "epoch 95 iter 69 loss=0.061190489679574966\n",
      "epoch 95 iter 70 loss=0.06684283167123795\n",
      "epoch 95 iter 71 loss=0.06007034704089165\n",
      "epoch 95 iter 72 loss=0.03360610827803612\n",
      "epoch 95 iter 73 loss=0.06633496284484863\n",
      "epoch 95 iter 74 loss=0.07207922637462616\n",
      "epoch 96 iter 0 loss=0.057779211550951004\n",
      "epoch 96 iter 1 loss=0.06125972792506218\n",
      "epoch 96 iter 2 loss=0.047138512134552\n",
      "epoch 96 iter 3 loss=0.03587570786476135\n",
      "epoch 96 iter 4 loss=0.061930615454912186\n",
      "epoch 96 iter 5 loss=0.047623004764318466\n",
      "epoch 96 iter 6 loss=0.06538187712430954\n",
      "epoch 96 iter 7 loss=0.027377255260944366\n",
      "epoch 96 iter 8 loss=0.044991716742515564\n",
      "epoch 96 iter 9 loss=0.04657573252916336\n",
      "epoch 96 iter 10 loss=0.031513918191194534\n",
      "epoch 96 iter 11 loss=0.08220237493515015\n",
      "epoch 96 iter 12 loss=0.04348760470747948\n",
      "epoch 96 iter 13 loss=0.05853131040930748\n",
      "epoch 96 iter 14 loss=0.05193275213241577\n",
      "epoch 96 iter 15 loss=0.0628599226474762\n",
      "epoch 96 iter 16 loss=0.03961201012134552\n",
      "epoch 96 iter 17 loss=0.043715108186006546\n",
      "epoch 96 iter 18 loss=0.06385938823223114\n",
      "epoch 96 iter 19 loss=0.05658956244587898\n",
      "epoch 96 iter 20 loss=0.042137887328863144\n",
      "epoch 96 iter 21 loss=0.04947168380022049\n",
      "epoch 96 iter 22 loss=0.06166405603289604\n",
      "epoch 96 iter 23 loss=0.0947488471865654\n",
      "epoch 96 iter 24 loss=0.08336968719959259\n",
      "epoch 96 iter 25 loss=0.05161137878894806\n",
      "epoch 96 iter 26 loss=0.0674300566315651\n",
      "epoch 96 iter 27 loss=0.07724789530038834\n",
      "epoch 96 iter 28 loss=0.034096986055374146\n",
      "epoch 96 iter 29 loss=0.06584852188825607\n",
      "epoch 96 iter 30 loss=0.03828847408294678\n",
      "epoch 96 iter 31 loss=0.03842414543032646\n",
      "epoch 96 iter 32 loss=0.04713468998670578\n",
      "epoch 96 iter 33 loss=0.06528761982917786\n",
      "epoch 96 iter 34 loss=0.057351578027009964\n",
      "epoch 96 iter 35 loss=0.037397004663944244\n",
      "epoch 96 iter 36 loss=0.0449485145509243\n",
      "epoch 96 iter 37 loss=0.07557474821805954\n",
      "epoch 96 iter 38 loss=0.0811920166015625\n",
      "epoch 96 iter 39 loss=0.032000862061977386\n",
      "epoch 96 iter 40 loss=0.03767246752977371\n",
      "epoch 96 iter 41 loss=0.05696118623018265\n",
      "epoch 96 iter 42 loss=0.06633435189723969\n",
      "epoch 96 iter 43 loss=0.060798246413469315\n",
      "epoch 96 iter 44 loss=0.04558435082435608\n",
      "epoch 96 iter 45 loss=0.06306351721286774\n",
      "epoch 96 iter 46 loss=0.036349546164274216\n",
      "epoch 96 iter 47 loss=0.0603119321167469\n",
      "epoch 96 iter 48 loss=0.07980238646268845\n",
      "epoch 96 iter 49 loss=0.05222489684820175\n",
      "epoch 96 iter 50 loss=0.04732593521475792\n",
      "epoch 96 iter 51 loss=0.0538056306540966\n",
      "epoch 96 iter 52 loss=0.07901947945356369\n",
      "epoch 96 iter 53 loss=0.049323465675115585\n",
      "epoch 96 iter 54 loss=0.058090366423130035\n",
      "epoch 96 iter 55 loss=0.07532266527414322\n",
      "epoch 96 iter 56 loss=0.05270454287528992\n",
      "epoch 96 iter 57 loss=0.08251188695430756\n",
      "epoch 96 iter 58 loss=0.0549745038151741\n",
      "epoch 96 iter 59 loss=0.049229253083467484\n",
      "epoch 96 iter 60 loss=0.06403236836194992\n",
      "epoch 96 iter 61 loss=0.06639465689659119\n",
      "epoch 96 iter 62 loss=0.08007621020078659\n",
      "epoch 96 iter 63 loss=0.06746939569711685\n",
      "epoch 96 iter 64 loss=0.039609890431165695\n",
      "epoch 96 iter 65 loss=0.05144919082522392\n",
      "epoch 96 iter 66 loss=0.050567321479320526\n",
      "epoch 96 iter 67 loss=0.04504967853426933\n",
      "epoch 96 iter 68 loss=0.0803348571062088\n",
      "epoch 96 iter 69 loss=0.08139532059431076\n",
      "epoch 96 iter 70 loss=0.0704081654548645\n",
      "epoch 96 iter 71 loss=0.0650234967470169\n",
      "epoch 96 iter 72 loss=0.06195194274187088\n",
      "epoch 96 iter 73 loss=0.0654144436120987\n",
      "epoch 96 iter 74 loss=0.058771099895238876\n",
      "epoch 97 iter 0 loss=0.03856175020337105\n",
      "epoch 97 iter 1 loss=0.06172505393624306\n",
      "epoch 97 iter 2 loss=0.05075819417834282\n",
      "epoch 97 iter 3 loss=0.06653214246034622\n",
      "epoch 97 iter 4 loss=0.04765315726399422\n",
      "epoch 97 iter 5 loss=0.08898307383060455\n",
      "epoch 97 iter 6 loss=0.049323670566082\n",
      "epoch 97 iter 7 loss=0.052557677030563354\n",
      "epoch 97 iter 8 loss=0.07459907233715057\n",
      "epoch 97 iter 9 loss=0.036479704082012177\n",
      "epoch 97 iter 10 loss=0.056602343916893005\n",
      "epoch 97 iter 11 loss=0.043973565101623535\n",
      "epoch 97 iter 12 loss=0.08390850573778152\n",
      "epoch 97 iter 13 loss=0.046471353620290756\n",
      "epoch 97 iter 14 loss=0.03238515183329582\n",
      "epoch 97 iter 15 loss=0.04117988422513008\n",
      "epoch 97 iter 16 loss=0.05084208771586418\n",
      "epoch 97 iter 17 loss=0.07217331230640411\n",
      "epoch 97 iter 18 loss=0.056648291647434235\n",
      "epoch 97 iter 19 loss=0.04363807290792465\n",
      "epoch 97 iter 20 loss=0.04983649030327797\n",
      "epoch 97 iter 21 loss=0.09673323482275009\n",
      "epoch 97 iter 22 loss=0.054742228239774704\n",
      "epoch 97 iter 23 loss=0.0421360544860363\n",
      "epoch 97 iter 24 loss=0.0439411960542202\n",
      "epoch 97 iter 25 loss=0.058758072555065155\n",
      "epoch 97 iter 26 loss=0.05024755001068115\n",
      "epoch 97 iter 27 loss=0.04729171842336655\n",
      "epoch 97 iter 28 loss=0.0645926296710968\n",
      "epoch 97 iter 29 loss=0.06571227312088013\n",
      "epoch 97 iter 30 loss=0.06285235285758972\n",
      "epoch 97 iter 31 loss=0.06606042385101318\n",
      "epoch 97 iter 32 loss=0.0999133363366127\n",
      "epoch 97 iter 33 loss=0.0703042671084404\n",
      "epoch 97 iter 34 loss=0.0720534473657608\n",
      "epoch 97 iter 35 loss=0.036154791712760925\n",
      "epoch 97 iter 36 loss=0.0817197635769844\n",
      "epoch 97 iter 37 loss=0.05574880167841911\n",
      "epoch 97 iter 38 loss=0.03509261831641197\n",
      "epoch 97 iter 39 loss=0.05933065712451935\n",
      "epoch 97 iter 40 loss=0.06158694997429848\n",
      "epoch 97 iter 41 loss=0.04219772294163704\n",
      "epoch 97 iter 42 loss=0.06681019067764282\n",
      "epoch 97 iter 43 loss=0.05514828860759735\n",
      "epoch 97 iter 44 loss=0.08736696094274521\n",
      "epoch 97 iter 45 loss=0.07848207652568817\n",
      "epoch 97 iter 46 loss=0.065071702003479\n",
      "epoch 97 iter 47 loss=0.059074439108371735\n",
      "epoch 97 iter 48 loss=0.036202799528837204\n",
      "epoch 97 iter 49 loss=0.05989410728216171\n",
      "epoch 97 iter 50 loss=0.058633312582969666\n",
      "epoch 97 iter 51 loss=0.06234663724899292\n",
      "epoch 97 iter 52 loss=0.03820493444800377\n",
      "epoch 97 iter 53 loss=0.06525665521621704\n",
      "epoch 97 iter 54 loss=0.055867377668619156\n",
      "epoch 97 iter 55 loss=0.04563494026660919\n",
      "epoch 97 iter 56 loss=0.04639957845211029\n",
      "epoch 97 iter 57 loss=0.03837234154343605\n",
      "epoch 97 iter 58 loss=0.08345770835876465\n",
      "epoch 97 iter 59 loss=0.06827817112207413\n",
      "epoch 97 iter 60 loss=0.08776254206895828\n",
      "epoch 97 iter 61 loss=0.04745511710643768\n",
      "epoch 97 iter 62 loss=0.03654339164495468\n",
      "epoch 97 iter 63 loss=0.03993552550673485\n",
      "epoch 97 iter 64 loss=0.05415961146354675\n",
      "epoch 97 iter 65 loss=0.04002855345606804\n",
      "epoch 97 iter 66 loss=0.06043344363570213\n",
      "epoch 97 iter 67 loss=0.053328484296798706\n",
      "epoch 97 iter 68 loss=0.06195028871297836\n",
      "epoch 97 iter 69 loss=0.0691758394241333\n",
      "epoch 97 iter 70 loss=0.044457998126745224\n",
      "epoch 97 iter 71 loss=0.034387290477752686\n",
      "epoch 97 iter 72 loss=0.05364495888352394\n",
      "epoch 97 iter 73 loss=0.05428590252995491\n",
      "epoch 97 iter 74 loss=0.058914270251989365\n",
      "epoch 98 iter 0 loss=0.05457330122590065\n",
      "epoch 98 iter 1 loss=0.0449649840593338\n",
      "epoch 98 iter 2 loss=0.049242228269577026\n",
      "epoch 98 iter 3 loss=0.07815030962228775\n",
      "epoch 98 iter 4 loss=0.04994644969701767\n",
      "epoch 98 iter 5 loss=0.06053071469068527\n",
      "epoch 98 iter 6 loss=0.04821207746863365\n",
      "epoch 98 iter 7 loss=0.054686758667230606\n",
      "epoch 98 iter 8 loss=0.037543945014476776\n",
      "epoch 98 iter 9 loss=0.03826721012592316\n",
      "epoch 98 iter 10 loss=0.06648121029138565\n",
      "epoch 98 iter 11 loss=0.045233000069856644\n",
      "epoch 98 iter 12 loss=0.0590280219912529\n",
      "epoch 98 iter 13 loss=0.04942859709262848\n",
      "epoch 98 iter 14 loss=0.028734562918543816\n",
      "epoch 98 iter 15 loss=0.07309303432703018\n",
      "epoch 98 iter 16 loss=0.04684004932641983\n",
      "epoch 98 iter 17 loss=0.0678945928812027\n",
      "epoch 98 iter 18 loss=0.044742364436388016\n",
      "epoch 98 iter 19 loss=0.0759364441037178\n",
      "epoch 98 iter 20 loss=0.06111593544483185\n",
      "epoch 98 iter 21 loss=0.0967382937669754\n",
      "epoch 98 iter 22 loss=0.05882028862833977\n",
      "epoch 98 iter 23 loss=0.05177201330661774\n",
      "epoch 98 iter 24 loss=0.048975929617881775\n",
      "epoch 98 iter 25 loss=0.044948603957891464\n",
      "epoch 98 iter 26 loss=0.03974214568734169\n",
      "epoch 98 iter 27 loss=0.04855186492204666\n",
      "epoch 98 iter 28 loss=0.05594860017299652\n",
      "epoch 98 iter 29 loss=0.05818474292755127\n",
      "epoch 98 iter 30 loss=0.05350878834724426\n",
      "epoch 98 iter 31 loss=0.06624989956617355\n",
      "epoch 98 iter 32 loss=0.07983183115720749\n",
      "epoch 98 iter 33 loss=0.08671150356531143\n",
      "epoch 98 iter 34 loss=0.034070469439029694\n",
      "epoch 98 iter 35 loss=0.05140436813235283\n",
      "epoch 98 iter 36 loss=0.0445207878947258\n",
      "epoch 98 iter 37 loss=0.07864245772361755\n",
      "epoch 98 iter 38 loss=0.03831316530704498\n",
      "epoch 98 iter 39 loss=0.051322389394044876\n",
      "epoch 98 iter 40 loss=0.05068052560091019\n",
      "epoch 98 iter 41 loss=0.060713279992341995\n",
      "epoch 98 iter 42 loss=0.04202266037464142\n",
      "epoch 98 iter 43 loss=0.05595726892352104\n",
      "epoch 98 iter 44 loss=0.02934378944337368\n",
      "epoch 98 iter 45 loss=0.04564925655722618\n",
      "epoch 98 iter 46 loss=0.05476175993680954\n",
      "epoch 98 iter 47 loss=0.07344098389148712\n",
      "epoch 98 iter 48 loss=0.04840691760182381\n",
      "epoch 98 iter 49 loss=0.054845377802848816\n",
      "epoch 98 iter 50 loss=0.06558741629123688\n",
      "epoch 98 iter 51 loss=0.08579587936401367\n",
      "epoch 98 iter 52 loss=0.037001833319664\n",
      "epoch 98 iter 53 loss=0.032842982560396194\n",
      "epoch 98 iter 54 loss=0.04656052589416504\n",
      "epoch 98 iter 55 loss=0.04535411298274994\n",
      "epoch 98 iter 56 loss=0.07524550706148148\n",
      "epoch 98 iter 57 loss=0.05748341977596283\n",
      "epoch 98 iter 58 loss=0.06684844940900803\n",
      "epoch 98 iter 59 loss=0.057471103966236115\n",
      "epoch 98 iter 60 loss=0.06603877991437912\n",
      "epoch 98 iter 61 loss=0.04417392611503601\n",
      "epoch 98 iter 62 loss=0.0462137833237648\n",
      "epoch 98 iter 63 loss=0.06394492089748383\n",
      "epoch 98 iter 64 loss=0.058473266661167145\n",
      "epoch 98 iter 65 loss=0.028125280514359474\n",
      "epoch 98 iter 66 loss=0.07004944235086441\n",
      "epoch 98 iter 67 loss=0.08671160042285919\n",
      "epoch 98 iter 68 loss=0.06604895740747452\n",
      "epoch 98 iter 69 loss=0.07619990408420563\n",
      "epoch 98 iter 70 loss=0.07008349150419235\n",
      "epoch 98 iter 71 loss=0.06158529967069626\n",
      "epoch 98 iter 72 loss=0.05878416821360588\n",
      "epoch 98 iter 73 loss=0.04196295514702797\n",
      "epoch 98 iter 74 loss=0.07231120020151138\n",
      "epoch 99 iter 0 loss=0.04819335788488388\n",
      "epoch 99 iter 1 loss=0.07385692745447159\n",
      "epoch 99 iter 2 loss=0.06088520586490631\n",
      "epoch 99 iter 3 loss=0.04119667410850525\n",
      "epoch 99 iter 4 loss=0.03314144164323807\n",
      "epoch 99 iter 5 loss=0.0534466989338398\n",
      "epoch 99 iter 6 loss=0.052653949707746506\n",
      "epoch 99 iter 7 loss=0.060520630329847336\n",
      "epoch 99 iter 8 loss=0.05710265412926674\n",
      "epoch 99 iter 9 loss=0.05741691216826439\n",
      "epoch 99 iter 10 loss=0.06379982829093933\n",
      "epoch 99 iter 11 loss=0.06221473962068558\n",
      "epoch 99 iter 12 loss=0.04197275638580322\n",
      "epoch 99 iter 13 loss=0.04428141936659813\n",
      "epoch 99 iter 14 loss=0.058249786496162415\n",
      "epoch 99 iter 15 loss=0.05772888660430908\n",
      "epoch 99 iter 16 loss=0.06414593756198883\n",
      "epoch 99 iter 17 loss=0.04855072870850563\n",
      "epoch 99 iter 18 loss=0.0580286867916584\n",
      "epoch 99 iter 19 loss=0.04981129616498947\n",
      "epoch 99 iter 20 loss=0.04361513629555702\n",
      "epoch 99 iter 21 loss=0.044467583298683167\n",
      "epoch 99 iter 22 loss=0.05532946437597275\n",
      "epoch 99 iter 23 loss=0.06516049057245255\n",
      "epoch 99 iter 24 loss=0.05499343201518059\n",
      "epoch 99 iter 25 loss=0.04786069318652153\n",
      "epoch 99 iter 26 loss=0.035614125430583954\n",
      "epoch 99 iter 27 loss=0.04966374859213829\n",
      "epoch 99 iter 28 loss=0.05622280389070511\n",
      "epoch 99 iter 29 loss=0.04721066355705261\n",
      "epoch 99 iter 30 loss=0.0585193932056427\n",
      "epoch 99 iter 31 loss=0.037736814469099045\n",
      "epoch 99 iter 32 loss=0.04420401528477669\n",
      "epoch 99 iter 33 loss=0.06920327246189117\n",
      "epoch 99 iter 34 loss=0.05775340646505356\n",
      "epoch 99 iter 35 loss=0.06171538308262825\n",
      "epoch 99 iter 36 loss=0.0581282302737236\n",
      "epoch 99 iter 37 loss=0.07650989294052124\n",
      "epoch 99 iter 38 loss=0.057726576924324036\n",
      "epoch 99 iter 39 loss=0.05032975226640701\n",
      "epoch 99 iter 40 loss=0.06300060451030731\n",
      "epoch 99 iter 41 loss=0.04810352623462677\n",
      "epoch 99 iter 42 loss=0.05409080162644386\n",
      "epoch 99 iter 43 loss=0.030862979590892792\n",
      "epoch 99 iter 44 loss=0.04056304693222046\n",
      "epoch 99 iter 45 loss=0.04710127413272858\n",
      "epoch 99 iter 46 loss=0.03682825714349747\n",
      "epoch 99 iter 47 loss=0.04057272896170616\n",
      "epoch 99 iter 48 loss=0.039686158299446106\n",
      "epoch 99 iter 49 loss=0.07009579241275787\n",
      "epoch 99 iter 50 loss=0.05247519537806511\n",
      "epoch 99 iter 51 loss=0.06130140274763107\n",
      "epoch 99 iter 52 loss=0.04919002950191498\n",
      "epoch 99 iter 53 loss=0.06129450350999832\n",
      "epoch 99 iter 54 loss=0.03330868110060692\n",
      "epoch 99 iter 55 loss=0.07949838042259216\n",
      "epoch 99 iter 56 loss=0.049760304391384125\n",
      "epoch 99 iter 57 loss=0.047967854887247086\n",
      "epoch 99 iter 58 loss=0.05411631241440773\n",
      "epoch 99 iter 59 loss=0.05478914454579353\n",
      "epoch 99 iter 60 loss=0.08005533367395401\n",
      "epoch 99 iter 61 loss=0.05037904530763626\n",
      "epoch 99 iter 62 loss=0.0693778246641159\n",
      "epoch 99 iter 63 loss=0.058465901762247086\n",
      "epoch 99 iter 64 loss=0.06014525145292282\n",
      "epoch 99 iter 65 loss=0.07546362280845642\n",
      "epoch 99 iter 66 loss=0.05805422365665436\n",
      "epoch 99 iter 67 loss=0.05620155110955238\n",
      "epoch 99 iter 68 loss=0.026805300265550613\n",
      "epoch 99 iter 69 loss=0.049151401966810226\n",
      "epoch 99 iter 70 loss=0.04002288356423378\n",
      "epoch 99 iter 71 loss=0.07535120099782944\n",
      "epoch 99 iter 72 loss=0.053213853389024734\n",
      "epoch 99 iter 73 loss=0.07945099472999573\n",
      "epoch 99 iter 74 loss=0.04629872366786003\n",
      "epoch 100 iter 0 loss=0.04809248819947243\n",
      "epoch 100 iter 1 loss=0.042659807950258255\n",
      "epoch 100 iter 2 loss=0.05193782225251198\n",
      "epoch 100 iter 3 loss=0.05848730728030205\n",
      "epoch 100 iter 4 loss=0.03383105993270874\n",
      "epoch 100 iter 5 loss=0.06925405561923981\n",
      "epoch 100 iter 6 loss=0.04037286713719368\n",
      "epoch 100 iter 7 loss=0.031205996870994568\n",
      "epoch 100 iter 8 loss=0.04048191383481026\n",
      "epoch 100 iter 9 loss=0.0675741583108902\n",
      "epoch 100 iter 10 loss=0.023990992456674576\n",
      "epoch 100 iter 11 loss=0.03996900096535683\n",
      "epoch 100 iter 12 loss=0.04082917794585228\n",
      "epoch 100 iter 13 loss=0.0573669858276844\n",
      "epoch 100 iter 14 loss=0.04241221770644188\n",
      "epoch 100 iter 15 loss=0.047509245574474335\n",
      "epoch 100 iter 16 loss=0.042098671197891235\n",
      "epoch 100 iter 17 loss=0.06363940238952637\n",
      "epoch 100 iter 18 loss=0.059693582355976105\n",
      "epoch 100 iter 19 loss=0.0637568011879921\n",
      "epoch 100 iter 20 loss=0.07189792394638062\n",
      "epoch 100 iter 21 loss=0.04920302703976631\n",
      "epoch 100 iter 22 loss=0.07831191271543503\n",
      "epoch 100 iter 23 loss=0.04585082828998566\n",
      "epoch 100 iter 24 loss=0.04395943507552147\n",
      "epoch 100 iter 25 loss=0.055238429456949234\n",
      "epoch 100 iter 26 loss=0.05923473462462425\n",
      "epoch 100 iter 27 loss=0.07145562767982483\n",
      "epoch 100 iter 28 loss=0.04527886584401131\n",
      "epoch 100 iter 29 loss=0.0436827726662159\n",
      "epoch 100 iter 30 loss=0.04054376482963562\n",
      "epoch 100 iter 31 loss=0.043501853942871094\n",
      "epoch 100 iter 32 loss=0.0540461502969265\n",
      "epoch 100 iter 33 loss=0.04494526982307434\n",
      "epoch 100 iter 34 loss=0.054276589304208755\n",
      "epoch 100 iter 35 loss=0.05753394216299057\n",
      "epoch 100 iter 36 loss=0.035117365419864655\n",
      "epoch 100 iter 37 loss=0.0313432402908802\n",
      "epoch 100 iter 38 loss=0.043938249349594116\n",
      "epoch 100 iter 39 loss=0.046855028718709946\n",
      "epoch 100 iter 40 loss=0.033723101019859314\n",
      "epoch 100 iter 41 loss=0.054744087159633636\n",
      "epoch 100 iter 42 loss=0.04948688670992851\n",
      "epoch 100 iter 43 loss=0.05107027664780617\n",
      "epoch 100 iter 44 loss=0.055548232048749924\n",
      "epoch 100 iter 45 loss=0.041530951857566833\n",
      "epoch 100 iter 46 loss=0.06536837667226791\n",
      "epoch 100 iter 47 loss=0.04562056064605713\n",
      "epoch 100 iter 48 loss=0.06769956648349762\n",
      "epoch 100 iter 49 loss=0.0879073217511177\n",
      "epoch 100 iter 50 loss=0.09101608395576477\n",
      "epoch 100 iter 51 loss=0.09461261332035065\n",
      "epoch 100 iter 52 loss=0.06850684434175491\n",
      "epoch 100 iter 53 loss=0.0808042660355568\n",
      "epoch 100 iter 54 loss=0.06373041123151779\n",
      "epoch 100 iter 55 loss=0.06715086847543716\n",
      "epoch 100 iter 56 loss=0.062425319105386734\n",
      "epoch 100 iter 57 loss=0.045162901282310486\n",
      "epoch 100 iter 58 loss=0.06507938355207443\n",
      "epoch 100 iter 59 loss=0.07467281073331833\n",
      "epoch 100 iter 60 loss=0.060811206698417664\n",
      "epoch 100 iter 61 loss=0.04147886112332344\n",
      "epoch 100 iter 62 loss=0.05764321982860565\n",
      "epoch 100 iter 63 loss=0.08177551627159119\n",
      "epoch 100 iter 64 loss=0.047779712826013565\n",
      "epoch 100 iter 65 loss=0.06083521619439125\n",
      "epoch 100 iter 66 loss=0.07845732569694519\n",
      "epoch 100 iter 67 loss=0.05611756816506386\n",
      "epoch 100 iter 68 loss=0.076405368745327\n",
      "epoch 100 iter 69 loss=0.060061439871788025\n",
      "epoch 100 iter 70 loss=0.0373980775475502\n",
      "epoch 100 iter 71 loss=0.052094731479883194\n",
      "epoch 100 iter 72 loss=0.0517251119017601\n",
      "epoch 100 iter 73 loss=0.05690461024641991\n",
      "epoch 100 iter 74 loss=0.03575398027896881\n",
      "epoch 101 iter 0 loss=0.07729391753673553\n",
      "epoch 101 iter 1 loss=0.05558430030941963\n",
      "epoch 101 iter 2 loss=0.04704858735203743\n",
      "epoch 101 iter 3 loss=0.052531346678733826\n",
      "epoch 101 iter 4 loss=0.08015424758195877\n",
      "epoch 101 iter 5 loss=0.05063669756054878\n",
      "epoch 101 iter 6 loss=0.052555494010448456\n",
      "epoch 101 iter 7 loss=0.05371210351586342\n",
      "epoch 101 iter 8 loss=0.027325395494699478\n",
      "epoch 101 iter 9 loss=0.052958521991968155\n",
      "epoch 101 iter 10 loss=0.07667538523674011\n",
      "epoch 101 iter 11 loss=0.04543815925717354\n",
      "epoch 101 iter 12 loss=0.05087543651461601\n",
      "epoch 101 iter 13 loss=0.0448920875787735\n",
      "epoch 101 iter 14 loss=0.03288285806775093\n",
      "epoch 101 iter 15 loss=0.0774068534374237\n",
      "epoch 101 iter 16 loss=0.043878912925720215\n",
      "epoch 101 iter 17 loss=0.044290341436862946\n",
      "epoch 101 iter 18 loss=0.06904356926679611\n",
      "epoch 101 iter 19 loss=0.04343482851982117\n",
      "epoch 101 iter 20 loss=0.05161804333329201\n",
      "epoch 101 iter 21 loss=0.07665442675352097\n",
      "epoch 101 iter 22 loss=0.032695379108190536\n",
      "epoch 101 iter 23 loss=0.059117626398801804\n",
      "epoch 101 iter 24 loss=0.054422829300165176\n",
      "epoch 101 iter 25 loss=0.0458928719162941\n",
      "epoch 101 iter 26 loss=0.050379443913698196\n",
      "epoch 101 iter 27 loss=0.05695980787277222\n",
      "epoch 101 iter 28 loss=0.091762013733387\n",
      "epoch 101 iter 29 loss=0.06372934579849243\n",
      "epoch 101 iter 30 loss=0.04937828332185745\n",
      "epoch 101 iter 31 loss=0.05515625327825546\n",
      "epoch 101 iter 32 loss=0.06067752465605736\n",
      "epoch 101 iter 33 loss=0.06915983557701111\n",
      "epoch 101 iter 34 loss=0.10152456164360046\n",
      "epoch 101 iter 35 loss=0.027061020955443382\n",
      "epoch 101 iter 36 loss=0.07147733122110367\n",
      "epoch 101 iter 37 loss=0.04611673578619957\n",
      "epoch 101 iter 38 loss=0.057694800198078156\n",
      "epoch 101 iter 39 loss=0.0570821650326252\n",
      "epoch 101 iter 40 loss=0.039575763046741486\n",
      "epoch 101 iter 41 loss=0.04537218064069748\n",
      "epoch 101 iter 42 loss=0.03791247680783272\n",
      "epoch 101 iter 43 loss=0.04768725112080574\n",
      "epoch 101 iter 44 loss=0.0394841767847538\n",
      "epoch 101 iter 45 loss=0.08307245373725891\n",
      "epoch 101 iter 46 loss=0.03961651772260666\n",
      "epoch 101 iter 47 loss=0.06571600586175919\n",
      "epoch 101 iter 48 loss=0.05563320964574814\n",
      "epoch 101 iter 49 loss=0.032010432332754135\n",
      "epoch 101 iter 50 loss=0.0650540217757225\n",
      "epoch 101 iter 51 loss=0.05530180409550667\n",
      "epoch 101 iter 52 loss=0.05202905461192131\n",
      "epoch 101 iter 53 loss=0.06106279045343399\n",
      "epoch 101 iter 54 loss=0.04910333827137947\n",
      "epoch 101 iter 55 loss=0.08263815194368362\n",
      "epoch 101 iter 56 loss=0.04959286004304886\n",
      "epoch 101 iter 57 loss=0.06119142845273018\n",
      "epoch 101 iter 58 loss=0.035987723618745804\n",
      "epoch 101 iter 59 loss=0.03809038922190666\n",
      "epoch 101 iter 60 loss=0.07179225236177444\n",
      "epoch 101 iter 61 loss=0.06081398203969002\n",
      "epoch 101 iter 62 loss=0.05607275664806366\n",
      "epoch 101 iter 63 loss=0.06173966825008392\n",
      "epoch 101 iter 64 loss=0.03935990482568741\n",
      "epoch 101 iter 65 loss=0.07980360090732574\n",
      "epoch 101 iter 66 loss=0.06593416631221771\n",
      "epoch 101 iter 67 loss=0.07120548188686371\n",
      "epoch 101 iter 68 loss=0.05988600477576256\n",
      "epoch 101 iter 69 loss=0.031885404139757156\n",
      "epoch 101 iter 70 loss=0.023776959627866745\n",
      "epoch 101 iter 71 loss=0.0754457637667656\n",
      "epoch 101 iter 72 loss=0.06027934327721596\n",
      "epoch 101 iter 73 loss=0.07985521852970123\n",
      "epoch 101 iter 74 loss=0.0263349711894989\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3310.\n",
      "epoch 102 iter 0 loss=0.05175977200269699\n",
      "epoch 102 iter 1 loss=0.025422144681215286\n",
      "epoch 102 iter 2 loss=0.06484075635671616\n",
      "epoch 102 iter 3 loss=0.07225441187620163\n",
      "epoch 102 iter 4 loss=0.07222019135951996\n",
      "epoch 102 iter 5 loss=0.04897766187787056\n",
      "epoch 102 iter 6 loss=0.04746825620532036\n",
      "epoch 102 iter 7 loss=0.058952696621418\n",
      "epoch 102 iter 8 loss=0.06399224698543549\n",
      "epoch 102 iter 9 loss=0.023155895993113518\n",
      "epoch 102 iter 10 loss=0.03715032711625099\n",
      "epoch 102 iter 11 loss=0.052054259926080704\n",
      "epoch 102 iter 12 loss=0.06353049725294113\n",
      "epoch 102 iter 13 loss=0.06831014901399612\n",
      "epoch 102 iter 14 loss=0.03141629323363304\n",
      "epoch 102 iter 15 loss=0.06753166019916534\n",
      "epoch 102 iter 16 loss=0.04818990081548691\n",
      "epoch 102 iter 17 loss=0.046239130198955536\n",
      "epoch 102 iter 18 loss=0.047242362052202225\n",
      "epoch 102 iter 19 loss=0.05713927745819092\n",
      "epoch 102 iter 20 loss=0.053952403366565704\n",
      "epoch 102 iter 21 loss=0.05432568117976189\n",
      "epoch 102 iter 22 loss=0.04524858668446541\n",
      "epoch 102 iter 23 loss=0.054283205419778824\n",
      "epoch 102 iter 24 loss=0.05578075349330902\n",
      "epoch 102 iter 25 loss=0.056253716349601746\n",
      "epoch 102 iter 26 loss=0.07900314033031464\n",
      "epoch 102 iter 27 loss=0.05296315625309944\n",
      "epoch 102 iter 28 loss=0.045428380370140076\n",
      "epoch 102 iter 29 loss=0.060616303235292435\n",
      "epoch 102 iter 30 loss=0.06409616023302078\n",
      "epoch 102 iter 31 loss=0.050850655883550644\n",
      "epoch 102 iter 32 loss=0.05165819823741913\n",
      "epoch 102 iter 33 loss=0.062315020710229874\n",
      "epoch 102 iter 34 loss=0.04719076305627823\n",
      "epoch 102 iter 35 loss=0.028369013220071793\n",
      "epoch 102 iter 36 loss=0.038763053715229034\n",
      "epoch 102 iter 37 loss=0.050440866500139236\n",
      "epoch 102 iter 38 loss=0.07567404955625534\n",
      "epoch 102 iter 39 loss=0.04311528429389\n",
      "epoch 102 iter 40 loss=0.03545351326465607\n",
      "epoch 102 iter 41 loss=0.06532318145036697\n",
      "epoch 102 iter 42 loss=0.05558265000581741\n",
      "epoch 102 iter 43 loss=0.06717155873775482\n",
      "epoch 102 iter 44 loss=0.05777791887521744\n",
      "epoch 102 iter 45 loss=0.033030614256858826\n",
      "epoch 102 iter 46 loss=0.08454076200723648\n",
      "epoch 102 iter 47 loss=0.05023409053683281\n",
      "epoch 102 iter 48 loss=0.04887170344591141\n",
      "epoch 102 iter 49 loss=0.04066567122936249\n",
      "epoch 102 iter 50 loss=0.05876456946134567\n",
      "epoch 102 iter 51 loss=0.05755310505628586\n",
      "epoch 102 iter 52 loss=0.04235142096877098\n",
      "epoch 102 iter 53 loss=0.056720588356256485\n",
      "epoch 102 iter 54 loss=0.04845844581723213\n",
      "epoch 102 iter 55 loss=0.08138741552829742\n",
      "epoch 102 iter 56 loss=0.055582351982593536\n",
      "epoch 102 iter 57 loss=0.04593366011977196\n",
      "epoch 102 iter 58 loss=0.04922553151845932\n",
      "epoch 102 iter 59 loss=0.05973437801003456\n",
      "epoch 102 iter 60 loss=0.041776079684495926\n",
      "epoch 102 iter 61 loss=0.07754844427108765\n",
      "epoch 102 iter 62 loss=0.06943289190530777\n",
      "epoch 102 iter 63 loss=0.025052301585674286\n",
      "epoch 102 iter 64 loss=0.05667205899953842\n",
      "epoch 102 iter 65 loss=0.07912671566009521\n",
      "epoch 102 iter 66 loss=0.08933327347040176\n",
      "epoch 102 iter 67 loss=0.037889670580625534\n",
      "epoch 102 iter 68 loss=0.04080711305141449\n",
      "epoch 102 iter 69 loss=0.06445439904928207\n",
      "epoch 102 iter 70 loss=0.040134817361831665\n",
      "epoch 102 iter 71 loss=0.045806948095560074\n",
      "epoch 102 iter 72 loss=0.05070733278989792\n",
      "epoch 102 iter 73 loss=0.05537015199661255\n",
      "epoch 102 iter 74 loss=0.0522068552672863\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3260.\n",
      "epoch 103 iter 0 loss=0.049645837396383286\n",
      "epoch 103 iter 1 loss=0.0645749494433403\n",
      "epoch 103 iter 2 loss=0.04252730309963226\n",
      "epoch 103 iter 3 loss=0.046611540019512177\n",
      "epoch 103 iter 4 loss=0.04229508712887764\n",
      "epoch 103 iter 5 loss=0.0536440834403038\n",
      "epoch 103 iter 6 loss=0.08083613216876984\n",
      "epoch 103 iter 7 loss=0.0652165338397026\n",
      "epoch 103 iter 8 loss=0.04196390509605408\n",
      "epoch 103 iter 9 loss=0.07362565398216248\n",
      "epoch 103 iter 10 loss=0.057583797723054886\n",
      "epoch 103 iter 11 loss=0.06526302546262741\n",
      "epoch 103 iter 12 loss=0.07497695088386536\n",
      "epoch 103 iter 13 loss=0.04503627493977547\n",
      "epoch 103 iter 14 loss=0.06256505101919174\n",
      "epoch 103 iter 15 loss=0.0535929910838604\n",
      "epoch 103 iter 16 loss=0.058829165995121\n",
      "epoch 103 iter 17 loss=0.04114873334765434\n",
      "epoch 103 iter 18 loss=0.052381958812475204\n",
      "epoch 103 iter 19 loss=0.039009321480989456\n",
      "epoch 103 iter 20 loss=0.041939858347177505\n",
      "epoch 103 iter 21 loss=0.0548933707177639\n",
      "epoch 103 iter 22 loss=0.07448366284370422\n",
      "epoch 103 iter 23 loss=0.039374347776174545\n",
      "epoch 103 iter 24 loss=0.06753852963447571\n",
      "epoch 103 iter 25 loss=0.03533760458230972\n",
      "epoch 103 iter 26 loss=0.06968371570110321\n",
      "epoch 103 iter 27 loss=0.03797361999750137\n",
      "epoch 103 iter 28 loss=0.05177902430295944\n",
      "epoch 103 iter 29 loss=0.031776510179042816\n",
      "epoch 103 iter 30 loss=0.041675303131341934\n",
      "epoch 103 iter 31 loss=0.04494418203830719\n",
      "epoch 103 iter 32 loss=0.02777947671711445\n",
      "epoch 103 iter 33 loss=0.06852742284536362\n",
      "epoch 103 iter 34 loss=0.061083368957042694\n",
      "epoch 103 iter 35 loss=0.04737419635057449\n",
      "epoch 103 iter 36 loss=0.036018311977386475\n",
      "epoch 103 iter 37 loss=0.09411587566137314\n",
      "epoch 103 iter 38 loss=0.06984611600637436\n",
      "epoch 103 iter 39 loss=0.07244764268398285\n",
      "epoch 103 iter 40 loss=0.03561696037650108\n",
      "epoch 103 iter 41 loss=0.0518401637673378\n",
      "epoch 103 iter 42 loss=0.05923614650964737\n",
      "epoch 103 iter 43 loss=0.062136460095644\n",
      "epoch 103 iter 44 loss=0.04651908949017525\n",
      "epoch 103 iter 45 loss=0.04677555710077286\n",
      "epoch 103 iter 46 loss=0.04786870628595352\n",
      "epoch 103 iter 47 loss=0.05707338824868202\n",
      "epoch 103 iter 48 loss=0.053711384534835815\n",
      "epoch 103 iter 49 loss=0.06883705407381058\n",
      "epoch 103 iter 50 loss=0.0328257754445076\n",
      "epoch 103 iter 51 loss=0.036096084862947464\n",
      "epoch 103 iter 52 loss=0.050183024257421494\n",
      "epoch 103 iter 53 loss=0.05395285412669182\n",
      "epoch 103 iter 54 loss=0.048420488834381104\n",
      "epoch 103 iter 55 loss=0.04471730440855026\n",
      "epoch 103 iter 56 loss=0.04236351698637009\n",
      "epoch 103 iter 57 loss=0.06193118914961815\n",
      "epoch 103 iter 58 loss=0.03617720305919647\n",
      "epoch 103 iter 59 loss=0.03359077125787735\n",
      "epoch 103 iter 60 loss=0.06929674744606018\n",
      "epoch 103 iter 61 loss=0.06447263807058334\n",
      "epoch 103 iter 62 loss=0.049339912831783295\n",
      "epoch 103 iter 63 loss=0.030182605609297752\n",
      "epoch 103 iter 64 loss=0.06045699119567871\n",
      "epoch 103 iter 65 loss=0.06614001840353012\n",
      "epoch 103 iter 66 loss=0.04460342228412628\n",
      "epoch 103 iter 67 loss=0.04866430535912514\n",
      "epoch 103 iter 68 loss=0.06461905688047409\n",
      "epoch 103 iter 69 loss=0.04781518876552582\n",
      "epoch 103 iter 70 loss=0.04840192198753357\n",
      "epoch 103 iter 71 loss=0.08679521083831787\n",
      "epoch 103 iter 72 loss=0.024028902873396873\n",
      "epoch 103 iter 73 loss=0.07402241230010986\n",
      "epoch 103 iter 74 loss=0.05305071547627449\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3242.\n",
      "epoch 104 iter 0 loss=0.07748478651046753\n",
      "epoch 104 iter 1 loss=0.06676537543535233\n",
      "epoch 104 iter 2 loss=0.062301602214574814\n",
      "epoch 104 iter 3 loss=0.059547245502471924\n",
      "epoch 104 iter 4 loss=0.04688809812068939\n",
      "epoch 104 iter 5 loss=0.045963872224092484\n",
      "epoch 104 iter 6 loss=0.04824654757976532\n",
      "epoch 104 iter 7 loss=0.05387667194008827\n",
      "epoch 104 iter 8 loss=0.08852352946996689\n",
      "epoch 104 iter 9 loss=0.03630625456571579\n",
      "epoch 104 iter 10 loss=0.054904755204916\n",
      "epoch 104 iter 11 loss=0.05814559385180473\n",
      "epoch 104 iter 12 loss=0.03493542969226837\n",
      "epoch 104 iter 13 loss=0.04646293818950653\n",
      "epoch 104 iter 14 loss=0.08575569093227386\n",
      "epoch 104 iter 15 loss=0.053170401602983475\n",
      "epoch 104 iter 16 loss=0.06272047013044357\n",
      "epoch 104 iter 17 loss=0.0335119254887104\n",
      "epoch 104 iter 18 loss=0.042773425579071045\n",
      "epoch 104 iter 19 loss=0.0718650072813034\n",
      "epoch 104 iter 20 loss=0.07130992412567139\n",
      "epoch 104 iter 21 loss=0.04267232120037079\n",
      "epoch 104 iter 22 loss=0.05836436152458191\n",
      "epoch 104 iter 23 loss=0.045983292162418365\n",
      "epoch 104 iter 24 loss=0.03754294663667679\n",
      "epoch 104 iter 25 loss=0.04481101408600807\n",
      "epoch 104 iter 26 loss=0.048531342297792435\n",
      "epoch 104 iter 27 loss=0.03137897700071335\n",
      "epoch 104 iter 28 loss=0.06236208602786064\n",
      "epoch 104 iter 29 loss=0.04576578736305237\n",
      "epoch 104 iter 30 loss=0.05942085012793541\n",
      "epoch 104 iter 31 loss=0.06622321158647537\n",
      "epoch 104 iter 32 loss=0.07418162375688553\n",
      "epoch 104 iter 33 loss=0.048897676169872284\n",
      "epoch 104 iter 34 loss=0.04396587237715721\n",
      "epoch 104 iter 35 loss=0.05398477986454964\n",
      "epoch 104 iter 36 loss=0.03451238200068474\n",
      "epoch 104 iter 37 loss=0.0391971655189991\n",
      "epoch 104 iter 38 loss=0.032460570335388184\n",
      "epoch 104 iter 39 loss=0.04909230023622513\n",
      "epoch 104 iter 40 loss=0.03434330224990845\n",
      "epoch 104 iter 41 loss=0.05416378006339073\n",
      "epoch 104 iter 42 loss=0.038640096783638\n",
      "epoch 104 iter 43 loss=0.04961651563644409\n",
      "epoch 104 iter 44 loss=0.06913749873638153\n",
      "epoch 104 iter 45 loss=0.06715234369039536\n",
      "epoch 104 iter 46 loss=0.029964957386255264\n",
      "epoch 104 iter 47 loss=0.09002774208784103\n",
      "epoch 104 iter 48 loss=0.04713599383831024\n",
      "epoch 104 iter 49 loss=0.027288977056741714\n",
      "epoch 104 iter 50 loss=0.06498534232378006\n",
      "epoch 104 iter 51 loss=0.09678592532873154\n",
      "epoch 104 iter 52 loss=0.05364122614264488\n",
      "epoch 104 iter 53 loss=0.054052334278821945\n",
      "epoch 104 iter 54 loss=0.03287344053387642\n",
      "epoch 104 iter 55 loss=0.049928516149520874\n",
      "epoch 104 iter 56 loss=0.05697271600365639\n",
      "epoch 104 iter 57 loss=0.0341186560690403\n",
      "epoch 104 iter 58 loss=0.052605364471673965\n",
      "epoch 104 iter 59 loss=0.06187823787331581\n",
      "epoch 104 iter 60 loss=0.029571646824479103\n",
      "epoch 104 iter 61 loss=0.0533062107861042\n",
      "epoch 104 iter 62 loss=0.0523323155939579\n",
      "epoch 104 iter 63 loss=0.043526019901037216\n",
      "epoch 104 iter 64 loss=0.03618776798248291\n",
      "epoch 104 iter 65 loss=0.04440676048398018\n",
      "epoch 104 iter 66 loss=0.05847898870706558\n",
      "epoch 104 iter 67 loss=0.0848056823015213\n",
      "epoch 104 iter 68 loss=0.06517932564020157\n",
      "epoch 104 iter 69 loss=0.043027039617300034\n",
      "epoch 104 iter 70 loss=0.08115476369857788\n",
      "epoch 104 iter 71 loss=0.06193149462342262\n",
      "epoch 104 iter 72 loss=0.06631413102149963\n",
      "epoch 104 iter 73 loss=0.045211516320705414\n",
      "epoch 104 iter 74 loss=0.048103272914886475\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3294.\n",
      "epoch 105 iter 0 loss=0.04886511713266373\n",
      "epoch 105 iter 1 loss=0.06588628143072128\n",
      "epoch 105 iter 2 loss=0.05649762973189354\n",
      "epoch 105 iter 3 loss=0.031536512076854706\n",
      "epoch 105 iter 4 loss=0.030352339148521423\n",
      "epoch 105 iter 5 loss=0.03397888317704201\n",
      "epoch 105 iter 6 loss=0.02766258828341961\n",
      "epoch 105 iter 7 loss=0.05709570273756981\n",
      "epoch 105 iter 8 loss=0.05755150690674782\n",
      "epoch 105 iter 9 loss=0.06548647582530975\n",
      "epoch 105 iter 10 loss=0.0781426727771759\n",
      "epoch 105 iter 11 loss=0.05830858275294304\n",
      "epoch 105 iter 12 loss=0.04112992063164711\n",
      "epoch 105 iter 13 loss=0.06835383921861649\n",
      "epoch 105 iter 14 loss=0.05942452698945999\n",
      "epoch 105 iter 15 loss=0.06000486761331558\n",
      "epoch 105 iter 16 loss=0.04798115789890289\n",
      "epoch 105 iter 17 loss=0.057368624955415726\n",
      "epoch 105 iter 18 loss=0.0247983206063509\n",
      "epoch 105 iter 19 loss=0.04620102792978287\n",
      "epoch 105 iter 20 loss=0.05380331352353096\n",
      "epoch 105 iter 21 loss=0.06326987594366074\n",
      "epoch 105 iter 22 loss=0.05322544649243355\n",
      "epoch 105 iter 23 loss=0.05463215708732605\n",
      "epoch 105 iter 24 loss=0.08576448261737823\n",
      "epoch 105 iter 25 loss=0.07367409765720367\n",
      "epoch 105 iter 26 loss=0.06187012419104576\n",
      "epoch 105 iter 27 loss=0.04693194851279259\n",
      "epoch 105 iter 28 loss=0.05378059670329094\n",
      "epoch 105 iter 29 loss=0.05609949678182602\n",
      "epoch 105 iter 30 loss=0.07164283096790314\n",
      "epoch 105 iter 31 loss=0.04891242831945419\n",
      "epoch 105 iter 32 loss=0.03959636017680168\n",
      "epoch 105 iter 33 loss=0.04524120315909386\n",
      "epoch 105 iter 34 loss=0.03889673203229904\n",
      "epoch 105 iter 35 loss=0.05973202362656593\n",
      "epoch 105 iter 36 loss=0.06194555386900902\n",
      "epoch 105 iter 37 loss=0.04230806976556778\n",
      "epoch 105 iter 38 loss=0.06557314842939377\n",
      "epoch 105 iter 39 loss=0.024416133761405945\n",
      "epoch 105 iter 40 loss=0.0717686116695404\n",
      "epoch 105 iter 41 loss=0.0418914258480072\n",
      "epoch 105 iter 42 loss=0.030036604031920433\n",
      "epoch 105 iter 43 loss=0.06429363787174225\n",
      "epoch 105 iter 44 loss=0.06483910232782364\n",
      "epoch 105 iter 45 loss=0.037975333631038666\n",
      "epoch 105 iter 46 loss=0.04263041913509369\n",
      "epoch 105 iter 47 loss=0.0650491788983345\n",
      "epoch 105 iter 48 loss=0.058816928416490555\n",
      "epoch 105 iter 49 loss=0.07886306941509247\n",
      "epoch 105 iter 50 loss=0.05219339206814766\n",
      "epoch 105 iter 51 loss=0.07293063402175903\n",
      "epoch 105 iter 52 loss=0.07036838680505753\n",
      "epoch 105 iter 53 loss=0.05273975431919098\n",
      "epoch 105 iter 54 loss=0.07383571565151215\n",
      "epoch 105 iter 55 loss=0.08126094937324524\n",
      "epoch 105 iter 56 loss=0.05002275109291077\n",
      "epoch 105 iter 57 loss=0.04964665323495865\n",
      "epoch 105 iter 58 loss=0.0613211989402771\n",
      "epoch 105 iter 59 loss=0.046342283487319946\n",
      "epoch 105 iter 60 loss=0.044471073895692825\n",
      "epoch 105 iter 61 loss=0.047190792858600616\n",
      "epoch 105 iter 62 loss=0.0449410118162632\n",
      "epoch 105 iter 63 loss=0.027525151148438454\n",
      "epoch 105 iter 64 loss=0.061275824904441833\n",
      "epoch 105 iter 65 loss=0.04616117849946022\n",
      "epoch 105 iter 66 loss=0.03959682583808899\n",
      "epoch 105 iter 67 loss=0.037990327924489975\n",
      "epoch 105 iter 68 loss=0.050519514828920364\n",
      "epoch 105 iter 69 loss=0.0526704266667366\n",
      "epoch 105 iter 70 loss=0.054618414491415024\n",
      "epoch 105 iter 71 loss=0.05748685076832771\n",
      "epoch 105 iter 72 loss=0.04080735519528389\n",
      "epoch 105 iter 73 loss=0.06906962394714355\n",
      "epoch 105 iter 74 loss=0.0310791227966547\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3238.\n",
      "epoch 106 iter 0 loss=0.06361717730760574\n",
      "epoch 106 iter 1 loss=0.07753457129001617\n",
      "epoch 106 iter 2 loss=0.04351683333516121\n",
      "epoch 106 iter 3 loss=0.03386147320270538\n",
      "epoch 106 iter 4 loss=0.055428046733140945\n",
      "epoch 106 iter 5 loss=0.052864883095026016\n",
      "epoch 106 iter 6 loss=0.05440570414066315\n",
      "epoch 106 iter 7 loss=0.04351210966706276\n",
      "epoch 106 iter 8 loss=0.04645071551203728\n",
      "epoch 106 iter 9 loss=0.04119893163442612\n",
      "epoch 106 iter 10 loss=0.04084441438317299\n",
      "epoch 106 iter 11 loss=0.04388058930635452\n",
      "epoch 106 iter 12 loss=0.060371577739715576\n",
      "epoch 106 iter 13 loss=0.03363544121384621\n",
      "epoch 106 iter 14 loss=0.048462506383657455\n",
      "epoch 106 iter 15 loss=0.05306401476264\n",
      "epoch 106 iter 16 loss=0.05181891843676567\n",
      "epoch 106 iter 17 loss=0.05423786863684654\n",
      "epoch 106 iter 18 loss=0.047739412635564804\n",
      "epoch 106 iter 19 loss=0.06108243763446808\n",
      "epoch 106 iter 20 loss=0.05394810438156128\n",
      "epoch 106 iter 21 loss=0.054220039397478104\n",
      "epoch 106 iter 22 loss=0.042170796543359756\n",
      "epoch 106 iter 23 loss=0.05438333377242088\n",
      "epoch 106 iter 24 loss=0.046205051243305206\n",
      "epoch 106 iter 25 loss=0.0506710484623909\n",
      "epoch 106 iter 26 loss=0.04192810133099556\n",
      "epoch 106 iter 27 loss=0.05608066916465759\n",
      "epoch 106 iter 28 loss=0.1063588336110115\n",
      "epoch 106 iter 29 loss=0.028293689712882042\n",
      "epoch 106 iter 30 loss=0.04133574292063713\n",
      "epoch 106 iter 31 loss=0.03715476766228676\n",
      "epoch 106 iter 32 loss=0.05018478259444237\n",
      "epoch 106 iter 33 loss=0.06556260585784912\n",
      "epoch 106 iter 34 loss=0.05773726478219032\n",
      "epoch 106 iter 35 loss=0.05662083625793457\n",
      "epoch 106 iter 36 loss=0.04242914170026779\n",
      "epoch 106 iter 37 loss=0.05679234117269516\n",
      "epoch 106 iter 38 loss=0.0627264529466629\n",
      "epoch 106 iter 39 loss=0.04716511443257332\n",
      "epoch 106 iter 40 loss=0.04297268018126488\n",
      "epoch 106 iter 41 loss=0.043445900082588196\n",
      "epoch 106 iter 42 loss=0.03149036318063736\n",
      "epoch 106 iter 43 loss=0.029693108052015305\n",
      "epoch 106 iter 44 loss=0.04805107042193413\n",
      "epoch 106 iter 45 loss=0.07478126883506775\n",
      "epoch 106 iter 46 loss=0.09331820905208588\n",
      "epoch 106 iter 47 loss=0.06427956372499466\n",
      "epoch 106 iter 48 loss=0.05027242377400398\n",
      "epoch 106 iter 49 loss=0.06167677417397499\n",
      "epoch 106 iter 50 loss=0.030713845044374466\n",
      "epoch 106 iter 51 loss=0.07918479293584824\n",
      "epoch 106 iter 52 loss=0.05364484712481499\n",
      "epoch 106 iter 53 loss=0.05192390829324722\n",
      "epoch 106 iter 54 loss=0.03717884048819542\n",
      "epoch 106 iter 55 loss=0.06716977059841156\n",
      "epoch 106 iter 56 loss=0.04391016811132431\n",
      "epoch 106 iter 57 loss=0.06473646312952042\n",
      "epoch 106 iter 58 loss=0.040818918496370316\n",
      "epoch 106 iter 59 loss=0.035017043352127075\n",
      "epoch 106 iter 60 loss=0.06640124320983887\n",
      "epoch 106 iter 61 loss=0.06278883665800095\n",
      "epoch 106 iter 62 loss=0.043208688497543335\n",
      "epoch 106 iter 63 loss=0.06584395468235016\n",
      "epoch 106 iter 64 loss=0.06736589223146439\n",
      "epoch 106 iter 65 loss=0.030025217682123184\n",
      "epoch 106 iter 66 loss=0.08180492371320724\n",
      "epoch 106 iter 67 loss=0.04951218143105507\n",
      "epoch 106 iter 68 loss=0.030886905267834663\n",
      "epoch 106 iter 69 loss=0.04426133260130882\n",
      "epoch 106 iter 70 loss=0.04430617392063141\n",
      "epoch 106 iter 71 loss=0.04394833743572235\n",
      "epoch 106 iter 72 loss=0.049157895147800446\n",
      "epoch 106 iter 73 loss=0.03571990504860878\n",
      "epoch 106 iter 74 loss=0.059213943779468536\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3167.\n",
      "epoch 107 iter 0 loss=0.04366329312324524\n",
      "epoch 107 iter 1 loss=0.07497356832027435\n",
      "epoch 107 iter 2 loss=0.03664516657590866\n",
      "epoch 107 iter 3 loss=0.0770375207066536\n",
      "epoch 107 iter 4 loss=0.05944880470633507\n",
      "epoch 107 iter 5 loss=0.04648079350590706\n",
      "epoch 107 iter 6 loss=0.059204455465078354\n",
      "epoch 107 iter 7 loss=0.06314694881439209\n",
      "epoch 107 iter 8 loss=0.06559021770954132\n",
      "epoch 107 iter 9 loss=0.0427597276866436\n",
      "epoch 107 iter 10 loss=0.04353341460227966\n",
      "epoch 107 iter 11 loss=0.07950641959905624\n",
      "epoch 107 iter 12 loss=0.038273874670267105\n",
      "epoch 107 iter 13 loss=0.0460832454264164\n",
      "epoch 107 iter 14 loss=0.049978919327259064\n",
      "epoch 107 iter 15 loss=0.052885573357343674\n",
      "epoch 107 iter 16 loss=0.023170579224824905\n",
      "epoch 107 iter 17 loss=0.044547345489263535\n",
      "epoch 107 iter 18 loss=0.04347149655222893\n",
      "epoch 107 iter 19 loss=0.05285274237394333\n",
      "epoch 107 iter 20 loss=0.04525401443243027\n",
      "epoch 107 iter 21 loss=0.06296711415052414\n",
      "epoch 107 iter 22 loss=0.06416996568441391\n",
      "epoch 107 iter 23 loss=0.07035787403583527\n",
      "epoch 107 iter 24 loss=0.06830314546823502\n",
      "epoch 107 iter 25 loss=0.03465696796774864\n",
      "epoch 107 iter 26 loss=0.07322131842374802\n",
      "epoch 107 iter 27 loss=0.061219923198223114\n",
      "epoch 107 iter 28 loss=0.04324164241552353\n",
      "epoch 107 iter 29 loss=0.05167640745639801\n",
      "epoch 107 iter 30 loss=0.043143320828676224\n",
      "epoch 107 iter 31 loss=0.035285793244838715\n",
      "epoch 107 iter 32 loss=0.05560079962015152\n",
      "epoch 107 iter 33 loss=0.039993397891521454\n",
      "epoch 107 iter 34 loss=0.045316457748413086\n",
      "epoch 107 iter 35 loss=0.027869030833244324\n",
      "epoch 107 iter 36 loss=0.05960432067513466\n",
      "epoch 107 iter 37 loss=0.039168231189250946\n",
      "epoch 107 iter 38 loss=0.0566023588180542\n",
      "epoch 107 iter 39 loss=0.044216398149728775\n",
      "epoch 107 iter 40 loss=0.05782579630613327\n",
      "epoch 107 iter 41 loss=0.06667758524417877\n",
      "epoch 107 iter 42 loss=0.046751756221055984\n",
      "epoch 107 iter 43 loss=0.05124781280755997\n",
      "epoch 107 iter 44 loss=0.05723622068762779\n",
      "epoch 107 iter 45 loss=0.05939208343625069\n",
      "epoch 107 iter 46 loss=0.03917553275823593\n",
      "epoch 107 iter 47 loss=0.04340384528040886\n",
      "epoch 107 iter 48 loss=0.038310881704092026\n",
      "epoch 107 iter 49 loss=0.03640426695346832\n",
      "epoch 107 iter 50 loss=0.05085940659046173\n",
      "epoch 107 iter 51 loss=0.0865921750664711\n",
      "epoch 107 iter 52 loss=0.06349845230579376\n",
      "epoch 107 iter 53 loss=0.032763633877038956\n",
      "epoch 107 iter 54 loss=0.05573515221476555\n",
      "epoch 107 iter 55 loss=0.06861323118209839\n",
      "epoch 107 iter 56 loss=0.0493970587849617\n",
      "epoch 107 iter 57 loss=0.04005652293562889\n",
      "epoch 107 iter 58 loss=0.030297186225652695\n",
      "epoch 107 iter 59 loss=0.05602024868130684\n",
      "epoch 107 iter 60 loss=0.053342606872320175\n",
      "epoch 107 iter 61 loss=0.07092723995447159\n",
      "epoch 107 iter 62 loss=0.03198752924799919\n",
      "epoch 107 iter 63 loss=0.06868012249469757\n",
      "epoch 107 iter 64 loss=0.03322810307145119\n",
      "epoch 107 iter 65 loss=0.03381071239709854\n",
      "epoch 107 iter 66 loss=0.03815258666872978\n",
      "epoch 107 iter 67 loss=0.05064445361495018\n",
      "epoch 107 iter 68 loss=0.0746338814496994\n",
      "epoch 107 iter 69 loss=0.049282707273960114\n",
      "epoch 107 iter 70 loss=0.04563530907034874\n",
      "epoch 107 iter 71 loss=0.04210229590535164\n",
      "epoch 107 iter 72 loss=0.02792566455900669\n",
      "epoch 107 iter 73 loss=0.09246623516082764\n",
      "epoch 107 iter 74 loss=0.04384295269846916\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3249.\n",
      "epoch 108 iter 0 loss=0.07179836183786392\n",
      "epoch 108 iter 1 loss=0.04852423444390297\n",
      "epoch 108 iter 2 loss=0.03929140046238899\n",
      "epoch 108 iter 3 loss=0.033030685037374496\n",
      "epoch 108 iter 4 loss=0.03668713569641113\n",
      "epoch 108 iter 5 loss=0.05273125320672989\n",
      "epoch 108 iter 6 loss=0.04913291335105896\n",
      "epoch 108 iter 7 loss=0.041948795318603516\n",
      "epoch 108 iter 8 loss=0.06745851039886475\n",
      "epoch 108 iter 9 loss=0.07627187669277191\n",
      "epoch 108 iter 10 loss=0.0301279965788126\n",
      "epoch 108 iter 11 loss=0.06521245837211609\n",
      "epoch 108 iter 12 loss=0.035697661340236664\n",
      "epoch 108 iter 13 loss=0.0600016824901104\n",
      "epoch 108 iter 14 loss=0.05812541022896767\n",
      "epoch 108 iter 15 loss=0.05141955241560936\n",
      "epoch 108 iter 16 loss=0.059587717056274414\n",
      "epoch 108 iter 17 loss=0.05654361844062805\n",
      "epoch 108 iter 18 loss=0.03933080658316612\n",
      "epoch 108 iter 19 loss=0.04503990709781647\n",
      "epoch 108 iter 20 loss=0.03871715068817139\n",
      "epoch 108 iter 21 loss=0.04776017740368843\n",
      "epoch 108 iter 22 loss=0.04890073090791702\n",
      "epoch 108 iter 23 loss=0.02855374664068222\n",
      "epoch 108 iter 24 loss=0.03217910975217819\n",
      "epoch 108 iter 25 loss=0.05964142456650734\n",
      "epoch 108 iter 26 loss=0.0671810731291771\n",
      "epoch 108 iter 27 loss=0.03998346999287605\n",
      "epoch 108 iter 28 loss=0.041885655373334885\n",
      "epoch 108 iter 29 loss=0.04029138758778572\n",
      "epoch 108 iter 30 loss=0.026764079928398132\n",
      "epoch 108 iter 31 loss=0.040647558867931366\n",
      "epoch 108 iter 32 loss=0.053772881627082825\n",
      "epoch 108 iter 33 loss=0.04679455608129501\n",
      "epoch 108 iter 34 loss=0.0412319116294384\n",
      "epoch 108 iter 35 loss=0.034543175250291824\n",
      "epoch 108 iter 36 loss=0.0321316123008728\n",
      "epoch 108 iter 37 loss=0.05704203620553017\n",
      "epoch 108 iter 38 loss=0.03847821056842804\n",
      "epoch 108 iter 39 loss=0.040577929466962814\n",
      "epoch 108 iter 40 loss=0.08052736520767212\n",
      "epoch 108 iter 41 loss=0.04741061478853226\n",
      "epoch 108 iter 42 loss=0.03500204533338547\n",
      "epoch 108 iter 43 loss=0.04188353195786476\n",
      "epoch 108 iter 44 loss=0.07373473048210144\n",
      "epoch 108 iter 45 loss=0.042387135326862335\n",
      "epoch 108 iter 46 loss=0.09219738095998764\n",
      "epoch 108 iter 47 loss=0.041159942746162415\n",
      "epoch 108 iter 48 loss=0.061677977442741394\n",
      "epoch 108 iter 49 loss=0.05081450939178467\n",
      "epoch 108 iter 50 loss=0.06304430961608887\n",
      "epoch 108 iter 51 loss=0.050293613225221634\n",
      "epoch 108 iter 52 loss=0.04460638388991356\n",
      "epoch 108 iter 53 loss=0.027956780046224594\n",
      "epoch 108 iter 54 loss=0.05587993189692497\n",
      "epoch 108 iter 55 loss=0.04182986915111542\n",
      "epoch 108 iter 56 loss=0.03794608265161514\n",
      "epoch 108 iter 57 loss=0.06178746744990349\n",
      "epoch 108 iter 58 loss=0.04987887293100357\n",
      "epoch 108 iter 59 loss=0.034682497382164\n",
      "epoch 108 iter 60 loss=0.04845575615763664\n",
      "epoch 108 iter 61 loss=0.05211268737912178\n",
      "epoch 108 iter 62 loss=0.059473272413015366\n",
      "epoch 108 iter 63 loss=0.0628633201122284\n",
      "epoch 108 iter 64 loss=0.05272955819964409\n",
      "epoch 108 iter 65 loss=0.06602486222982407\n",
      "epoch 108 iter 66 loss=0.07321994006633759\n",
      "epoch 108 iter 67 loss=0.06004412844777107\n",
      "epoch 108 iter 68 loss=0.05147835984826088\n",
      "epoch 108 iter 69 loss=0.07038460671901703\n",
      "epoch 108 iter 70 loss=0.04921387508511543\n",
      "epoch 108 iter 71 loss=0.053875572979450226\n",
      "epoch 108 iter 72 loss=0.06734588742256165\n",
      "epoch 108 iter 73 loss=0.06848811358213425\n",
      "epoch 108 iter 74 loss=0.06813444197177887\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3194.\n",
      "epoch 109 iter 0 loss=0.04849777743220329\n",
      "epoch 109 iter 1 loss=0.025989776477217674\n",
      "epoch 109 iter 2 loss=0.0381593219935894\n",
      "epoch 109 iter 3 loss=0.045067720115184784\n",
      "epoch 109 iter 4 loss=0.037884678691625595\n",
      "epoch 109 iter 5 loss=0.031788419932127\n",
      "epoch 109 iter 6 loss=0.05819656327366829\n",
      "epoch 109 iter 7 loss=0.025100044906139374\n",
      "epoch 109 iter 8 loss=0.045577798038721085\n",
      "epoch 109 iter 9 loss=0.041835516691207886\n",
      "epoch 109 iter 10 loss=0.045249972492456436\n",
      "epoch 109 iter 11 loss=0.06708455830812454\n",
      "epoch 109 iter 12 loss=0.035600125789642334\n",
      "epoch 109 iter 13 loss=0.05588109418749809\n",
      "epoch 109 iter 14 loss=0.043178483843803406\n",
      "epoch 109 iter 15 loss=0.0354841984808445\n",
      "epoch 109 iter 16 loss=0.07764934748411179\n",
      "epoch 109 iter 17 loss=0.030728349462151527\n",
      "epoch 109 iter 18 loss=0.0491357259452343\n",
      "epoch 109 iter 19 loss=0.06756015121936798\n",
      "epoch 109 iter 20 loss=0.0426969975233078\n",
      "epoch 109 iter 21 loss=0.041983406990766525\n",
      "epoch 109 iter 22 loss=0.05630974844098091\n",
      "epoch 109 iter 23 loss=0.07313685119152069\n",
      "epoch 109 iter 24 loss=0.03458955138921738\n",
      "epoch 109 iter 25 loss=0.03588586300611496\n",
      "epoch 109 iter 26 loss=0.052440885454416275\n",
      "epoch 109 iter 27 loss=0.04719829931855202\n",
      "epoch 109 iter 28 loss=0.08522724360227585\n",
      "epoch 109 iter 29 loss=0.05044329911470413\n",
      "epoch 109 iter 30 loss=0.04745465889573097\n",
      "epoch 109 iter 31 loss=0.08894188702106476\n",
      "epoch 109 iter 32 loss=0.03992166742682457\n",
      "epoch 109 iter 33 loss=0.048178210854530334\n",
      "epoch 109 iter 34 loss=0.03644566982984543\n",
      "epoch 109 iter 35 loss=0.02971052937209606\n",
      "epoch 109 iter 36 loss=0.050410348922014236\n",
      "epoch 109 iter 37 loss=0.04000100493431091\n",
      "epoch 109 iter 38 loss=0.028244325891137123\n",
      "epoch 109 iter 39 loss=0.04893447831273079\n",
      "epoch 109 iter 40 loss=0.06647074967622757\n",
      "epoch 109 iter 41 loss=0.039122164249420166\n",
      "epoch 109 iter 42 loss=0.04000324010848999\n",
      "epoch 109 iter 43 loss=0.051242366433143616\n",
      "epoch 109 iter 44 loss=0.06422866135835648\n",
      "epoch 109 iter 45 loss=0.05547897517681122\n",
      "epoch 109 iter 46 loss=0.04327685013413429\n",
      "epoch 109 iter 47 loss=0.04125408083200455\n",
      "epoch 109 iter 48 loss=0.03244711831212044\n",
      "epoch 109 iter 49 loss=0.06041474640369415\n",
      "epoch 109 iter 50 loss=0.05605196952819824\n",
      "epoch 109 iter 51 loss=0.06750687211751938\n",
      "epoch 109 iter 52 loss=0.0702575072646141\n",
      "epoch 109 iter 53 loss=0.0561828538775444\n",
      "epoch 109 iter 54 loss=0.050434660166502\n",
      "epoch 109 iter 55 loss=0.0571250393986702\n",
      "epoch 109 iter 56 loss=0.06658833473920822\n",
      "epoch 109 iter 57 loss=0.05904007703065872\n",
      "epoch 109 iter 58 loss=0.05698779225349426\n",
      "epoch 109 iter 59 loss=0.025731969624757767\n",
      "epoch 109 iter 60 loss=0.05295274406671524\n",
      "epoch 109 iter 61 loss=0.05289178341627121\n",
      "epoch 109 iter 62 loss=0.06232235208153725\n",
      "epoch 109 iter 63 loss=0.05427468940615654\n",
      "epoch 109 iter 64 loss=0.06027660891413689\n",
      "epoch 109 iter 65 loss=0.0448034442961216\n",
      "epoch 109 iter 66 loss=0.055005185306072235\n",
      "epoch 109 iter 67 loss=0.06794247776269913\n",
      "epoch 109 iter 68 loss=0.08458998799324036\n",
      "epoch 109 iter 69 loss=0.03552975133061409\n",
      "epoch 109 iter 70 loss=0.026438886299729347\n",
      "epoch 109 iter 71 loss=0.04439343139529228\n",
      "epoch 109 iter 72 loss=0.063849538564682\n",
      "epoch 109 iter 73 loss=0.03814435750246048\n",
      "epoch 109 iter 74 loss=0.04544074833393097\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3264.\n",
      "epoch 110 iter 0 loss=0.029941191896796227\n",
      "epoch 110 iter 1 loss=0.03253309801220894\n",
      "epoch 110 iter 2 loss=0.027487551793456078\n",
      "epoch 110 iter 3 loss=0.03462837263941765\n",
      "epoch 110 iter 4 loss=0.057472389191389084\n",
      "epoch 110 iter 5 loss=0.03130311891436577\n",
      "epoch 110 iter 6 loss=0.03563889116048813\n",
      "epoch 110 iter 7 loss=0.060390982776880264\n",
      "epoch 110 iter 8 loss=0.053805191069841385\n",
      "epoch 110 iter 9 loss=0.052257370203733444\n",
      "epoch 110 iter 10 loss=0.07418996095657349\n",
      "epoch 110 iter 11 loss=0.04485373571515083\n",
      "epoch 110 iter 12 loss=0.03406697139143944\n",
      "epoch 110 iter 13 loss=0.03900257125496864\n",
      "epoch 110 iter 14 loss=0.06217358261346817\n",
      "epoch 110 iter 15 loss=0.07902504503726959\n",
      "epoch 110 iter 16 loss=0.029237721115350723\n",
      "epoch 110 iter 17 loss=0.055454570800065994\n",
      "epoch 110 iter 18 loss=0.07473719120025635\n",
      "epoch 110 iter 19 loss=0.052069634199142456\n",
      "epoch 110 iter 20 loss=0.05155600607395172\n",
      "epoch 110 iter 21 loss=0.05555257573723793\n",
      "epoch 110 iter 22 loss=0.060286104679107666\n",
      "epoch 110 iter 23 loss=0.03693404421210289\n",
      "epoch 110 iter 24 loss=0.03980524092912674\n",
      "epoch 110 iter 25 loss=0.046977464109659195\n",
      "epoch 110 iter 26 loss=0.049306340515613556\n",
      "epoch 110 iter 27 loss=0.06313805282115936\n",
      "epoch 110 iter 28 loss=0.04075474664568901\n",
      "epoch 110 iter 29 loss=0.03806426376104355\n",
      "epoch 110 iter 30 loss=0.04426339641213417\n",
      "epoch 110 iter 31 loss=0.04738308861851692\n",
      "epoch 110 iter 32 loss=0.07125476002693176\n",
      "epoch 110 iter 33 loss=0.02856493927538395\n",
      "epoch 110 iter 34 loss=0.05604516714811325\n",
      "epoch 110 iter 35 loss=0.05316569656133652\n",
      "epoch 110 iter 36 loss=0.06842998415231705\n",
      "epoch 110 iter 37 loss=0.05565992742776871\n",
      "epoch 110 iter 38 loss=0.05093356594443321\n",
      "epoch 110 iter 39 loss=0.05057229846715927\n",
      "epoch 110 iter 40 loss=0.04889822378754616\n",
      "epoch 110 iter 41 loss=0.051047418266534805\n",
      "epoch 110 iter 42 loss=0.07878856360912323\n",
      "epoch 110 iter 43 loss=0.044982366263866425\n",
      "epoch 110 iter 44 loss=0.049792464822530746\n",
      "epoch 110 iter 45 loss=0.05271138623356819\n",
      "epoch 110 iter 46 loss=0.05043641850352287\n",
      "epoch 110 iter 47 loss=0.034749966114759445\n",
      "epoch 110 iter 48 loss=0.025874771177768707\n",
      "epoch 110 iter 49 loss=0.046336423605680466\n",
      "epoch 110 iter 50 loss=0.08824504911899567\n",
      "epoch 110 iter 51 loss=0.03963002935051918\n",
      "epoch 110 iter 52 loss=0.03955285996198654\n",
      "epoch 110 iter 53 loss=0.06450778990983963\n",
      "epoch 110 iter 54 loss=0.04427597299218178\n",
      "epoch 110 iter 55 loss=0.03846675530076027\n",
      "epoch 110 iter 56 loss=0.025426845997571945\n",
      "epoch 110 iter 57 loss=0.03888897970318794\n",
      "epoch 110 iter 58 loss=0.042348265647888184\n",
      "epoch 110 iter 59 loss=0.03804532438516617\n",
      "epoch 110 iter 60 loss=0.052672188729047775\n",
      "epoch 110 iter 61 loss=0.05912759527564049\n",
      "epoch 110 iter 62 loss=0.05127832293510437\n",
      "epoch 110 iter 63 loss=0.061030756682157516\n",
      "epoch 110 iter 64 loss=0.0629730224609375\n",
      "epoch 110 iter 65 loss=0.04452831298112869\n",
      "epoch 110 iter 66 loss=0.04916604235768318\n",
      "epoch 110 iter 67 loss=0.06675338000059128\n",
      "epoch 110 iter 68 loss=0.048723794519901276\n",
      "epoch 110 iter 69 loss=0.059111930429935455\n",
      "epoch 110 iter 70 loss=0.04335542395710945\n",
      "epoch 110 iter 71 loss=0.03406306356191635\n",
      "epoch 110 iter 72 loss=0.06919889152050018\n",
      "epoch 110 iter 73 loss=0.08766666799783707\n",
      "epoch 110 iter 74 loss=0.05010588839650154\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3101.\n",
      "epoch 111 iter 0 loss=0.04450272023677826\n",
      "epoch 111 iter 1 loss=0.034797295928001404\n",
      "epoch 111 iter 2 loss=0.06554306298494339\n",
      "epoch 111 iter 3 loss=0.051446352154016495\n",
      "epoch 111 iter 4 loss=0.03954661637544632\n",
      "epoch 111 iter 5 loss=0.0816955640912056\n",
      "epoch 111 iter 6 loss=0.04095425456762314\n",
      "epoch 111 iter 7 loss=0.057216495275497437\n",
      "epoch 111 iter 8 loss=0.03214329853653908\n",
      "epoch 111 iter 9 loss=0.03297288715839386\n",
      "epoch 111 iter 10 loss=0.06847609579563141\n",
      "epoch 111 iter 11 loss=0.09456111490726471\n",
      "epoch 111 iter 12 loss=0.06309571862220764\n",
      "epoch 111 iter 13 loss=0.053562410175800323\n",
      "epoch 111 iter 14 loss=0.058689218014478683\n",
      "epoch 111 iter 15 loss=0.03862972557544708\n",
      "epoch 111 iter 16 loss=0.039830707013607025\n",
      "epoch 111 iter 17 loss=0.05072619020938873\n",
      "epoch 111 iter 18 loss=0.09177211672067642\n",
      "epoch 111 iter 19 loss=0.08548437803983688\n",
      "epoch 111 iter 20 loss=0.04378435015678406\n",
      "epoch 111 iter 21 loss=0.05780092999339104\n",
      "epoch 111 iter 22 loss=0.0388774499297142\n",
      "epoch 111 iter 23 loss=0.05312428995966911\n",
      "epoch 111 iter 24 loss=0.035174962133169174\n",
      "epoch 111 iter 25 loss=0.03790928050875664\n",
      "epoch 111 iter 26 loss=0.029029183089733124\n",
      "epoch 111 iter 27 loss=0.04883623495697975\n",
      "epoch 111 iter 28 loss=0.03423500433564186\n",
      "epoch 111 iter 29 loss=0.055067338049411774\n",
      "epoch 111 iter 30 loss=0.03493647649884224\n",
      "epoch 111 iter 31 loss=0.04862932115793228\n",
      "epoch 111 iter 32 loss=0.056503865867853165\n",
      "epoch 111 iter 33 loss=0.06715135276317596\n",
      "epoch 111 iter 34 loss=0.06309103965759277\n",
      "epoch 111 iter 35 loss=0.044621918350458145\n",
      "epoch 111 iter 36 loss=0.058586228638887405\n",
      "epoch 111 iter 37 loss=0.0720738023519516\n",
      "epoch 111 iter 38 loss=0.05402712896466255\n",
      "epoch 111 iter 39 loss=0.047140590846538544\n",
      "epoch 111 iter 40 loss=0.05290258675813675\n",
      "epoch 111 iter 41 loss=0.027904313057661057\n",
      "epoch 111 iter 42 loss=0.037784356623888016\n",
      "epoch 111 iter 43 loss=0.04893505200743675\n",
      "epoch 111 iter 44 loss=0.04077179357409477\n",
      "epoch 111 iter 45 loss=0.07024659961462021\n",
      "epoch 111 iter 46 loss=0.049938805401325226\n",
      "epoch 111 iter 47 loss=0.04799230024218559\n",
      "epoch 111 iter 48 loss=0.058440305292606354\n",
      "epoch 111 iter 49 loss=0.046814270317554474\n",
      "epoch 111 iter 50 loss=0.06544128060340881\n",
      "epoch 111 iter 51 loss=0.03737254813313484\n",
      "epoch 111 iter 52 loss=0.0444924458861351\n",
      "epoch 111 iter 53 loss=0.04140781611204147\n",
      "epoch 111 iter 54 loss=0.043727029114961624\n",
      "epoch 111 iter 55 loss=0.034784648567438126\n",
      "epoch 111 iter 56 loss=0.04509979113936424\n",
      "epoch 111 iter 57 loss=0.04798319190740585\n",
      "epoch 111 iter 58 loss=0.046350423246622086\n",
      "epoch 111 iter 59 loss=0.061207499355077744\n",
      "epoch 111 iter 60 loss=0.04899166151881218\n",
      "epoch 111 iter 61 loss=0.06508023291826248\n",
      "epoch 111 iter 62 loss=0.05257363244891167\n",
      "epoch 111 iter 63 loss=0.046161748468875885\n",
      "epoch 111 iter 64 loss=0.03974248468875885\n",
      "epoch 111 iter 65 loss=0.0649682879447937\n",
      "epoch 111 iter 66 loss=0.051585227251052856\n",
      "epoch 111 iter 67 loss=0.0431392677128315\n",
      "epoch 111 iter 68 loss=0.04288323596119881\n",
      "epoch 111 iter 69 loss=0.05344307795166969\n",
      "epoch 111 iter 70 loss=0.050053682178258896\n",
      "epoch 111 iter 71 loss=0.05582833290100098\n",
      "epoch 111 iter 72 loss=0.04360990971326828\n",
      "epoch 111 iter 73 loss=0.036075737327337265\n",
      "epoch 111 iter 74 loss=0.07771008461713791\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3421.\n",
      "epoch 112 iter 0 loss=0.055275075137615204\n",
      "epoch 112 iter 1 loss=0.04686858132481575\n",
      "epoch 112 iter 2 loss=0.054179344326257706\n",
      "epoch 112 iter 3 loss=0.03061707317829132\n",
      "epoch 112 iter 4 loss=0.06201432645320892\n",
      "epoch 112 iter 5 loss=0.0356447733938694\n",
      "epoch 112 iter 6 loss=0.06814681738615036\n",
      "epoch 112 iter 7 loss=0.038510095328092575\n",
      "epoch 112 iter 8 loss=0.05752149969339371\n",
      "epoch 112 iter 9 loss=0.041724931448698044\n",
      "epoch 112 iter 10 loss=0.04864293336868286\n",
      "epoch 112 iter 11 loss=0.06075221300125122\n",
      "epoch 112 iter 12 loss=0.030473755672574043\n",
      "epoch 112 iter 13 loss=0.03912699222564697\n",
      "epoch 112 iter 14 loss=0.05835973471403122\n",
      "epoch 112 iter 15 loss=0.029966047033667564\n",
      "epoch 112 iter 16 loss=0.03345680609345436\n",
      "epoch 112 iter 17 loss=0.03492315486073494\n",
      "epoch 112 iter 18 loss=0.024796264246106148\n",
      "epoch 112 iter 19 loss=0.07864000648260117\n",
      "epoch 112 iter 20 loss=0.03682003542780876\n",
      "epoch 112 iter 21 loss=0.03464841842651367\n",
      "epoch 112 iter 22 loss=0.04385453835129738\n",
      "epoch 112 iter 23 loss=0.05916869267821312\n",
      "epoch 112 iter 24 loss=0.04942835122346878\n",
      "epoch 112 iter 25 loss=0.03643825650215149\n",
      "epoch 112 iter 26 loss=0.06936923414468765\n",
      "epoch 112 iter 27 loss=0.0595560260117054\n",
      "epoch 112 iter 28 loss=0.07020657509565353\n",
      "epoch 112 iter 29 loss=0.038747478276491165\n",
      "epoch 112 iter 30 loss=0.061796024441719055\n",
      "epoch 112 iter 31 loss=0.04006852209568024\n",
      "epoch 112 iter 32 loss=0.05652191489934921\n",
      "epoch 112 iter 33 loss=0.06626563519239426\n",
      "epoch 112 iter 34 loss=0.04767248407006264\n",
      "epoch 112 iter 35 loss=0.04643737152218819\n",
      "epoch 112 iter 36 loss=0.06782825291156769\n",
      "epoch 112 iter 37 loss=0.04192078113555908\n",
      "epoch 112 iter 38 loss=0.06431534141302109\n",
      "epoch 112 iter 39 loss=0.06701669842004776\n",
      "epoch 112 iter 40 loss=0.05202434957027435\n",
      "epoch 112 iter 41 loss=0.04187338054180145\n",
      "epoch 112 iter 42 loss=0.07030820846557617\n",
      "epoch 112 iter 43 loss=0.06853868067264557\n",
      "epoch 112 iter 44 loss=0.027177300304174423\n",
      "epoch 112 iter 45 loss=0.034679971635341644\n",
      "epoch 112 iter 46 loss=0.04947684705257416\n",
      "epoch 112 iter 47 loss=0.03191151097416878\n",
      "epoch 112 iter 48 loss=0.06697920709848404\n",
      "epoch 112 iter 49 loss=0.04158053174614906\n",
      "epoch 112 iter 50 loss=0.06349950283765793\n",
      "epoch 112 iter 51 loss=0.060291606932878494\n",
      "epoch 112 iter 52 loss=0.05428948625922203\n",
      "epoch 112 iter 53 loss=0.04189189150929451\n",
      "epoch 112 iter 54 loss=0.043446578085422516\n",
      "epoch 112 iter 55 loss=0.053425781428813934\n",
      "epoch 112 iter 56 loss=0.07437380403280258\n",
      "epoch 112 iter 57 loss=0.0394747294485569\n",
      "epoch 112 iter 58 loss=0.05139622837305069\n",
      "epoch 112 iter 59 loss=0.042996104806661606\n",
      "epoch 112 iter 60 loss=0.08626912534236908\n",
      "epoch 112 iter 61 loss=0.031124822795391083\n",
      "epoch 112 iter 62 loss=0.07136564701795578\n",
      "epoch 112 iter 63 loss=0.03369726240634918\n",
      "epoch 112 iter 64 loss=0.05021439865231514\n",
      "epoch 112 iter 65 loss=0.0632626861333847\n",
      "epoch 112 iter 66 loss=0.04667635262012482\n",
      "epoch 112 iter 67 loss=0.04944980517029762\n",
      "epoch 112 iter 68 loss=0.0640038326382637\n",
      "epoch 112 iter 69 loss=0.03218971937894821\n",
      "epoch 112 iter 70 loss=0.06204131618142128\n",
      "epoch 112 iter 71 loss=0.0574689619243145\n",
      "epoch 112 iter 72 loss=0.06580155342817307\n",
      "epoch 112 iter 73 loss=0.036010775715112686\n",
      "epoch 112 iter 74 loss=0.022213030606508255\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3201.\n",
      "epoch 113 iter 0 loss=0.0588480606675148\n",
      "epoch 113 iter 1 loss=0.0430467315018177\n",
      "epoch 113 iter 2 loss=0.03995368629693985\n",
      "epoch 113 iter 3 loss=0.04874830320477486\n",
      "epoch 113 iter 4 loss=0.058917805552482605\n",
      "epoch 113 iter 5 loss=0.0524706095457077\n",
      "epoch 113 iter 6 loss=0.0538371205329895\n",
      "epoch 113 iter 7 loss=0.055007509887218475\n",
      "epoch 113 iter 8 loss=0.03392123803496361\n",
      "epoch 113 iter 9 loss=0.037461817264556885\n",
      "epoch 113 iter 10 loss=0.05898652598261833\n",
      "epoch 113 iter 11 loss=0.02925434336066246\n",
      "epoch 113 iter 12 loss=0.048088159412145615\n",
      "epoch 113 iter 13 loss=0.049202773720026016\n",
      "epoch 113 iter 14 loss=0.07856019586324692\n",
      "epoch 113 iter 15 loss=0.041949864476919174\n",
      "epoch 113 iter 16 loss=0.06610193103551865\n",
      "epoch 113 iter 17 loss=0.02837151661515236\n",
      "epoch 113 iter 18 loss=0.052048373967409134\n",
      "epoch 113 iter 19 loss=0.07853077352046967\n",
      "epoch 113 iter 20 loss=0.061172667890787125\n",
      "epoch 113 iter 21 loss=0.06043947860598564\n",
      "epoch 113 iter 22 loss=0.02641741931438446\n",
      "epoch 113 iter 23 loss=0.041141703724861145\n",
      "epoch 113 iter 24 loss=0.04391079023480415\n",
      "epoch 113 iter 25 loss=0.05089658498764038\n",
      "epoch 113 iter 26 loss=0.043341342359781265\n",
      "epoch 113 iter 27 loss=0.04995417594909668\n",
      "epoch 113 iter 28 loss=0.035381920635700226\n",
      "epoch 113 iter 29 loss=0.053962841629981995\n",
      "epoch 113 iter 30 loss=0.056098274886608124\n",
      "epoch 113 iter 31 loss=0.052759066224098206\n",
      "epoch 113 iter 32 loss=0.056609317660331726\n",
      "epoch 113 iter 33 loss=0.06860474497079849\n",
      "epoch 113 iter 34 loss=0.055028364062309265\n",
      "epoch 113 iter 35 loss=0.06827466189861298\n",
      "epoch 113 iter 36 loss=0.060757193714380264\n",
      "epoch 113 iter 37 loss=0.03681202977895737\n",
      "epoch 113 iter 38 loss=0.030328068882226944\n",
      "epoch 113 iter 39 loss=0.034442804753780365\n",
      "epoch 113 iter 40 loss=0.028379952535033226\n",
      "epoch 113 iter 41 loss=0.09845700860023499\n",
      "epoch 113 iter 42 loss=0.05572162941098213\n",
      "epoch 113 iter 43 loss=0.07075755298137665\n",
      "epoch 113 iter 44 loss=0.03909612074494362\n",
      "epoch 113 iter 45 loss=0.10017647594213486\n",
      "epoch 113 iter 46 loss=0.051588136702775955\n",
      "epoch 113 iter 47 loss=0.04711192846298218\n",
      "epoch 113 iter 48 loss=0.07205002009868622\n",
      "epoch 113 iter 49 loss=0.05092809349298477\n",
      "epoch 113 iter 50 loss=0.09254966676235199\n",
      "epoch 113 iter 51 loss=0.03807338699698448\n",
      "epoch 113 iter 52 loss=0.055883459746837616\n",
      "epoch 113 iter 53 loss=0.0608767531812191\n",
      "epoch 113 iter 54 loss=0.04613034799695015\n",
      "epoch 113 iter 55 loss=0.0515364445745945\n",
      "epoch 113 iter 56 loss=0.09172055125236511\n",
      "epoch 113 iter 57 loss=0.06573542952537537\n",
      "epoch 113 iter 58 loss=0.05387529358267784\n",
      "epoch 113 iter 59 loss=0.07231529802083969\n",
      "epoch 113 iter 60 loss=0.06514851748943329\n",
      "epoch 113 iter 61 loss=0.05895211920142174\n",
      "epoch 113 iter 62 loss=0.05357050523161888\n",
      "epoch 113 iter 63 loss=0.047629214823246\n",
      "epoch 113 iter 64 loss=0.04473821073770523\n",
      "epoch 113 iter 65 loss=0.05914339795708656\n",
      "epoch 113 iter 66 loss=0.062475137412548065\n",
      "epoch 113 iter 67 loss=0.03617832064628601\n",
      "epoch 113 iter 68 loss=0.04104837402701378\n",
      "epoch 113 iter 69 loss=0.06479492783546448\n",
      "epoch 113 iter 70 loss=0.060559991747140884\n",
      "epoch 113 iter 71 loss=0.02661621756851673\n",
      "epoch 113 iter 72 loss=0.040671445429325104\n",
      "epoch 113 iter 73 loss=0.08099985122680664\n",
      "epoch 113 iter 74 loss=0.07362689077854156\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3129.\n",
      "epoch 114 iter 0 loss=0.047774557024240494\n",
      "epoch 114 iter 1 loss=0.07757073640823364\n",
      "epoch 114 iter 2 loss=0.0620453916490078\n",
      "epoch 114 iter 3 loss=0.056001920253038406\n",
      "epoch 114 iter 4 loss=0.03956868126988411\n",
      "epoch 114 iter 5 loss=0.07140856236219406\n",
      "epoch 114 iter 6 loss=0.04039272293448448\n",
      "epoch 114 iter 7 loss=0.03581981360912323\n",
      "epoch 114 iter 8 loss=0.09591452777385712\n",
      "epoch 114 iter 9 loss=0.03269755467772484\n",
      "epoch 114 iter 10 loss=0.03260544687509537\n",
      "epoch 114 iter 11 loss=0.07371392846107483\n",
      "epoch 114 iter 12 loss=0.04239319637417793\n",
      "epoch 114 iter 13 loss=0.06456317752599716\n",
      "epoch 114 iter 14 loss=0.03328065201640129\n",
      "epoch 114 iter 15 loss=0.04100678488612175\n",
      "epoch 114 iter 16 loss=0.05624528229236603\n",
      "epoch 114 iter 17 loss=0.07672663033008575\n",
      "epoch 114 iter 18 loss=0.04233948886394501\n",
      "epoch 114 iter 19 loss=0.03311343118548393\n",
      "epoch 114 iter 20 loss=0.07065034657716751\n",
      "epoch 114 iter 21 loss=0.05070273578166962\n",
      "epoch 114 iter 22 loss=0.05190820246934891\n",
      "epoch 114 iter 23 loss=0.03068973869085312\n",
      "epoch 114 iter 24 loss=0.04057237133383751\n",
      "epoch 114 iter 25 loss=0.062297970056533813\n",
      "epoch 114 iter 26 loss=0.07037664949893951\n",
      "epoch 114 iter 27 loss=0.0809355154633522\n",
      "epoch 114 iter 28 loss=0.057321108877658844\n",
      "epoch 114 iter 29 loss=0.04052792489528656\n",
      "epoch 114 iter 30 loss=0.04759489372372627\n",
      "epoch 114 iter 31 loss=0.05612417683005333\n",
      "epoch 114 iter 32 loss=0.035700827836990356\n",
      "epoch 114 iter 33 loss=0.03981636092066765\n",
      "epoch 114 iter 34 loss=0.038651783019304276\n",
      "epoch 114 iter 35 loss=0.045836981385946274\n",
      "epoch 114 iter 36 loss=0.06682714819908142\n",
      "epoch 114 iter 37 loss=0.05576809123158455\n",
      "epoch 114 iter 38 loss=0.07898549735546112\n",
      "epoch 114 iter 39 loss=0.035507626831531525\n",
      "epoch 114 iter 40 loss=0.05143928527832031\n",
      "epoch 114 iter 41 loss=0.058798689395189285\n",
      "epoch 114 iter 42 loss=0.03556099906563759\n",
      "epoch 114 iter 43 loss=0.04370398446917534\n",
      "epoch 114 iter 44 loss=0.04660426452755928\n",
      "epoch 114 iter 45 loss=0.02867424674332142\n",
      "epoch 114 iter 46 loss=0.033558961004018784\n",
      "epoch 114 iter 47 loss=0.02851003222167492\n",
      "epoch 114 iter 48 loss=0.05312503129243851\n",
      "epoch 114 iter 49 loss=0.06574999541044235\n",
      "epoch 114 iter 50 loss=0.04829910770058632\n",
      "epoch 114 iter 51 loss=0.055763933807611465\n",
      "epoch 114 iter 52 loss=0.0661468505859375\n",
      "epoch 114 iter 53 loss=0.05441100522875786\n",
      "epoch 114 iter 54 loss=0.06583692878484726\n",
      "epoch 114 iter 55 loss=0.06824322044849396\n",
      "epoch 114 iter 56 loss=0.04495777562260628\n",
      "epoch 114 iter 57 loss=0.036313630640506744\n",
      "epoch 114 iter 58 loss=0.04869430884718895\n",
      "epoch 114 iter 59 loss=0.05309906601905823\n",
      "epoch 114 iter 60 loss=0.0554419606924057\n",
      "epoch 114 iter 61 loss=0.022653799504041672\n",
      "epoch 114 iter 62 loss=0.06831357628107071\n",
      "epoch 114 iter 63 loss=0.050311483442783356\n",
      "epoch 114 iter 64 loss=0.032213155180215836\n",
      "epoch 114 iter 65 loss=0.04863489419221878\n",
      "epoch 114 iter 66 loss=0.056838810443878174\n",
      "epoch 114 iter 67 loss=0.03660450503230095\n",
      "epoch 114 iter 68 loss=0.050078731030225754\n",
      "epoch 114 iter 69 loss=0.03083653561770916\n",
      "epoch 114 iter 70 loss=0.047503307461738586\n",
      "epoch 114 iter 71 loss=0.052273184061050415\n",
      "epoch 114 iter 72 loss=0.04877857863903046\n",
      "epoch 114 iter 73 loss=0.0582396537065506\n",
      "epoch 114 iter 74 loss=0.058468110859394073\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3221.\n",
      "epoch 115 iter 0 loss=0.06391386687755585\n",
      "epoch 115 iter 1 loss=0.03008333221077919\n",
      "epoch 115 iter 2 loss=0.04526873305439949\n",
      "epoch 115 iter 3 loss=0.058654509484767914\n",
      "epoch 115 iter 4 loss=0.0366349034011364\n",
      "epoch 115 iter 5 loss=0.06770853698253632\n",
      "epoch 115 iter 6 loss=0.03556602820754051\n",
      "epoch 115 iter 7 loss=0.04136928915977478\n",
      "epoch 115 iter 8 loss=0.039195384830236435\n",
      "epoch 115 iter 9 loss=0.047811780124902725\n",
      "epoch 115 iter 10 loss=0.047397445887327194\n",
      "epoch 115 iter 11 loss=0.043912868946790695\n",
      "epoch 115 iter 12 loss=0.05183633789420128\n",
      "epoch 115 iter 13 loss=0.06121399998664856\n",
      "epoch 115 iter 14 loss=0.03598347306251526\n",
      "epoch 115 iter 15 loss=0.05285370722413063\n",
      "epoch 115 iter 16 loss=0.021527307108044624\n",
      "epoch 115 iter 17 loss=0.04041508957743645\n",
      "epoch 115 iter 18 loss=0.06865224242210388\n",
      "epoch 115 iter 19 loss=0.048789896070957184\n",
      "epoch 115 iter 20 loss=0.04875290393829346\n",
      "epoch 115 iter 21 loss=0.03586111590266228\n",
      "epoch 115 iter 22 loss=0.03268098086118698\n",
      "epoch 115 iter 23 loss=0.06290855258703232\n",
      "epoch 115 iter 24 loss=0.044383373111486435\n",
      "epoch 115 iter 25 loss=0.060656819492578506\n",
      "epoch 115 iter 26 loss=0.041991278529167175\n",
      "epoch 115 iter 27 loss=0.052142683416604996\n",
      "epoch 115 iter 28 loss=0.04385270178318024\n",
      "epoch 115 iter 29 loss=0.07602426409721375\n",
      "epoch 115 iter 30 loss=0.041373588144779205\n",
      "epoch 115 iter 31 loss=0.03053644299507141\n",
      "epoch 115 iter 32 loss=0.0704197958111763\n",
      "epoch 115 iter 33 loss=0.03100588545203209\n",
      "epoch 115 iter 34 loss=0.06618299335241318\n",
      "epoch 115 iter 35 loss=0.05252578854560852\n",
      "epoch 115 iter 36 loss=0.055571988224983215\n",
      "epoch 115 iter 37 loss=0.05733990669250488\n",
      "epoch 115 iter 38 loss=0.051997557282447815\n",
      "epoch 115 iter 39 loss=0.0410182885825634\n",
      "epoch 115 iter 40 loss=0.044462304562330246\n",
      "epoch 115 iter 41 loss=0.07288093864917755\n",
      "epoch 115 iter 42 loss=0.030825749039649963\n",
      "epoch 115 iter 43 loss=0.03403631970286369\n",
      "epoch 115 iter 44 loss=0.03564663603901863\n",
      "epoch 115 iter 45 loss=0.04481617361307144\n",
      "epoch 115 iter 46 loss=0.0438692644238472\n",
      "epoch 115 iter 47 loss=0.03634616732597351\n",
      "epoch 115 iter 48 loss=0.03661410138010979\n",
      "epoch 115 iter 49 loss=0.06323258578777313\n",
      "epoch 115 iter 50 loss=0.06043226644396782\n",
      "epoch 115 iter 51 loss=0.05450744926929474\n",
      "epoch 115 iter 52 loss=0.07725141942501068\n",
      "epoch 115 iter 53 loss=0.04683521389961243\n",
      "epoch 115 iter 54 loss=0.0537194088101387\n",
      "epoch 115 iter 55 loss=0.048473771661520004\n",
      "epoch 115 iter 56 loss=0.052810341119766235\n",
      "epoch 115 iter 57 loss=0.043480515480041504\n",
      "epoch 115 iter 58 loss=0.04864102602005005\n",
      "epoch 115 iter 59 loss=0.046655088663101196\n",
      "epoch 115 iter 60 loss=0.04551870375871658\n",
      "epoch 115 iter 61 loss=0.06880193203687668\n",
      "epoch 115 iter 62 loss=0.05101664736866951\n",
      "epoch 115 iter 63 loss=0.06637018173933029\n",
      "epoch 115 iter 64 loss=0.06756006926298141\n",
      "epoch 115 iter 65 loss=0.04685549437999725\n",
      "epoch 115 iter 66 loss=0.0571945421397686\n",
      "epoch 115 iter 67 loss=0.04939359426498413\n",
      "epoch 115 iter 68 loss=0.0458596870303154\n",
      "epoch 115 iter 69 loss=0.037060897797346115\n",
      "epoch 115 iter 70 loss=0.05286625772714615\n",
      "epoch 115 iter 71 loss=0.05393820255994797\n",
      "epoch 115 iter 72 loss=0.04139462113380432\n",
      "epoch 115 iter 73 loss=0.024256136268377304\n",
      "epoch 115 iter 74 loss=0.037339113652706146\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3236.\n",
      "epoch 116 iter 0 loss=0.0362435020506382\n",
      "epoch 116 iter 1 loss=0.060516443103551865\n",
      "epoch 116 iter 2 loss=0.03873850405216217\n",
      "epoch 116 iter 3 loss=0.050447262823581696\n",
      "epoch 116 iter 4 loss=0.04077782481908798\n",
      "epoch 116 iter 5 loss=0.02575260028243065\n",
      "epoch 116 iter 6 loss=0.03410021588206291\n",
      "epoch 116 iter 7 loss=0.04876609146595001\n",
      "epoch 116 iter 8 loss=0.05352969840168953\n",
      "epoch 116 iter 9 loss=0.03730802237987518\n",
      "epoch 116 iter 10 loss=0.0518377423286438\n",
      "epoch 116 iter 11 loss=0.08530309051275253\n",
      "epoch 116 iter 12 loss=0.0558735728263855\n",
      "epoch 116 iter 13 loss=0.04388955608010292\n",
      "epoch 116 iter 14 loss=0.05392976477742195\n",
      "epoch 116 iter 15 loss=0.0744757354259491\n",
      "epoch 116 iter 16 loss=0.03471079096198082\n",
      "epoch 116 iter 17 loss=0.05942844599485397\n",
      "epoch 116 iter 18 loss=0.04574952647089958\n",
      "epoch 116 iter 19 loss=0.053922250866889954\n",
      "epoch 116 iter 20 loss=0.04899553209543228\n",
      "epoch 116 iter 21 loss=0.038498565554618835\n",
      "epoch 116 iter 22 loss=0.027383174747228622\n",
      "epoch 116 iter 23 loss=0.06478145718574524\n",
      "epoch 116 iter 24 loss=0.024266518652439117\n",
      "epoch 116 iter 25 loss=0.053729016333818436\n",
      "epoch 116 iter 26 loss=0.06027451530098915\n",
      "epoch 116 iter 27 loss=0.07808591425418854\n",
      "epoch 116 iter 28 loss=0.04240119457244873\n",
      "epoch 116 iter 29 loss=0.04964501038193703\n",
      "epoch 116 iter 30 loss=0.054576948285102844\n",
      "epoch 116 iter 31 loss=0.03775717690587044\n",
      "epoch 116 iter 32 loss=0.03451932221651077\n",
      "epoch 116 iter 33 loss=0.06715094298124313\n",
      "epoch 116 iter 34 loss=0.029331056401133537\n",
      "epoch 116 iter 35 loss=0.043424028903245926\n",
      "epoch 116 iter 36 loss=0.07751812040805817\n",
      "epoch 116 iter 37 loss=0.0740433782339096\n",
      "epoch 116 iter 38 loss=0.05615614727139473\n",
      "epoch 116 iter 39 loss=0.021560465916991234\n",
      "epoch 116 iter 40 loss=0.03810228034853935\n",
      "epoch 116 iter 41 loss=0.04909970611333847\n",
      "epoch 116 iter 42 loss=0.053470395505428314\n",
      "epoch 116 iter 43 loss=0.03815064579248428\n",
      "epoch 116 iter 44 loss=0.037848155945539474\n",
      "epoch 116 iter 45 loss=0.03727666661143303\n",
      "epoch 116 iter 46 loss=0.033308446407318115\n",
      "epoch 116 iter 47 loss=0.0483890175819397\n",
      "epoch 116 iter 48 loss=0.028039125725626945\n",
      "epoch 116 iter 49 loss=0.03636278584599495\n",
      "epoch 116 iter 50 loss=0.0488893985748291\n",
      "epoch 116 iter 51 loss=0.055698830634355545\n",
      "epoch 116 iter 52 loss=0.04168218746781349\n",
      "epoch 116 iter 53 loss=0.06645926088094711\n",
      "epoch 116 iter 54 loss=0.025847425684332848\n",
      "epoch 116 iter 55 loss=0.05799741670489311\n",
      "epoch 116 iter 56 loss=0.07198057323694229\n",
      "epoch 116 iter 57 loss=0.03874018415808678\n",
      "epoch 116 iter 58 loss=0.03938226401805878\n",
      "epoch 116 iter 59 loss=0.08647613227367401\n",
      "epoch 116 iter 60 loss=0.059928614646196365\n",
      "epoch 116 iter 61 loss=0.041477639228105545\n",
      "epoch 116 iter 62 loss=0.059659119695425034\n",
      "epoch 116 iter 63 loss=0.04331015795469284\n",
      "epoch 116 iter 64 loss=0.03068968839943409\n",
      "epoch 116 iter 65 loss=0.028074165806174278\n",
      "epoch 116 iter 66 loss=0.05053248628973961\n",
      "epoch 116 iter 67 loss=0.04946945980191231\n",
      "epoch 116 iter 68 loss=0.04757412523031235\n",
      "epoch 116 iter 69 loss=0.062322232872247696\n",
      "epoch 116 iter 70 loss=0.028472568839788437\n",
      "epoch 116 iter 71 loss=0.05767601728439331\n",
      "epoch 116 iter 72 loss=0.043012458831071854\n",
      "epoch 116 iter 73 loss=0.04831582307815552\n",
      "epoch 116 iter 74 loss=0.05074824392795563\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3195.\n",
      "epoch 117 iter 0 loss=0.029024701565504074\n",
      "epoch 117 iter 1 loss=0.050693728029727936\n",
      "epoch 117 iter 2 loss=0.040175385773181915\n",
      "epoch 117 iter 3 loss=0.05118057504296303\n",
      "epoch 117 iter 4 loss=0.041605353355407715\n",
      "epoch 117 iter 5 loss=0.04104660451412201\n",
      "epoch 117 iter 6 loss=0.08220916986465454\n",
      "epoch 117 iter 7 loss=0.0469418428838253\n",
      "epoch 117 iter 8 loss=0.04425284266471863\n",
      "epoch 117 iter 9 loss=0.049359843134880066\n",
      "epoch 117 iter 10 loss=0.04897645488381386\n",
      "epoch 117 iter 11 loss=0.026024229824543\n",
      "epoch 117 iter 12 loss=0.04340622201561928\n",
      "epoch 117 iter 13 loss=0.051268789917230606\n",
      "epoch 117 iter 14 loss=0.03861123323440552\n",
      "epoch 117 iter 15 loss=0.029070645570755005\n",
      "epoch 117 iter 16 loss=0.05580433830618858\n",
      "epoch 117 iter 17 loss=0.046848930418491364\n",
      "epoch 117 iter 18 loss=0.034907519817352295\n",
      "epoch 117 iter 19 loss=0.028869332745671272\n",
      "epoch 117 iter 20 loss=0.04201686754822731\n",
      "epoch 117 iter 21 loss=0.06133495643734932\n",
      "epoch 117 iter 22 loss=0.02971350960433483\n",
      "epoch 117 iter 23 loss=0.03936530277132988\n",
      "epoch 117 iter 24 loss=0.04609547555446625\n",
      "epoch 117 iter 25 loss=0.05179494619369507\n",
      "epoch 117 iter 26 loss=0.04407164826989174\n",
      "epoch 117 iter 27 loss=0.06094524636864662\n",
      "epoch 117 iter 28 loss=0.04682036116719246\n",
      "epoch 117 iter 29 loss=0.07469574362039566\n",
      "epoch 117 iter 30 loss=0.03708122298121452\n",
      "epoch 117 iter 31 loss=0.05841036140918732\n",
      "epoch 117 iter 32 loss=0.035508573055267334\n",
      "epoch 117 iter 33 loss=0.06343960762023926\n",
      "epoch 117 iter 34 loss=0.057627856731414795\n",
      "epoch 117 iter 35 loss=0.03954677656292915\n",
      "epoch 117 iter 36 loss=0.06553048640489578\n",
      "epoch 117 iter 37 loss=0.04685869812965393\n",
      "epoch 117 iter 38 loss=0.045376766473054886\n",
      "epoch 117 iter 39 loss=0.06284496933221817\n",
      "epoch 117 iter 40 loss=0.04020233452320099\n",
      "epoch 117 iter 41 loss=0.04499678686261177\n",
      "epoch 117 iter 42 loss=0.040434107184410095\n",
      "epoch 117 iter 43 loss=0.07618192583322525\n",
      "epoch 117 iter 44 loss=0.04932493716478348\n",
      "epoch 117 iter 45 loss=0.07761302590370178\n",
      "epoch 117 iter 46 loss=0.04004603624343872\n",
      "epoch 117 iter 47 loss=0.0379098504781723\n",
      "epoch 117 iter 48 loss=0.0366453193128109\n",
      "epoch 117 iter 49 loss=0.05086071044206619\n",
      "epoch 117 iter 50 loss=0.06131330132484436\n",
      "epoch 117 iter 51 loss=0.0409880094230175\n",
      "epoch 117 iter 52 loss=0.04525495320558548\n",
      "epoch 117 iter 53 loss=0.03587619215250015\n",
      "epoch 117 iter 54 loss=0.03749144822359085\n",
      "epoch 117 iter 55 loss=0.026097841560840607\n",
      "epoch 117 iter 56 loss=0.06702398508787155\n",
      "epoch 117 iter 57 loss=0.06453752517700195\n",
      "epoch 117 iter 58 loss=0.05170341581106186\n",
      "epoch 117 iter 59 loss=0.049769528210163116\n",
      "epoch 117 iter 60 loss=0.030375797301530838\n",
      "epoch 117 iter 61 loss=0.040840573608875275\n",
      "epoch 117 iter 62 loss=0.047548651695251465\n",
      "epoch 117 iter 63 loss=0.07075194269418716\n",
      "epoch 117 iter 64 loss=0.068825863301754\n",
      "epoch 117 iter 65 loss=0.050240423530340195\n",
      "epoch 117 iter 66 loss=0.04048608988523483\n",
      "epoch 117 iter 67 loss=0.04134909436106682\n",
      "epoch 117 iter 68 loss=0.03778832405805588\n",
      "epoch 117 iter 69 loss=0.04443278908729553\n",
      "epoch 117 iter 70 loss=0.042969319969415665\n",
      "epoch 117 iter 71 loss=0.058169323951005936\n",
      "epoch 117 iter 72 loss=0.035211723297834396\n",
      "epoch 117 iter 73 loss=0.04082717373967171\n",
      "epoch 117 iter 74 loss=0.03688007965683937\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3291.\n",
      "epoch 118 iter 0 loss=0.047076266258955\n",
      "epoch 118 iter 1 loss=0.05182299017906189\n",
      "epoch 118 iter 2 loss=0.05209629610180855\n",
      "epoch 118 iter 3 loss=0.05634983256459236\n",
      "epoch 118 iter 4 loss=0.04737170413136482\n",
      "epoch 118 iter 5 loss=0.0838535875082016\n",
      "epoch 118 iter 6 loss=0.060623280704021454\n",
      "epoch 118 iter 7 loss=0.04623574763536453\n",
      "epoch 118 iter 8 loss=0.03407340124249458\n",
      "epoch 118 iter 9 loss=0.041298575699329376\n",
      "epoch 118 iter 10 loss=0.041251737624406815\n",
      "epoch 118 iter 11 loss=0.05083256587386131\n",
      "epoch 118 iter 12 loss=0.02649638243019581\n",
      "epoch 118 iter 13 loss=0.039739906787872314\n",
      "epoch 118 iter 14 loss=0.05507691577076912\n",
      "epoch 118 iter 15 loss=0.02882811427116394\n",
      "epoch 118 iter 16 loss=0.06266238540410995\n",
      "epoch 118 iter 17 loss=0.02913803793489933\n",
      "epoch 118 iter 18 loss=0.04191076382994652\n",
      "epoch 118 iter 19 loss=0.06956496834754944\n",
      "epoch 118 iter 20 loss=0.04749790206551552\n",
      "epoch 118 iter 21 loss=0.0353398434817791\n",
      "epoch 118 iter 22 loss=0.06605467200279236\n",
      "epoch 118 iter 23 loss=0.04889620468020439\n",
      "epoch 118 iter 24 loss=0.020031986758112907\n",
      "epoch 118 iter 25 loss=0.03187685087323189\n",
      "epoch 118 iter 26 loss=0.06053891032934189\n",
      "epoch 118 iter 27 loss=0.06275356560945511\n",
      "epoch 118 iter 28 loss=0.052369359880685806\n",
      "epoch 118 iter 29 loss=0.054374899715185165\n",
      "epoch 118 iter 30 loss=0.047906000167131424\n",
      "epoch 118 iter 31 loss=0.06180811673402786\n",
      "epoch 118 iter 32 loss=0.07098649442195892\n",
      "epoch 118 iter 33 loss=0.04166676849126816\n",
      "epoch 118 iter 34 loss=0.06356333196163177\n",
      "epoch 118 iter 35 loss=0.027806604281067848\n",
      "epoch 118 iter 36 loss=0.03100319765508175\n",
      "epoch 118 iter 37 loss=0.04170455038547516\n",
      "epoch 118 iter 38 loss=0.04540129378437996\n",
      "epoch 118 iter 39 loss=0.049706846475601196\n",
      "epoch 118 iter 40 loss=0.0670986995100975\n",
      "epoch 118 iter 41 loss=0.062365349382162094\n",
      "epoch 118 iter 42 loss=0.04310283437371254\n",
      "epoch 118 iter 43 loss=0.047866497188806534\n",
      "epoch 118 iter 44 loss=0.028485052287578583\n",
      "epoch 118 iter 45 loss=0.024354632943868637\n",
      "epoch 118 iter 46 loss=0.0490264818072319\n",
      "epoch 118 iter 47 loss=0.04969313368201256\n",
      "epoch 118 iter 48 loss=0.03879503905773163\n",
      "epoch 118 iter 49 loss=0.0496223047375679\n",
      "epoch 118 iter 50 loss=0.04698631167411804\n",
      "epoch 118 iter 51 loss=0.03551560640335083\n",
      "epoch 118 iter 52 loss=0.0416407510638237\n",
      "epoch 118 iter 53 loss=0.03932051733136177\n",
      "epoch 118 iter 54 loss=0.029682129621505737\n",
      "epoch 118 iter 55 loss=0.02585381641983986\n",
      "epoch 118 iter 56 loss=0.027660168707370758\n",
      "epoch 118 iter 57 loss=0.03819818049669266\n",
      "epoch 118 iter 58 loss=0.06735944002866745\n",
      "epoch 118 iter 59 loss=0.027215005829930305\n",
      "epoch 118 iter 60 loss=0.0599212720990181\n",
      "epoch 118 iter 61 loss=0.03692727908492088\n",
      "epoch 118 iter 62 loss=0.05534568056464195\n",
      "epoch 118 iter 63 loss=0.04579230025410652\n",
      "epoch 118 iter 64 loss=0.05159697309136391\n",
      "epoch 118 iter 65 loss=0.04541429132223129\n",
      "epoch 118 iter 66 loss=0.033661823719739914\n",
      "epoch 118 iter 67 loss=0.05939678102731705\n",
      "epoch 118 iter 68 loss=0.0475650317966938\n",
      "epoch 118 iter 69 loss=0.07368917763233185\n",
      "epoch 118 iter 70 loss=0.04891066253185272\n",
      "epoch 118 iter 71 loss=0.057606905698776245\n",
      "epoch 118 iter 72 loss=0.04257652908563614\n",
      "epoch 118 iter 73 loss=0.05007433891296387\n",
      "epoch 118 iter 74 loss=0.05118938907980919\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3245.\n",
      "epoch 119 iter 0 loss=0.03909360244870186\n",
      "epoch 119 iter 1 loss=0.03155970573425293\n",
      "epoch 119 iter 2 loss=0.03987106308341026\n",
      "epoch 119 iter 3 loss=0.033544883131980896\n",
      "epoch 119 iter 4 loss=0.04095194861292839\n",
      "epoch 119 iter 5 loss=0.05572706460952759\n",
      "epoch 119 iter 6 loss=0.04936130717396736\n",
      "epoch 119 iter 7 loss=0.06958667933940887\n",
      "epoch 119 iter 8 loss=0.031209737062454224\n",
      "epoch 119 iter 9 loss=0.050189658999443054\n",
      "epoch 119 iter 10 loss=0.0542982742190361\n",
      "epoch 119 iter 11 loss=0.0258720014244318\n",
      "epoch 119 iter 12 loss=0.05185391381382942\n",
      "epoch 119 iter 13 loss=0.03910873830318451\n",
      "epoch 119 iter 14 loss=0.05881938710808754\n",
      "epoch 119 iter 15 loss=0.03694312646985054\n",
      "epoch 119 iter 16 loss=0.04121137410402298\n",
      "epoch 119 iter 17 loss=0.057166289538145065\n",
      "epoch 119 iter 18 loss=0.0526956208050251\n",
      "epoch 119 iter 19 loss=0.05039171874523163\n",
      "epoch 119 iter 20 loss=0.04996756464242935\n",
      "epoch 119 iter 21 loss=0.04560878500342369\n",
      "epoch 119 iter 22 loss=0.06360555440187454\n",
      "epoch 119 iter 23 loss=0.034022971987724304\n",
      "epoch 119 iter 24 loss=0.03781939297914505\n",
      "epoch 119 iter 25 loss=0.04891325905919075\n",
      "epoch 119 iter 26 loss=0.03959536924958229\n",
      "epoch 119 iter 27 loss=0.06185091659426689\n",
      "epoch 119 iter 28 loss=0.050535403192043304\n",
      "epoch 119 iter 29 loss=0.03100055269896984\n",
      "epoch 119 iter 30 loss=0.043728701770305634\n",
      "epoch 119 iter 31 loss=0.06377003341913223\n",
      "epoch 119 iter 32 loss=0.027910232543945312\n",
      "epoch 119 iter 33 loss=0.029994618147611618\n",
      "epoch 119 iter 34 loss=0.02862752601504326\n",
      "epoch 119 iter 35 loss=0.0546741783618927\n",
      "epoch 119 iter 36 loss=0.03846730664372444\n",
      "epoch 119 iter 37 loss=0.03252428397536278\n",
      "epoch 119 iter 38 loss=0.06715480238199234\n",
      "epoch 119 iter 39 loss=0.03018326498568058\n",
      "epoch 119 iter 40 loss=0.03702482581138611\n",
      "epoch 119 iter 41 loss=0.0498553030192852\n",
      "epoch 119 iter 42 loss=0.047356098890304565\n",
      "epoch 119 iter 43 loss=0.0723649114370346\n",
      "epoch 119 iter 44 loss=0.04168358072638512\n",
      "epoch 119 iter 45 loss=0.07195285707712173\n",
      "epoch 119 iter 46 loss=0.05241984501481056\n",
      "epoch 119 iter 47 loss=0.041497960686683655\n",
      "epoch 119 iter 48 loss=0.04618275165557861\n",
      "epoch 119 iter 49 loss=0.03652295470237732\n",
      "epoch 119 iter 50 loss=0.046921562403440475\n",
      "epoch 119 iter 51 loss=0.05690718814730644\n",
      "epoch 119 iter 52 loss=0.043552201241254807\n",
      "epoch 119 iter 53 loss=0.06394720822572708\n",
      "epoch 119 iter 54 loss=0.04092050716280937\n",
      "epoch 119 iter 55 loss=0.08590102195739746\n",
      "epoch 119 iter 56 loss=0.045314908027648926\n",
      "epoch 119 iter 57 loss=0.04835284501314163\n",
      "epoch 119 iter 58 loss=0.04472567141056061\n",
      "epoch 119 iter 59 loss=0.04707722365856171\n",
      "epoch 119 iter 60 loss=0.029557660222053528\n",
      "epoch 119 iter 61 loss=0.06699109077453613\n",
      "epoch 119 iter 62 loss=0.05624743178486824\n",
      "epoch 119 iter 63 loss=0.057895027101039886\n",
      "epoch 119 iter 64 loss=0.021952224895358086\n",
      "epoch 119 iter 65 loss=0.04615524038672447\n",
      "epoch 119 iter 66 loss=0.05825785920023918\n",
      "epoch 119 iter 67 loss=0.053282905369997025\n",
      "epoch 119 iter 68 loss=0.03761971741914749\n",
      "epoch 119 iter 69 loss=0.0362246073782444\n",
      "epoch 119 iter 70 loss=0.0488414391875267\n",
      "epoch 119 iter 71 loss=0.05728902667760849\n",
      "epoch 119 iter 72 loss=0.036229368299245834\n",
      "epoch 119 iter 73 loss=0.04306543618440628\n",
      "epoch 119 iter 74 loss=0.033471208065748215\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3302.\n",
      "epoch 120 iter 0 loss=0.05151411518454552\n",
      "epoch 120 iter 1 loss=0.04777202755212784\n",
      "epoch 120 iter 2 loss=0.05288811773061752\n",
      "epoch 120 iter 3 loss=0.03556791692972183\n",
      "epoch 120 iter 4 loss=0.03684741631150246\n",
      "epoch 120 iter 5 loss=0.04687809944152832\n",
      "epoch 120 iter 6 loss=0.06186690554022789\n",
      "epoch 120 iter 7 loss=0.036052677780389786\n",
      "epoch 120 iter 8 loss=0.05076006054878235\n",
      "epoch 120 iter 9 loss=0.02199232392013073\n",
      "epoch 120 iter 10 loss=0.04995650798082352\n",
      "epoch 120 iter 11 loss=0.05701418220996857\n",
      "epoch 120 iter 12 loss=0.047111984342336655\n",
      "epoch 120 iter 13 loss=0.04623693600296974\n",
      "epoch 120 iter 14 loss=0.06766249239444733\n",
      "epoch 120 iter 15 loss=0.04266532137989998\n",
      "epoch 120 iter 16 loss=0.0350366085767746\n",
      "epoch 120 iter 17 loss=0.04792223125696182\n",
      "epoch 120 iter 18 loss=0.04391555115580559\n",
      "epoch 120 iter 19 loss=0.04105106368660927\n",
      "epoch 120 iter 20 loss=0.02571105770766735\n",
      "epoch 120 iter 21 loss=0.04019491747021675\n",
      "epoch 120 iter 22 loss=0.028387418016791344\n",
      "epoch 120 iter 23 loss=0.039083972573280334\n",
      "epoch 120 iter 24 loss=0.04437924921512604\n",
      "epoch 120 iter 25 loss=0.0351862907409668\n",
      "epoch 120 iter 26 loss=0.03887796029448509\n",
      "epoch 120 iter 27 loss=0.026600848883390427\n",
      "epoch 120 iter 28 loss=0.04962851479649544\n",
      "epoch 120 iter 29 loss=0.035976212471723557\n",
      "epoch 120 iter 30 loss=0.05336127057671547\n",
      "epoch 120 iter 31 loss=0.03455323353409767\n",
      "epoch 120 iter 32 loss=0.048360634595155716\n",
      "epoch 120 iter 33 loss=0.06128266453742981\n",
      "epoch 120 iter 34 loss=0.03278711810708046\n",
      "epoch 120 iter 35 loss=0.054642047733068466\n",
      "epoch 120 iter 36 loss=0.0412871316075325\n",
      "epoch 120 iter 37 loss=0.048568084836006165\n",
      "epoch 120 iter 38 loss=0.039477214217185974\n",
      "epoch 120 iter 39 loss=0.05011685565114021\n",
      "epoch 120 iter 40 loss=0.03785461559891701\n",
      "epoch 120 iter 41 loss=0.02326376363635063\n",
      "epoch 120 iter 42 loss=0.05215627700090408\n",
      "epoch 120 iter 43 loss=0.05136345326900482\n",
      "epoch 120 iter 44 loss=0.049176692962646484\n",
      "epoch 120 iter 45 loss=0.030813070014119148\n",
      "epoch 120 iter 46 loss=0.040798064321279526\n",
      "epoch 120 iter 47 loss=0.07283530384302139\n",
      "epoch 120 iter 48 loss=0.0647675096988678\n",
      "epoch 120 iter 49 loss=0.06265363097190857\n",
      "epoch 120 iter 50 loss=0.04289267584681511\n",
      "epoch 120 iter 51 loss=0.022065602242946625\n",
      "epoch 120 iter 52 loss=0.027465498074889183\n",
      "epoch 120 iter 53 loss=0.03090456873178482\n",
      "epoch 120 iter 54 loss=0.028826171532273293\n",
      "epoch 120 iter 55 loss=0.04128871113061905\n",
      "epoch 120 iter 56 loss=0.05532902106642723\n",
      "epoch 120 iter 57 loss=0.027710897848010063\n",
      "epoch 120 iter 58 loss=0.06098903343081474\n",
      "epoch 120 iter 59 loss=0.04899352788925171\n",
      "epoch 120 iter 60 loss=0.06079667806625366\n",
      "epoch 120 iter 61 loss=0.03730632737278938\n",
      "epoch 120 iter 62 loss=0.054968614131212234\n",
      "epoch 120 iter 63 loss=0.03185264766216278\n",
      "epoch 120 iter 64 loss=0.04009523242712021\n",
      "epoch 120 iter 65 loss=0.0374290831387043\n",
      "epoch 120 iter 66 loss=0.02399599179625511\n",
      "epoch 120 iter 67 loss=0.059406280517578125\n",
      "epoch 120 iter 68 loss=0.0590987391769886\n",
      "epoch 120 iter 69 loss=0.09438007324934006\n",
      "epoch 120 iter 70 loss=0.06517521291971207\n",
      "epoch 120 iter 71 loss=0.05807448923587799\n",
      "epoch 120 iter 72 loss=0.05754753202199936\n",
      "epoch 120 iter 73 loss=0.0414123609662056\n",
      "epoch 120 iter 74 loss=0.07254130393266678\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3274.\n",
      "epoch 121 iter 0 loss=0.03759162127971649\n",
      "epoch 121 iter 1 loss=0.04418039694428444\n",
      "epoch 121 iter 2 loss=0.05072428658604622\n",
      "epoch 121 iter 3 loss=0.05320917069911957\n",
      "epoch 121 iter 4 loss=0.06316233426332474\n",
      "epoch 121 iter 5 loss=0.05893084406852722\n",
      "epoch 121 iter 6 loss=0.04679325595498085\n",
      "epoch 121 iter 7 loss=0.02903471328318119\n",
      "epoch 121 iter 8 loss=0.039774488657712936\n",
      "epoch 121 iter 9 loss=0.04194488376379013\n",
      "epoch 121 iter 10 loss=0.042086075991392136\n",
      "epoch 121 iter 11 loss=0.025212306529283524\n",
      "epoch 121 iter 12 loss=0.03119034878909588\n",
      "epoch 121 iter 13 loss=0.02829853445291519\n",
      "epoch 121 iter 14 loss=0.06343375146389008\n",
      "epoch 121 iter 15 loss=0.045060671865940094\n",
      "epoch 121 iter 16 loss=0.048699844628572464\n",
      "epoch 121 iter 17 loss=0.03841554746031761\n",
      "epoch 121 iter 18 loss=0.03452782705426216\n",
      "epoch 121 iter 19 loss=0.04438070207834244\n",
      "epoch 121 iter 20 loss=0.03260698541998863\n",
      "epoch 121 iter 21 loss=0.048674121499061584\n",
      "epoch 121 iter 22 loss=0.049307920038700104\n",
      "epoch 121 iter 23 loss=0.05363161861896515\n",
      "epoch 121 iter 24 loss=0.03841741383075714\n",
      "epoch 121 iter 25 loss=0.041788071393966675\n",
      "epoch 121 iter 26 loss=0.04312587529420853\n",
      "epoch 121 iter 27 loss=0.057668041437864304\n",
      "epoch 121 iter 28 loss=0.03764338046312332\n",
      "epoch 121 iter 29 loss=0.06386050581932068\n",
      "epoch 121 iter 30 loss=0.06452807039022446\n",
      "epoch 121 iter 31 loss=0.04182598367333412\n",
      "epoch 121 iter 32 loss=0.06666168570518494\n",
      "epoch 121 iter 33 loss=0.031017404049634933\n",
      "epoch 121 iter 34 loss=0.0383036844432354\n",
      "epoch 121 iter 35 loss=0.05359870567917824\n",
      "epoch 121 iter 36 loss=0.05962097644805908\n",
      "epoch 121 iter 37 loss=0.05236326903104782\n",
      "epoch 121 iter 38 loss=0.034190833568573\n",
      "epoch 121 iter 39 loss=0.046009764075279236\n",
      "epoch 121 iter 40 loss=0.03181251510977745\n",
      "epoch 121 iter 41 loss=0.0518869012594223\n",
      "epoch 121 iter 42 loss=0.04764430969953537\n",
      "epoch 121 iter 43 loss=0.020954933017492294\n",
      "epoch 121 iter 44 loss=0.057988181710243225\n",
      "epoch 121 iter 45 loss=0.05649762600660324\n",
      "epoch 121 iter 46 loss=0.026062004268169403\n",
      "epoch 121 iter 47 loss=0.04497123137116432\n",
      "epoch 121 iter 48 loss=0.06432565301656723\n",
      "epoch 121 iter 49 loss=0.0474657379090786\n",
      "epoch 121 iter 50 loss=0.04907182604074478\n",
      "epoch 121 iter 51 loss=0.030644819140434265\n",
      "epoch 121 iter 52 loss=0.0273613091558218\n",
      "epoch 121 iter 53 loss=0.045792315155267715\n",
      "epoch 121 iter 54 loss=0.06727253645658493\n",
      "epoch 121 iter 55 loss=0.037740908563137054\n",
      "epoch 121 iter 56 loss=0.06445382535457611\n",
      "epoch 121 iter 57 loss=0.057127393782138824\n",
      "epoch 121 iter 58 loss=0.06493687629699707\n",
      "epoch 121 iter 59 loss=0.03674741089344025\n",
      "epoch 121 iter 60 loss=0.024951471015810966\n",
      "epoch 121 iter 61 loss=0.05348607525229454\n",
      "epoch 121 iter 62 loss=0.06392254680395126\n",
      "epoch 121 iter 63 loss=0.049869440495967865\n",
      "epoch 121 iter 64 loss=0.03232012689113617\n",
      "epoch 121 iter 65 loss=0.03492913022637367\n",
      "epoch 121 iter 66 loss=0.026876112446188927\n",
      "epoch 121 iter 67 loss=0.04021997004747391\n",
      "epoch 121 iter 68 loss=0.05803960934281349\n",
      "epoch 121 iter 69 loss=0.048439957201480865\n",
      "epoch 121 iter 70 loss=0.05376293137669563\n",
      "epoch 121 iter 71 loss=0.03004695661365986\n",
      "epoch 121 iter 72 loss=0.062018901109695435\n",
      "epoch 121 iter 73 loss=0.07679656147956848\n",
      "epoch 121 iter 74 loss=0.037318985909223557\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3308.\n",
      "epoch 122 iter 0 loss=0.028664030134677887\n",
      "epoch 122 iter 1 loss=0.03871239721775055\n",
      "epoch 122 iter 2 loss=0.03427832946181297\n",
      "epoch 122 iter 3 loss=0.042046237736940384\n",
      "epoch 122 iter 4 loss=0.05355340614914894\n",
      "epoch 122 iter 5 loss=0.042839258909225464\n",
      "epoch 122 iter 6 loss=0.02899852767586708\n",
      "epoch 122 iter 7 loss=0.043810512870550156\n",
      "epoch 122 iter 8 loss=0.02432495541870594\n",
      "epoch 122 iter 9 loss=0.05793493613600731\n",
      "epoch 122 iter 10 loss=0.03855334222316742\n",
      "epoch 122 iter 11 loss=0.03700224682688713\n",
      "epoch 122 iter 12 loss=0.0713413655757904\n",
      "epoch 122 iter 13 loss=0.06784889101982117\n",
      "epoch 122 iter 14 loss=0.05669539049267769\n",
      "epoch 122 iter 15 loss=0.04893454909324646\n",
      "epoch 122 iter 16 loss=0.06020708754658699\n",
      "epoch 122 iter 17 loss=0.05413659289479256\n",
      "epoch 122 iter 18 loss=0.03632077947258949\n",
      "epoch 122 iter 19 loss=0.03156980872154236\n",
      "epoch 122 iter 20 loss=0.04529213905334473\n",
      "epoch 122 iter 21 loss=0.03183624520897865\n",
      "epoch 122 iter 22 loss=0.026368029415607452\n",
      "epoch 122 iter 23 loss=0.028949003666639328\n",
      "epoch 122 iter 24 loss=0.049386367201805115\n",
      "epoch 122 iter 25 loss=0.03779187798500061\n",
      "epoch 122 iter 26 loss=0.026055125519633293\n",
      "epoch 122 iter 27 loss=0.06493733823299408\n",
      "epoch 122 iter 28 loss=0.024094294756650925\n",
      "epoch 122 iter 29 loss=0.06768619269132614\n",
      "epoch 122 iter 30 loss=0.03141676262021065\n",
      "epoch 122 iter 31 loss=0.030709894374012947\n",
      "epoch 122 iter 32 loss=0.046086542308330536\n",
      "epoch 122 iter 33 loss=0.0498511828482151\n",
      "epoch 122 iter 34 loss=0.032322630286216736\n",
      "epoch 122 iter 35 loss=0.04431816563010216\n",
      "epoch 122 iter 36 loss=0.03939218446612358\n",
      "epoch 122 iter 37 loss=0.0650998055934906\n",
      "epoch 122 iter 38 loss=0.042513202875852585\n",
      "epoch 122 iter 39 loss=0.043089788407087326\n",
      "epoch 122 iter 40 loss=0.04834665358066559\n",
      "epoch 122 iter 41 loss=0.03878260776400566\n",
      "epoch 122 iter 42 loss=0.05122188478708267\n",
      "epoch 122 iter 43 loss=0.03759954497218132\n",
      "epoch 122 iter 44 loss=0.05148468539118767\n",
      "epoch 122 iter 45 loss=0.034384604543447495\n",
      "epoch 122 iter 46 loss=0.03276072070002556\n",
      "epoch 122 iter 47 loss=0.04129216447472572\n",
      "epoch 122 iter 48 loss=0.03709513694047928\n",
      "epoch 122 iter 49 loss=0.05887866020202637\n",
      "epoch 122 iter 50 loss=0.04791811853647232\n",
      "epoch 122 iter 51 loss=0.0649799108505249\n",
      "epoch 122 iter 52 loss=0.028838926926255226\n",
      "epoch 122 iter 53 loss=0.04515044018626213\n",
      "epoch 122 iter 54 loss=0.029267149046063423\n",
      "epoch 122 iter 55 loss=0.04587005823850632\n",
      "epoch 122 iter 56 loss=0.06028945371508598\n",
      "epoch 122 iter 57 loss=0.032367244362831116\n",
      "epoch 122 iter 58 loss=0.050129979848861694\n",
      "epoch 122 iter 59 loss=0.06583387404680252\n",
      "epoch 122 iter 60 loss=0.031236154958605766\n",
      "epoch 122 iter 61 loss=0.058523114770650864\n",
      "epoch 122 iter 62 loss=0.04155265539884567\n",
      "epoch 122 iter 63 loss=0.05298956483602524\n",
      "epoch 122 iter 64 loss=0.05800549313426018\n",
      "epoch 122 iter 65 loss=0.05349669232964516\n",
      "epoch 122 iter 66 loss=0.05119558796286583\n",
      "epoch 122 iter 67 loss=0.03454317897558212\n",
      "epoch 122 iter 68 loss=0.062426306307315826\n",
      "epoch 122 iter 69 loss=0.053379811346530914\n",
      "epoch 122 iter 70 loss=0.06742306053638458\n",
      "epoch 122 iter 71 loss=0.03300881385803223\n",
      "epoch 122 iter 72 loss=0.05030408129096031\n",
      "epoch 122 iter 73 loss=0.034985270351171494\n",
      "epoch 122 iter 74 loss=0.03907385095953941\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3226.\n",
      "epoch 123 iter 0 loss=0.03697751834988594\n",
      "epoch 123 iter 1 loss=0.04868384823203087\n",
      "epoch 123 iter 2 loss=0.061760082840919495\n",
      "epoch 123 iter 3 loss=0.04527134820818901\n",
      "epoch 123 iter 4 loss=0.04994148388504982\n",
      "epoch 123 iter 5 loss=0.03387646749615669\n",
      "epoch 123 iter 6 loss=0.03292952850461006\n",
      "epoch 123 iter 7 loss=0.02502691000699997\n",
      "epoch 123 iter 8 loss=0.05876261368393898\n",
      "epoch 123 iter 9 loss=0.0713115707039833\n",
      "epoch 123 iter 10 loss=0.032258301973342896\n",
      "epoch 123 iter 11 loss=0.04631873965263367\n",
      "epoch 123 iter 12 loss=0.05162939056754112\n",
      "epoch 123 iter 13 loss=0.055600058287382126\n",
      "epoch 123 iter 14 loss=0.04656395688652992\n",
      "epoch 123 iter 15 loss=0.0630909651517868\n",
      "epoch 123 iter 16 loss=0.07039868831634521\n",
      "epoch 123 iter 17 loss=0.059629254043102264\n",
      "epoch 123 iter 18 loss=0.03792669624090195\n",
      "epoch 123 iter 19 loss=0.02381579391658306\n",
      "epoch 123 iter 20 loss=0.03147716075181961\n",
      "epoch 123 iter 21 loss=0.03733803704380989\n",
      "epoch 123 iter 22 loss=0.07901524752378464\n",
      "epoch 123 iter 23 loss=0.04772843047976494\n",
      "epoch 123 iter 24 loss=0.037789907306432724\n",
      "epoch 123 iter 25 loss=0.044982727617025375\n",
      "epoch 123 iter 26 loss=0.041012149304151535\n",
      "epoch 123 iter 27 loss=0.047360628843307495\n",
      "epoch 123 iter 28 loss=0.04376426339149475\n",
      "epoch 123 iter 29 loss=0.041888393461704254\n",
      "epoch 123 iter 30 loss=0.02733611688017845\n",
      "epoch 123 iter 31 loss=0.03838375583291054\n",
      "epoch 123 iter 32 loss=0.05460764095187187\n",
      "epoch 123 iter 33 loss=0.05081813782453537\n",
      "epoch 123 iter 34 loss=0.024097800254821777\n",
      "epoch 123 iter 35 loss=0.04545675590634346\n",
      "epoch 123 iter 36 loss=0.05605702847242355\n",
      "epoch 123 iter 37 loss=0.0413140170276165\n",
      "epoch 123 iter 38 loss=0.049283817410469055\n",
      "epoch 123 iter 39 loss=0.04190373420715332\n",
      "epoch 123 iter 40 loss=0.0293559692800045\n",
      "epoch 123 iter 41 loss=0.03248542919754982\n",
      "epoch 123 iter 42 loss=0.04171395301818848\n",
      "epoch 123 iter 43 loss=0.05243619903922081\n",
      "epoch 123 iter 44 loss=0.04223107919096947\n",
      "epoch 123 iter 45 loss=0.04239247366786003\n",
      "epoch 123 iter 46 loss=0.045045506209135056\n",
      "epoch 123 iter 47 loss=0.03843912482261658\n",
      "epoch 123 iter 48 loss=0.035127948969602585\n",
      "epoch 123 iter 49 loss=0.0555451437830925\n",
      "epoch 123 iter 50 loss=0.03650369122624397\n",
      "epoch 123 iter 51 loss=0.050013355910778046\n",
      "epoch 123 iter 52 loss=0.0326872318983078\n",
      "epoch 123 iter 53 loss=0.05794816464185715\n",
      "epoch 123 iter 54 loss=0.045949194580316544\n",
      "epoch 123 iter 55 loss=0.06505510956048965\n",
      "epoch 123 iter 56 loss=0.045258503407239914\n",
      "epoch 123 iter 57 loss=0.05150061473250389\n",
      "epoch 123 iter 58 loss=0.05185623839497566\n",
      "epoch 123 iter 59 loss=0.050828881561756134\n",
      "epoch 123 iter 60 loss=0.04009944200515747\n",
      "epoch 123 iter 61 loss=0.06599225103855133\n",
      "epoch 123 iter 62 loss=0.035091038793325424\n",
      "epoch 123 iter 63 loss=0.04143577441573143\n",
      "epoch 123 iter 64 loss=0.02821684442460537\n",
      "epoch 123 iter 65 loss=0.037387099117040634\n",
      "epoch 123 iter 66 loss=0.051986437290906906\n",
      "epoch 123 iter 67 loss=0.043873131275177\n",
      "epoch 123 iter 68 loss=0.022922024130821228\n",
      "epoch 123 iter 69 loss=0.02208954468369484\n",
      "epoch 123 iter 70 loss=0.044120870530605316\n",
      "epoch 123 iter 71 loss=0.03741161525249481\n",
      "epoch 123 iter 72 loss=0.05206684768199921\n",
      "epoch 123 iter 73 loss=0.047702617943286896\n",
      "epoch 123 iter 74 loss=0.04538378491997719\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3216.\n",
      "epoch 124 iter 0 loss=0.054868463426828384\n",
      "epoch 124 iter 1 loss=0.043777838349342346\n",
      "epoch 124 iter 2 loss=0.03622815012931824\n",
      "epoch 124 iter 3 loss=0.05596911907196045\n",
      "epoch 124 iter 4 loss=0.0580422505736351\n",
      "epoch 124 iter 5 loss=0.03952525183558464\n",
      "epoch 124 iter 6 loss=0.04751336947083473\n",
      "epoch 124 iter 7 loss=0.052923258394002914\n",
      "epoch 124 iter 8 loss=0.03532451018691063\n",
      "epoch 124 iter 9 loss=0.04944269731640816\n",
      "epoch 124 iter 10 loss=0.05316391587257385\n",
      "epoch 124 iter 11 loss=0.05112616717815399\n",
      "epoch 124 iter 12 loss=0.034119755029678345\n",
      "epoch 124 iter 13 loss=0.04524915665388107\n",
      "epoch 124 iter 14 loss=0.04895344376564026\n",
      "epoch 124 iter 15 loss=0.05779498443007469\n",
      "epoch 124 iter 16 loss=0.04782108962535858\n",
      "epoch 124 iter 17 loss=0.03636173903942108\n",
      "epoch 124 iter 18 loss=0.03800801560282707\n",
      "epoch 124 iter 19 loss=0.0676269680261612\n",
      "epoch 124 iter 20 loss=0.049321576952934265\n",
      "epoch 124 iter 21 loss=0.028770502656698227\n",
      "epoch 124 iter 22 loss=0.04999138414859772\n",
      "epoch 124 iter 23 loss=0.026674821972846985\n",
      "epoch 124 iter 24 loss=0.05152921378612518\n",
      "epoch 124 iter 25 loss=0.024674061685800552\n",
      "epoch 124 iter 26 loss=0.04650536924600601\n",
      "epoch 124 iter 27 loss=0.03562776744365692\n",
      "epoch 124 iter 28 loss=0.03358696028590202\n",
      "epoch 124 iter 29 loss=0.033627212047576904\n",
      "epoch 124 iter 30 loss=0.036367274820804596\n",
      "epoch 124 iter 31 loss=0.056067001074552536\n",
      "epoch 124 iter 32 loss=0.02289607748389244\n",
      "epoch 124 iter 33 loss=0.03152686357498169\n",
      "epoch 124 iter 34 loss=0.07044409960508347\n",
      "epoch 124 iter 35 loss=0.05451539158821106\n",
      "epoch 124 iter 36 loss=0.033337902277708054\n",
      "epoch 124 iter 37 loss=0.05915232375264168\n",
      "epoch 124 iter 38 loss=0.033512841910123825\n",
      "epoch 124 iter 39 loss=0.030816301703453064\n",
      "epoch 124 iter 40 loss=0.03851442039012909\n",
      "epoch 124 iter 41 loss=0.0375271737575531\n",
      "epoch 124 iter 42 loss=0.02340359054505825\n",
      "epoch 124 iter 43 loss=0.03860807791352272\n",
      "epoch 124 iter 44 loss=0.06618548929691315\n",
      "epoch 124 iter 45 loss=0.04060206562280655\n",
      "epoch 124 iter 46 loss=0.05563734099268913\n",
      "epoch 124 iter 47 loss=0.0315462164580822\n",
      "epoch 124 iter 48 loss=0.018995802849531174\n",
      "epoch 124 iter 49 loss=0.055169060826301575\n",
      "epoch 124 iter 50 loss=0.03466973453760147\n",
      "epoch 124 iter 51 loss=0.040975917130708694\n",
      "epoch 124 iter 52 loss=0.07406934350728989\n",
      "epoch 124 iter 53 loss=0.02474939078092575\n",
      "epoch 124 iter 54 loss=0.05673499032855034\n",
      "epoch 124 iter 55 loss=0.03285861387848854\n",
      "epoch 124 iter 56 loss=0.04330206289887428\n",
      "epoch 124 iter 57 loss=0.05676598846912384\n",
      "epoch 124 iter 58 loss=0.04202299192547798\n",
      "epoch 124 iter 59 loss=0.04907059669494629\n",
      "epoch 124 iter 60 loss=0.04251603037118912\n",
      "epoch 124 iter 61 loss=0.043792735785245895\n",
      "epoch 124 iter 62 loss=0.029350319877266884\n",
      "epoch 124 iter 63 loss=0.05002298206090927\n",
      "epoch 124 iter 64 loss=0.059249360114336014\n",
      "epoch 124 iter 65 loss=0.04875905066728592\n",
      "epoch 124 iter 66 loss=0.059503212571144104\n",
      "epoch 124 iter 67 loss=0.037642721086740494\n",
      "epoch 124 iter 68 loss=0.049766212701797485\n",
      "epoch 124 iter 69 loss=0.039341580122709274\n",
      "epoch 124 iter 70 loss=0.04226074367761612\n",
      "epoch 124 iter 71 loss=0.04539724811911583\n",
      "epoch 124 iter 72 loss=0.04603653401136398\n",
      "epoch 124 iter 73 loss=0.04120612144470215\n",
      "epoch 124 iter 74 loss=0.03798966482281685\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3212.\n",
      "epoch 125 iter 0 loss=0.028867358341813087\n",
      "epoch 125 iter 1 loss=0.028826691210269928\n",
      "epoch 125 iter 2 loss=0.05053859204053879\n",
      "epoch 125 iter 3 loss=0.03711910918354988\n",
      "epoch 125 iter 4 loss=0.03476737067103386\n",
      "epoch 125 iter 5 loss=0.042215198278427124\n",
      "epoch 125 iter 6 loss=0.02471054345369339\n",
      "epoch 125 iter 7 loss=0.042129747569561005\n",
      "epoch 125 iter 8 loss=0.03811079263687134\n",
      "epoch 125 iter 9 loss=0.03434065729379654\n",
      "epoch 125 iter 10 loss=0.03541082516312599\n",
      "epoch 125 iter 11 loss=0.03813117370009422\n",
      "epoch 125 iter 12 loss=0.034242622554302216\n",
      "epoch 125 iter 13 loss=0.054637107998132706\n",
      "epoch 125 iter 14 loss=0.0438687801361084\n",
      "epoch 125 iter 15 loss=0.023020455613732338\n",
      "epoch 125 iter 16 loss=0.042382750660181046\n",
      "epoch 125 iter 17 loss=0.06034105271100998\n",
      "epoch 125 iter 18 loss=0.0507710762321949\n",
      "epoch 125 iter 19 loss=0.05228617042303085\n",
      "epoch 125 iter 20 loss=0.042177990078926086\n",
      "epoch 125 iter 21 loss=0.05489824712276459\n",
      "epoch 125 iter 22 loss=0.056925851851701736\n",
      "epoch 125 iter 23 loss=0.06053104251623154\n",
      "epoch 125 iter 24 loss=0.03937269002199173\n",
      "epoch 125 iter 25 loss=0.04535127058625221\n",
      "epoch 125 iter 26 loss=0.056084971874952316\n",
      "epoch 125 iter 27 loss=0.0457075797021389\n",
      "epoch 125 iter 28 loss=0.04517326131463051\n",
      "epoch 125 iter 29 loss=0.05273686721920967\n",
      "epoch 125 iter 30 loss=0.02556707337498665\n",
      "epoch 125 iter 31 loss=0.021074259653687477\n",
      "epoch 125 iter 32 loss=0.050229333341121674\n",
      "epoch 125 iter 33 loss=0.04723427817225456\n",
      "epoch 125 iter 34 loss=0.02740071341395378\n",
      "epoch 125 iter 35 loss=0.05150686576962471\n",
      "epoch 125 iter 36 loss=0.02553516812622547\n",
      "epoch 125 iter 37 loss=0.02599468268454075\n",
      "epoch 125 iter 38 loss=0.061050139367580414\n",
      "epoch 125 iter 39 loss=0.06352689862251282\n",
      "epoch 125 iter 40 loss=0.0450507327914238\n",
      "epoch 125 iter 41 loss=0.035585254430770874\n",
      "epoch 125 iter 42 loss=0.05553094670176506\n",
      "epoch 125 iter 43 loss=0.06047133356332779\n",
      "epoch 125 iter 44 loss=0.0327659547328949\n",
      "epoch 125 iter 45 loss=0.023382628336548805\n",
      "epoch 125 iter 46 loss=0.053103163838386536\n",
      "epoch 125 iter 47 loss=0.02711462415754795\n",
      "epoch 125 iter 48 loss=0.05595516785979271\n",
      "epoch 125 iter 49 loss=0.07019012421369553\n",
      "epoch 125 iter 50 loss=0.0453776940703392\n",
      "epoch 125 iter 51 loss=0.0411897636950016\n",
      "epoch 125 iter 52 loss=0.03510374203324318\n",
      "epoch 125 iter 53 loss=0.0455278679728508\n",
      "epoch 125 iter 54 loss=0.042197078466415405\n",
      "epoch 125 iter 55 loss=0.044401925057172775\n",
      "epoch 125 iter 56 loss=0.048791512846946716\n",
      "epoch 125 iter 57 loss=0.04333900660276413\n",
      "epoch 125 iter 58 loss=0.03687874600291252\n",
      "epoch 125 iter 59 loss=0.027872344478964806\n",
      "epoch 125 iter 60 loss=0.041596271097660065\n",
      "epoch 125 iter 61 loss=0.05607372522354126\n",
      "epoch 125 iter 62 loss=0.059629958122968674\n",
      "epoch 125 iter 63 loss=0.036530736833810806\n",
      "epoch 125 iter 64 loss=0.035888709127902985\n",
      "epoch 125 iter 65 loss=0.04050774127244949\n",
      "epoch 125 iter 66 loss=0.06593021005392075\n",
      "epoch 125 iter 67 loss=0.059378888458013535\n",
      "epoch 125 iter 68 loss=0.03736896812915802\n",
      "epoch 125 iter 69 loss=0.050393275916576385\n",
      "epoch 125 iter 70 loss=0.04972325265407562\n",
      "epoch 125 iter 71 loss=0.04683231562376022\n",
      "epoch 125 iter 72 loss=0.05124001204967499\n",
      "epoch 125 iter 73 loss=0.03564158454537392\n",
      "epoch 125 iter 74 loss=0.04812459647655487\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3218.\n",
      "epoch 126 iter 0 loss=0.02885160595178604\n",
      "epoch 126 iter 1 loss=0.046487852931022644\n",
      "epoch 126 iter 2 loss=0.0358111672103405\n",
      "epoch 126 iter 3 loss=0.030020924285054207\n",
      "epoch 126 iter 4 loss=0.04840075969696045\n",
      "epoch 126 iter 5 loss=0.022240852937102318\n",
      "epoch 126 iter 6 loss=0.057260673493146896\n",
      "epoch 126 iter 7 loss=0.032196011394262314\n",
      "epoch 126 iter 8 loss=0.04740734398365021\n",
      "epoch 126 iter 9 loss=0.0557192824780941\n",
      "epoch 126 iter 10 loss=0.03050740249454975\n",
      "epoch 126 iter 11 loss=0.055413246154785156\n",
      "epoch 126 iter 12 loss=0.04567335173487663\n",
      "epoch 126 iter 13 loss=0.042997073382139206\n",
      "epoch 126 iter 14 loss=0.037412792444229126\n",
      "epoch 126 iter 15 loss=0.03538902848958969\n",
      "epoch 126 iter 16 loss=0.08582734316587448\n",
      "epoch 126 iter 17 loss=0.028886128216981888\n",
      "epoch 126 iter 18 loss=0.027119094505906105\n",
      "epoch 126 iter 19 loss=0.04090617969632149\n",
      "epoch 126 iter 20 loss=0.05927792936563492\n",
      "epoch 126 iter 21 loss=0.061920564621686935\n",
      "epoch 126 iter 22 loss=0.03872108832001686\n",
      "epoch 126 iter 23 loss=0.05010233819484711\n",
      "epoch 126 iter 24 loss=0.038948170840740204\n",
      "epoch 126 iter 25 loss=0.03111572563648224\n",
      "epoch 126 iter 26 loss=0.045708443969488144\n",
      "epoch 126 iter 27 loss=0.03660055622458458\n",
      "epoch 126 iter 28 loss=0.03700638562440872\n",
      "epoch 126 iter 29 loss=0.04481664299964905\n",
      "epoch 126 iter 30 loss=0.04648768901824951\n",
      "epoch 126 iter 31 loss=0.05832305923104286\n",
      "epoch 126 iter 32 loss=0.019820496439933777\n",
      "epoch 126 iter 33 loss=0.05186227336525917\n",
      "epoch 126 iter 34 loss=0.05450152978301048\n",
      "epoch 126 iter 35 loss=0.056570056825876236\n",
      "epoch 126 iter 36 loss=0.053482137620449066\n",
      "epoch 126 iter 37 loss=0.02545793168246746\n",
      "epoch 126 iter 38 loss=0.02226405404508114\n",
      "epoch 126 iter 39 loss=0.038024719804525375\n",
      "epoch 126 iter 40 loss=0.044832468032836914\n",
      "epoch 126 iter 41 loss=0.040236059576272964\n",
      "epoch 126 iter 42 loss=0.040516722947359085\n",
      "epoch 126 iter 43 loss=0.044988956302404404\n",
      "epoch 126 iter 44 loss=0.04636543244123459\n",
      "epoch 126 iter 45 loss=0.022144347429275513\n",
      "epoch 126 iter 46 loss=0.04105541482567787\n",
      "epoch 126 iter 47 loss=0.03195076063275337\n",
      "epoch 126 iter 48 loss=0.03612416610121727\n",
      "epoch 126 iter 49 loss=0.04268327355384827\n",
      "epoch 126 iter 50 loss=0.03277309611439705\n",
      "epoch 126 iter 51 loss=0.04996088519692421\n",
      "epoch 126 iter 52 loss=0.04902493208646774\n",
      "epoch 126 iter 53 loss=0.07983938604593277\n",
      "epoch 126 iter 54 loss=0.05673116818070412\n",
      "epoch 126 iter 55 loss=0.03730320557951927\n",
      "epoch 126 iter 56 loss=0.04181782156229019\n",
      "epoch 126 iter 57 loss=0.03785238042473793\n",
      "epoch 126 iter 58 loss=0.0228652935475111\n",
      "epoch 126 iter 59 loss=0.05215553566813469\n",
      "epoch 126 iter 60 loss=0.03319273889064789\n",
      "epoch 126 iter 61 loss=0.04301196336746216\n",
      "epoch 126 iter 62 loss=0.0656021386384964\n",
      "epoch 126 iter 63 loss=0.05205107107758522\n",
      "epoch 126 iter 64 loss=0.029297297820448875\n",
      "epoch 126 iter 65 loss=0.03880300745368004\n",
      "epoch 126 iter 66 loss=0.07168176770210266\n",
      "epoch 126 iter 67 loss=0.03311773017048836\n",
      "epoch 126 iter 68 loss=0.0605519637465477\n",
      "epoch 126 iter 69 loss=0.049127038568258286\n",
      "epoch 126 iter 70 loss=0.04210781306028366\n",
      "epoch 126 iter 71 loss=0.054152317345142365\n",
      "epoch 126 iter 72 loss=0.05689302831888199\n",
      "epoch 126 iter 73 loss=0.036638274788856506\n",
      "epoch 126 iter 74 loss=0.03428301215171814\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3092.\n",
      "epoch 127 iter 0 loss=0.030639315024018288\n",
      "epoch 127 iter 1 loss=0.060149725526571274\n",
      "epoch 127 iter 2 loss=0.051453813910484314\n",
      "epoch 127 iter 3 loss=0.046270981431007385\n",
      "epoch 127 iter 4 loss=0.031686101108789444\n",
      "epoch 127 iter 5 loss=0.04080913960933685\n",
      "epoch 127 iter 6 loss=0.04578101262450218\n",
      "epoch 127 iter 7 loss=0.044282086193561554\n",
      "epoch 127 iter 8 loss=0.06426870822906494\n",
      "epoch 127 iter 9 loss=0.022072061896324158\n",
      "epoch 127 iter 10 loss=0.06057470664381981\n",
      "epoch 127 iter 11 loss=0.053609080612659454\n",
      "epoch 127 iter 12 loss=0.05947599560022354\n",
      "epoch 127 iter 13 loss=0.05383998528122902\n",
      "epoch 127 iter 14 loss=0.031946420669555664\n",
      "epoch 127 iter 15 loss=0.04449482634663582\n",
      "epoch 127 iter 16 loss=0.03427240997552872\n",
      "epoch 127 iter 17 loss=0.05055202543735504\n",
      "epoch 127 iter 18 loss=0.04064660519361496\n",
      "epoch 127 iter 19 loss=0.04137809947133064\n",
      "epoch 127 iter 20 loss=0.04917167127132416\n",
      "epoch 127 iter 21 loss=0.04161153733730316\n",
      "epoch 127 iter 22 loss=0.03836534917354584\n",
      "epoch 127 iter 23 loss=0.054637983441352844\n",
      "epoch 127 iter 24 loss=0.029160484671592712\n",
      "epoch 127 iter 25 loss=0.05196405574679375\n",
      "epoch 127 iter 26 loss=0.05292653292417526\n",
      "epoch 127 iter 27 loss=0.05571608245372772\n",
      "epoch 127 iter 28 loss=0.034184977412223816\n",
      "epoch 127 iter 29 loss=0.03749470040202141\n",
      "epoch 127 iter 30 loss=0.04527774080634117\n",
      "epoch 127 iter 31 loss=0.05236281454563141\n",
      "epoch 127 iter 32 loss=0.048998042941093445\n",
      "epoch 127 iter 33 loss=0.039946578443050385\n",
      "epoch 127 iter 34 loss=0.03843991458415985\n",
      "epoch 127 iter 35 loss=0.04420752450823784\n",
      "epoch 127 iter 36 loss=0.05264677479863167\n",
      "epoch 127 iter 37 loss=0.024390649050474167\n",
      "epoch 127 iter 38 loss=0.032891832292079926\n",
      "epoch 127 iter 39 loss=0.06040766462683678\n",
      "epoch 127 iter 40 loss=0.039793506264686584\n",
      "epoch 127 iter 41 loss=0.030767135322093964\n",
      "epoch 127 iter 42 loss=0.07353994995355606\n",
      "epoch 127 iter 43 loss=0.05295996740460396\n",
      "epoch 127 iter 44 loss=0.05068858340382576\n",
      "epoch 127 iter 45 loss=0.033154141157865524\n",
      "epoch 127 iter 46 loss=0.034397415816783905\n",
      "epoch 127 iter 47 loss=0.06341931968927383\n",
      "epoch 127 iter 48 loss=0.03414621949195862\n",
      "epoch 127 iter 49 loss=0.018692482262849808\n",
      "epoch 127 iter 50 loss=0.04492190480232239\n",
      "epoch 127 iter 51 loss=0.04460565745830536\n",
      "epoch 127 iter 52 loss=0.04913020506501198\n",
      "epoch 127 iter 53 loss=0.028411660343408585\n",
      "epoch 127 iter 54 loss=0.037545353174209595\n",
      "epoch 127 iter 55 loss=0.033149201422929764\n",
      "epoch 127 iter 56 loss=0.03336478769779205\n",
      "epoch 127 iter 57 loss=0.042679935693740845\n",
      "epoch 127 iter 58 loss=0.0329645574092865\n",
      "epoch 127 iter 59 loss=0.02867371030151844\n",
      "epoch 127 iter 60 loss=0.028806893154978752\n",
      "epoch 127 iter 61 loss=0.0379578061401844\n",
      "epoch 127 iter 62 loss=0.04087645560503006\n",
      "epoch 127 iter 63 loss=0.04294218122959137\n",
      "epoch 127 iter 64 loss=0.03087950125336647\n",
      "epoch 127 iter 65 loss=0.04748734459280968\n",
      "epoch 127 iter 66 loss=0.0392722561955452\n",
      "epoch 127 iter 67 loss=0.025197315961122513\n",
      "epoch 127 iter 68 loss=0.03181334212422371\n",
      "epoch 127 iter 69 loss=0.057207442820072174\n",
      "epoch 127 iter 70 loss=0.054683834314346313\n",
      "epoch 127 iter 71 loss=0.05015026405453682\n",
      "epoch 127 iter 72 loss=0.06903699040412903\n",
      "epoch 127 iter 73 loss=0.04519225284457207\n",
      "epoch 127 iter 74 loss=0.041136156767606735\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3202.\n",
      "epoch 128 iter 0 loss=0.04888157173991203\n",
      "epoch 128 iter 1 loss=0.03799314796924591\n",
      "epoch 128 iter 2 loss=0.047186240553855896\n",
      "epoch 128 iter 3 loss=0.04347679391503334\n",
      "epoch 128 iter 4 loss=0.036395370960235596\n",
      "epoch 128 iter 5 loss=0.03994133323431015\n",
      "epoch 128 iter 6 loss=0.03502976521849632\n",
      "epoch 128 iter 7 loss=0.0459725484251976\n",
      "epoch 128 iter 8 loss=0.054916393011808395\n",
      "epoch 128 iter 9 loss=0.06202755868434906\n",
      "epoch 128 iter 10 loss=0.04407180845737457\n",
      "epoch 128 iter 11 loss=0.020453931763768196\n",
      "epoch 128 iter 12 loss=0.0456705205142498\n",
      "epoch 128 iter 13 loss=0.03730325400829315\n",
      "epoch 128 iter 14 loss=0.029499804601073265\n",
      "epoch 128 iter 15 loss=0.057112738490104675\n",
      "epoch 128 iter 16 loss=0.08402521908283234\n",
      "epoch 128 iter 17 loss=0.03095155395567417\n",
      "epoch 128 iter 18 loss=0.03839294612407684\n",
      "epoch 128 iter 19 loss=0.02099146693944931\n",
      "epoch 128 iter 20 loss=0.061505015939474106\n",
      "epoch 128 iter 21 loss=0.05150993540883064\n",
      "epoch 128 iter 22 loss=0.06403503566980362\n",
      "epoch 128 iter 23 loss=0.04506654664874077\n",
      "epoch 128 iter 24 loss=0.05145665258169174\n",
      "epoch 128 iter 25 loss=0.045328568667173386\n",
      "epoch 128 iter 26 loss=0.04550362750887871\n",
      "epoch 128 iter 27 loss=0.03658973425626755\n",
      "epoch 128 iter 28 loss=0.025572283193469048\n",
      "epoch 128 iter 29 loss=0.04192804917693138\n",
      "epoch 128 iter 30 loss=0.03957878053188324\n",
      "epoch 128 iter 31 loss=0.02759469300508499\n",
      "epoch 128 iter 32 loss=0.04472728446125984\n",
      "epoch 128 iter 33 loss=0.0486128106713295\n",
      "epoch 128 iter 34 loss=0.038720082491636276\n",
      "epoch 128 iter 35 loss=0.031489428132772446\n",
      "epoch 128 iter 36 loss=0.03867074474692345\n",
      "epoch 128 iter 37 loss=0.04057897999882698\n",
      "epoch 128 iter 38 loss=0.034690212458372116\n",
      "epoch 128 iter 39 loss=0.05589362978935242\n",
      "epoch 128 iter 40 loss=0.060499344021081924\n",
      "epoch 128 iter 41 loss=0.044417861849069595\n",
      "epoch 128 iter 42 loss=0.025042813271284103\n",
      "epoch 128 iter 43 loss=0.03185606747865677\n",
      "epoch 128 iter 44 loss=0.05903761461377144\n",
      "epoch 128 iter 45 loss=0.042661938816308975\n",
      "epoch 128 iter 46 loss=0.04083414375782013\n",
      "epoch 128 iter 47 loss=0.03697989508509636\n",
      "epoch 128 iter 48 loss=0.049135513603687286\n",
      "epoch 128 iter 49 loss=0.03785634785890579\n",
      "epoch 128 iter 50 loss=0.03172355890274048\n",
      "epoch 128 iter 51 loss=0.05543135106563568\n",
      "epoch 128 iter 52 loss=0.056115709245204926\n",
      "epoch 128 iter 53 loss=0.04810488224029541\n",
      "epoch 128 iter 54 loss=0.024128545075654984\n",
      "epoch 128 iter 55 loss=0.03724174201488495\n",
      "epoch 128 iter 56 loss=0.0374939851462841\n",
      "epoch 128 iter 57 loss=0.03156581148505211\n",
      "epoch 128 iter 58 loss=0.033475130796432495\n",
      "epoch 128 iter 59 loss=0.07184436917304993\n",
      "epoch 128 iter 60 loss=0.051011085510253906\n",
      "epoch 128 iter 61 loss=0.05733784660696983\n",
      "epoch 128 iter 62 loss=0.03722144290804863\n",
      "epoch 128 iter 63 loss=0.024797843769192696\n",
      "epoch 128 iter 64 loss=0.028877750039100647\n",
      "epoch 128 iter 65 loss=0.062124401330947876\n",
      "epoch 128 iter 66 loss=0.03390368074178696\n",
      "epoch 128 iter 67 loss=0.045464809983968735\n",
      "epoch 128 iter 68 loss=0.03960791230201721\n",
      "epoch 128 iter 69 loss=0.02889721654355526\n",
      "epoch 128 iter 70 loss=0.039839476346969604\n",
      "epoch 128 iter 71 loss=0.05319954827427864\n",
      "epoch 128 iter 72 loss=0.027854634448885918\n",
      "epoch 128 iter 73 loss=0.036045487970113754\n",
      "epoch 128 iter 74 loss=0.04647669196128845\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3196.\n",
      "epoch 129 iter 0 loss=0.043980587273836136\n",
      "epoch 129 iter 1 loss=0.0392853245139122\n",
      "epoch 129 iter 2 loss=0.054291266947984695\n",
      "epoch 129 iter 3 loss=0.05506933853030205\n",
      "epoch 129 iter 4 loss=0.0496600866317749\n",
      "epoch 129 iter 5 loss=0.03513703495264053\n",
      "epoch 129 iter 6 loss=0.0303043145686388\n",
      "epoch 129 iter 7 loss=0.05083692446351051\n",
      "epoch 129 iter 8 loss=0.03225444257259369\n",
      "epoch 129 iter 9 loss=0.023272378370165825\n",
      "epoch 129 iter 10 loss=0.03437729552388191\n",
      "epoch 129 iter 11 loss=0.0289214625954628\n",
      "epoch 129 iter 12 loss=0.05938739702105522\n",
      "epoch 129 iter 13 loss=0.04378839209675789\n",
      "epoch 129 iter 14 loss=0.028458556160330772\n",
      "epoch 129 iter 15 loss=0.02851030044257641\n",
      "epoch 129 iter 16 loss=0.050699520856142044\n",
      "epoch 129 iter 17 loss=0.05627920478582382\n",
      "epoch 129 iter 18 loss=0.043607939034700394\n",
      "epoch 129 iter 19 loss=0.046420563012361526\n",
      "epoch 129 iter 20 loss=0.04575590789318085\n",
      "epoch 129 iter 21 loss=0.0425553135573864\n",
      "epoch 129 iter 22 loss=0.03271540254354477\n",
      "epoch 129 iter 23 loss=0.04595122113823891\n",
      "epoch 129 iter 24 loss=0.028047705069184303\n",
      "epoch 129 iter 25 loss=0.04495474323630333\n",
      "epoch 129 iter 26 loss=0.03533465042710304\n",
      "epoch 129 iter 27 loss=0.02864048443734646\n",
      "epoch 129 iter 28 loss=0.03909188136458397\n",
      "epoch 129 iter 29 loss=0.043970078229904175\n",
      "epoch 129 iter 30 loss=0.054284341633319855\n",
      "epoch 129 iter 31 loss=0.026588736101984978\n",
      "epoch 129 iter 32 loss=0.056316062808036804\n",
      "epoch 129 iter 33 loss=0.03846368193626404\n",
      "epoch 129 iter 34 loss=0.027936790138483047\n",
      "epoch 129 iter 35 loss=0.03145884349942207\n",
      "epoch 129 iter 36 loss=0.03081684000790119\n",
      "epoch 129 iter 37 loss=0.036407168954610825\n",
      "epoch 129 iter 38 loss=0.030097801238298416\n",
      "epoch 129 iter 39 loss=0.04173121601343155\n",
      "epoch 129 iter 40 loss=0.04290563985705376\n",
      "epoch 129 iter 41 loss=0.05983123928308487\n",
      "epoch 129 iter 42 loss=0.03806741535663605\n",
      "epoch 129 iter 43 loss=0.039385609328746796\n",
      "epoch 129 iter 44 loss=0.039626508951187134\n",
      "epoch 129 iter 45 loss=0.04017014801502228\n",
      "epoch 129 iter 46 loss=0.06180991977453232\n",
      "epoch 129 iter 47 loss=0.046237435191869736\n",
      "epoch 129 iter 48 loss=0.034842994064092636\n",
      "epoch 129 iter 49 loss=0.039879512041807175\n",
      "epoch 129 iter 50 loss=0.04196655750274658\n",
      "epoch 129 iter 51 loss=0.05105476453900337\n",
      "epoch 129 iter 52 loss=0.030846575275063515\n",
      "epoch 129 iter 53 loss=0.028542617335915565\n",
      "epoch 129 iter 54 loss=0.04331367835402489\n",
      "epoch 129 iter 55 loss=0.055466942489147186\n",
      "epoch 129 iter 56 loss=0.03337891399860382\n",
      "epoch 129 iter 57 loss=0.060674358159303665\n",
      "epoch 129 iter 58 loss=0.048801373690366745\n",
      "epoch 129 iter 59 loss=0.026004090905189514\n",
      "epoch 129 iter 60 loss=0.04189854487776756\n",
      "epoch 129 iter 61 loss=0.03839762508869171\n",
      "epoch 129 iter 62 loss=0.04070695862174034\n",
      "epoch 129 iter 63 loss=0.06497902423143387\n",
      "epoch 129 iter 64 loss=0.039661865681409836\n",
      "epoch 129 iter 65 loss=0.05878328159451485\n",
      "epoch 129 iter 66 loss=0.04613190144300461\n",
      "epoch 129 iter 67 loss=0.03891180455684662\n",
      "epoch 129 iter 68 loss=0.03240460529923439\n",
      "epoch 129 iter 69 loss=0.04319680854678154\n",
      "epoch 129 iter 70 loss=0.05264706164598465\n",
      "epoch 129 iter 71 loss=0.027198852971196175\n",
      "epoch 129 iter 72 loss=0.026403162628412247\n",
      "epoch 129 iter 73 loss=0.06410317867994308\n",
      "epoch 129 iter 74 loss=0.045114580541849136\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3222.\n",
      "epoch 130 iter 0 loss=0.0633123442530632\n",
      "epoch 130 iter 1 loss=0.031344980001449585\n",
      "epoch 130 iter 2 loss=0.03982199728488922\n",
      "epoch 130 iter 3 loss=0.06168179586529732\n",
      "epoch 130 iter 4 loss=0.04233480244874954\n",
      "epoch 130 iter 5 loss=0.04779941961169243\n",
      "epoch 130 iter 6 loss=0.051348257809877396\n",
      "epoch 130 iter 7 loss=0.03318320959806442\n",
      "epoch 130 iter 8 loss=0.04218875244259834\n",
      "epoch 130 iter 9 loss=0.04620402306318283\n",
      "epoch 130 iter 10 loss=0.04210183769464493\n",
      "epoch 130 iter 11 loss=0.029827091842889786\n",
      "epoch 130 iter 12 loss=0.04185567796230316\n",
      "epoch 130 iter 13 loss=0.040321845561265945\n",
      "epoch 130 iter 14 loss=0.05024431645870209\n",
      "epoch 130 iter 15 loss=0.030604185536503792\n",
      "epoch 130 iter 16 loss=0.03001450002193451\n",
      "epoch 130 iter 17 loss=0.05341534689068794\n",
      "epoch 130 iter 18 loss=0.041623011231422424\n",
      "epoch 130 iter 19 loss=0.03347090259194374\n",
      "epoch 130 iter 20 loss=0.03310457617044449\n",
      "epoch 130 iter 21 loss=0.023279456421732903\n",
      "epoch 130 iter 22 loss=0.051294632256031036\n",
      "epoch 130 iter 23 loss=0.039274223148822784\n",
      "epoch 130 iter 24 loss=0.040238671004772186\n",
      "epoch 130 iter 25 loss=0.044019345194101334\n",
      "epoch 130 iter 26 loss=0.04444455727934837\n",
      "epoch 130 iter 27 loss=0.027053561061620712\n",
      "epoch 130 iter 28 loss=0.04319048300385475\n",
      "epoch 130 iter 29 loss=0.051908645778894424\n",
      "epoch 130 iter 30 loss=0.058613453060388565\n",
      "epoch 130 iter 31 loss=0.029116131365299225\n",
      "epoch 130 iter 32 loss=0.0334400050342083\n",
      "epoch 130 iter 33 loss=0.025668514892458916\n",
      "epoch 130 iter 34 loss=0.023844921961426735\n",
      "epoch 130 iter 35 loss=0.0522749237716198\n",
      "epoch 130 iter 36 loss=0.04222001135349274\n",
      "epoch 130 iter 37 loss=0.024997921660542488\n",
      "epoch 130 iter 38 loss=0.02686019241809845\n",
      "epoch 130 iter 39 loss=0.0425199493765831\n",
      "epoch 130 iter 40 loss=0.031036173924803734\n",
      "epoch 130 iter 41 loss=0.02551143430173397\n",
      "epoch 130 iter 42 loss=0.04481224715709686\n",
      "epoch 130 iter 43 loss=0.031051209196448326\n",
      "epoch 130 iter 44 loss=0.0545739084482193\n",
      "epoch 130 iter 45 loss=0.03723657876253128\n",
      "epoch 130 iter 46 loss=0.025003734976053238\n",
      "epoch 130 iter 47 loss=0.06977439671754837\n",
      "epoch 130 iter 48 loss=0.044570885598659515\n",
      "epoch 130 iter 49 loss=0.0347101166844368\n",
      "epoch 130 iter 50 loss=0.044731106609106064\n",
      "epoch 130 iter 51 loss=0.058585263788700104\n",
      "epoch 130 iter 52 loss=0.05521261692047119\n",
      "epoch 130 iter 53 loss=0.04318079352378845\n",
      "epoch 130 iter 54 loss=0.02312650717794895\n",
      "epoch 130 iter 55 loss=0.04599570110440254\n",
      "epoch 130 iter 56 loss=0.030198030173778534\n",
      "epoch 130 iter 57 loss=0.04395851492881775\n",
      "epoch 130 iter 58 loss=0.057992398738861084\n",
      "epoch 130 iter 59 loss=0.035613320767879486\n",
      "epoch 130 iter 60 loss=0.043090593069791794\n",
      "epoch 130 iter 61 loss=0.07468416541814804\n",
      "epoch 130 iter 62 loss=0.024335185065865517\n",
      "epoch 130 iter 63 loss=0.04145338013768196\n",
      "epoch 130 iter 64 loss=0.032257452607154846\n",
      "epoch 130 iter 65 loss=0.04183463007211685\n",
      "epoch 130 iter 66 loss=0.02453465759754181\n",
      "epoch 130 iter 67 loss=0.05265878140926361\n",
      "epoch 130 iter 68 loss=0.0569821298122406\n",
      "epoch 130 iter 69 loss=0.06346375495195389\n",
      "epoch 130 iter 70 loss=0.055481936782598495\n",
      "epoch 130 iter 71 loss=0.028535885736346245\n",
      "epoch 130 iter 72 loss=0.04324096068739891\n",
      "epoch 130 iter 73 loss=0.053736768662929535\n",
      "epoch 130 iter 74 loss=0.054188601672649384\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3179.\n",
      "epoch 131 iter 0 loss=0.0684724897146225\n",
      "epoch 131 iter 1 loss=0.04908078536391258\n",
      "epoch 131 iter 2 loss=0.0541527085006237\n",
      "epoch 131 iter 3 loss=0.04059155657887459\n",
      "epoch 131 iter 4 loss=0.03347831219434738\n",
      "epoch 131 iter 5 loss=0.043172042816877365\n",
      "epoch 131 iter 6 loss=0.04877091199159622\n",
      "epoch 131 iter 7 loss=0.030226323753595352\n",
      "epoch 131 iter 8 loss=0.022343246266245842\n",
      "epoch 131 iter 9 loss=0.04432010278105736\n",
      "epoch 131 iter 10 loss=0.028599238023161888\n",
      "epoch 131 iter 11 loss=0.035252053290605545\n",
      "epoch 131 iter 12 loss=0.029570741578936577\n",
      "epoch 131 iter 13 loss=0.04881047084927559\n",
      "epoch 131 iter 14 loss=0.03749677911400795\n",
      "epoch 131 iter 15 loss=0.05538549646735191\n",
      "epoch 131 iter 16 loss=0.05389498174190521\n",
      "epoch 131 iter 17 loss=0.06849096715450287\n",
      "epoch 131 iter 18 loss=0.03854832425713539\n",
      "epoch 131 iter 19 loss=0.033363234251737595\n",
      "epoch 131 iter 20 loss=0.029990416020154953\n",
      "epoch 131 iter 21 loss=0.03180108964443207\n",
      "epoch 131 iter 22 loss=0.044847313314676285\n",
      "epoch 131 iter 23 loss=0.028050297871232033\n",
      "epoch 131 iter 24 loss=0.04900576174259186\n",
      "epoch 131 iter 25 loss=0.043306633830070496\n",
      "epoch 131 iter 26 loss=0.04066316783428192\n",
      "epoch 131 iter 27 loss=0.06117739528417587\n",
      "epoch 131 iter 28 loss=0.04229830577969551\n",
      "epoch 131 iter 29 loss=0.04912561923265457\n",
      "epoch 131 iter 30 loss=0.0222915206104517\n",
      "epoch 131 iter 31 loss=0.062466125935316086\n",
      "epoch 131 iter 32 loss=0.03582035377621651\n",
      "epoch 131 iter 33 loss=0.04408169537782669\n",
      "epoch 131 iter 34 loss=0.03993162512779236\n",
      "epoch 131 iter 35 loss=0.048060234636068344\n",
      "epoch 131 iter 36 loss=0.07133161276578903\n",
      "epoch 131 iter 37 loss=0.04162624850869179\n",
      "epoch 131 iter 38 loss=0.025610577315092087\n",
      "epoch 131 iter 39 loss=0.02781882882118225\n",
      "epoch 131 iter 40 loss=0.03367747366428375\n",
      "epoch 131 iter 41 loss=0.05093623697757721\n",
      "epoch 131 iter 42 loss=0.024371668696403503\n",
      "epoch 131 iter 43 loss=0.04362707585096359\n",
      "epoch 131 iter 44 loss=0.039702821522951126\n",
      "epoch 131 iter 45 loss=0.03786665201187134\n",
      "epoch 131 iter 46 loss=0.03748859465122223\n",
      "epoch 131 iter 47 loss=0.055335525423288345\n",
      "epoch 131 iter 48 loss=0.047662172466516495\n",
      "epoch 131 iter 49 loss=0.04109610617160797\n",
      "epoch 131 iter 50 loss=0.043756868690252304\n",
      "epoch 131 iter 51 loss=0.04142867401242256\n",
      "epoch 131 iter 52 loss=0.04499252885580063\n",
      "epoch 131 iter 53 loss=0.04519931972026825\n",
      "epoch 131 iter 54 loss=0.05184409022331238\n",
      "epoch 131 iter 55 loss=0.02882416360080242\n",
      "epoch 131 iter 56 loss=0.03178621828556061\n",
      "epoch 131 iter 57 loss=0.030546648427844048\n",
      "epoch 131 iter 58 loss=0.04807623475790024\n",
      "epoch 131 iter 59 loss=0.0224029328674078\n",
      "epoch 131 iter 60 loss=0.05749572068452835\n",
      "epoch 131 iter 61 loss=0.058297090232372284\n",
      "epoch 131 iter 62 loss=0.03150590509176254\n",
      "epoch 131 iter 63 loss=0.03957866504788399\n",
      "epoch 131 iter 64 loss=0.02787875384092331\n",
      "epoch 131 iter 65 loss=0.03863709047436714\n",
      "epoch 131 iter 66 loss=0.05681962892413139\n",
      "epoch 131 iter 67 loss=0.050182875245809555\n",
      "epoch 131 iter 68 loss=0.033788442611694336\n",
      "epoch 131 iter 69 loss=0.04270846024155617\n",
      "epoch 131 iter 70 loss=0.04480919986963272\n",
      "epoch 131 iter 71 loss=0.026771169155836105\n",
      "epoch 131 iter 72 loss=0.027525871992111206\n",
      "epoch 131 iter 73 loss=0.035338226705789566\n",
      "epoch 131 iter 74 loss=0.0554649718105793\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3274.\n",
      "epoch 132 iter 0 loss=0.037552449852228165\n",
      "epoch 132 iter 1 loss=0.05823514983057976\n",
      "epoch 132 iter 2 loss=0.04606723412871361\n",
      "epoch 132 iter 3 loss=0.049788907170295715\n",
      "epoch 132 iter 4 loss=0.03605609014630318\n",
      "epoch 132 iter 5 loss=0.03961942344903946\n",
      "epoch 132 iter 6 loss=0.05122608691453934\n",
      "epoch 132 iter 7 loss=0.04101411998271942\n",
      "epoch 132 iter 8 loss=0.03734369948506355\n",
      "epoch 132 iter 9 loss=0.026326706632971764\n",
      "epoch 132 iter 10 loss=0.04200474172830582\n",
      "epoch 132 iter 11 loss=0.02454715594649315\n",
      "epoch 132 iter 12 loss=0.04278261587023735\n",
      "epoch 132 iter 13 loss=0.060240451246500015\n",
      "epoch 132 iter 14 loss=0.026415159925818443\n",
      "epoch 132 iter 15 loss=0.02547103352844715\n",
      "epoch 132 iter 16 loss=0.038681771606206894\n",
      "epoch 132 iter 17 loss=0.03843250125646591\n",
      "epoch 132 iter 18 loss=0.03411243110895157\n",
      "epoch 132 iter 19 loss=0.03499368950724602\n",
      "epoch 132 iter 20 loss=0.04256507381796837\n",
      "epoch 132 iter 21 loss=0.04567784443497658\n",
      "epoch 132 iter 22 loss=0.03996579721570015\n",
      "epoch 132 iter 23 loss=0.04952261596918106\n",
      "epoch 132 iter 24 loss=0.050204772502183914\n",
      "epoch 132 iter 25 loss=0.05416521802544594\n",
      "epoch 132 iter 26 loss=0.02740732580423355\n",
      "epoch 132 iter 27 loss=0.05081743001937866\n",
      "epoch 132 iter 28 loss=0.039970338344573975\n",
      "epoch 132 iter 29 loss=0.04359852150082588\n",
      "epoch 132 iter 30 loss=0.028717419132590294\n",
      "epoch 132 iter 31 loss=0.044281646609306335\n",
      "epoch 132 iter 32 loss=0.04406358674168587\n",
      "epoch 132 iter 33 loss=0.03192942962050438\n",
      "epoch 132 iter 34 loss=0.05261383578181267\n",
      "epoch 132 iter 35 loss=0.0315965972840786\n",
      "epoch 132 iter 36 loss=0.031535375863313675\n",
      "epoch 132 iter 37 loss=0.03033469244837761\n",
      "epoch 132 iter 38 loss=0.05295984447002411\n",
      "epoch 132 iter 39 loss=0.04818601533770561\n",
      "epoch 132 iter 40 loss=0.03572574257850647\n",
      "epoch 132 iter 41 loss=0.04918942227959633\n",
      "epoch 132 iter 42 loss=0.02394060231745243\n",
      "epoch 132 iter 43 loss=0.049759555608034134\n",
      "epoch 132 iter 44 loss=0.03658928722143173\n",
      "epoch 132 iter 45 loss=0.047511689364910126\n",
      "epoch 132 iter 46 loss=0.039299849420785904\n",
      "epoch 132 iter 47 loss=0.03747449070215225\n",
      "epoch 132 iter 48 loss=0.03481477126479149\n",
      "epoch 132 iter 49 loss=0.045424383133649826\n",
      "epoch 132 iter 50 loss=0.07068298012018204\n",
      "epoch 132 iter 51 loss=0.030247587710618973\n",
      "epoch 132 iter 52 loss=0.05094854161143303\n",
      "epoch 132 iter 53 loss=0.050226882100105286\n",
      "epoch 132 iter 54 loss=0.03340109810233116\n",
      "epoch 132 iter 55 loss=0.03900284320116043\n",
      "epoch 132 iter 56 loss=0.05549576133489609\n",
      "epoch 132 iter 57 loss=0.04481375217437744\n",
      "epoch 132 iter 58 loss=0.04338214546442032\n",
      "epoch 132 iter 59 loss=0.034709565341472626\n",
      "epoch 132 iter 60 loss=0.037154581397771835\n",
      "epoch 132 iter 61 loss=0.062328193336725235\n",
      "epoch 132 iter 62 loss=0.036788977682590485\n",
      "epoch 132 iter 63 loss=0.051871687173843384\n",
      "epoch 132 iter 64 loss=0.03040376305580139\n",
      "epoch 132 iter 65 loss=0.031338807195425034\n",
      "epoch 132 iter 66 loss=0.04514870420098305\n",
      "epoch 132 iter 67 loss=0.05362872779369354\n",
      "epoch 132 iter 68 loss=0.04474411904811859\n",
      "epoch 132 iter 69 loss=0.0337415374815464\n",
      "epoch 132 iter 70 loss=0.03346560150384903\n",
      "epoch 132 iter 71 loss=0.08578890562057495\n",
      "epoch 132 iter 72 loss=0.03528036177158356\n",
      "epoch 132 iter 73 loss=0.03928416967391968\n",
      "epoch 132 iter 74 loss=0.02638915739953518\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3158.\n",
      "epoch 133 iter 0 loss=0.05277087166905403\n",
      "epoch 133 iter 1 loss=0.023359624668955803\n",
      "epoch 133 iter 2 loss=0.044951509684324265\n",
      "epoch 133 iter 3 loss=0.052387140691280365\n",
      "epoch 133 iter 4 loss=0.044261932373046875\n",
      "epoch 133 iter 5 loss=0.032689087092876434\n",
      "epoch 133 iter 6 loss=0.042851004749536514\n",
      "epoch 133 iter 7 loss=0.02524002268910408\n",
      "epoch 133 iter 8 loss=0.04807759448885918\n",
      "epoch 133 iter 9 loss=0.060212474316358566\n",
      "epoch 133 iter 10 loss=0.03425461798906326\n",
      "epoch 133 iter 11 loss=0.02476128190755844\n",
      "epoch 133 iter 12 loss=0.04708109796047211\n",
      "epoch 133 iter 13 loss=0.046314485371112823\n",
      "epoch 133 iter 14 loss=0.05205565690994263\n",
      "epoch 133 iter 15 loss=0.021143466234207153\n",
      "epoch 133 iter 16 loss=0.03115864284336567\n",
      "epoch 133 iter 17 loss=0.058022238314151764\n",
      "epoch 133 iter 18 loss=0.030589254572987556\n",
      "epoch 133 iter 19 loss=0.034148991107940674\n",
      "epoch 133 iter 20 loss=0.024396410211920738\n",
      "epoch 133 iter 21 loss=0.02972319722175598\n",
      "epoch 133 iter 22 loss=0.035628482699394226\n",
      "epoch 133 iter 23 loss=0.0332471989095211\n",
      "epoch 133 iter 24 loss=0.041950538754463196\n",
      "epoch 133 iter 25 loss=0.03048422373831272\n",
      "epoch 133 iter 26 loss=0.039570700377225876\n",
      "epoch 133 iter 27 loss=0.045902006328105927\n",
      "epoch 133 iter 28 loss=0.04736572504043579\n",
      "epoch 133 iter 29 loss=0.054580748081207275\n",
      "epoch 133 iter 30 loss=0.031206870451569557\n",
      "epoch 133 iter 31 loss=0.03269549086689949\n",
      "epoch 133 iter 32 loss=0.056236132979393005\n",
      "epoch 133 iter 33 loss=0.06496825814247131\n",
      "epoch 133 iter 34 loss=0.05225600674748421\n",
      "epoch 133 iter 35 loss=0.05798685923218727\n",
      "epoch 133 iter 36 loss=0.022919727489352226\n",
      "epoch 133 iter 37 loss=0.04345408082008362\n",
      "epoch 133 iter 38 loss=0.03295362740755081\n",
      "epoch 133 iter 39 loss=0.022232787683606148\n",
      "epoch 133 iter 40 loss=0.049545448273420334\n",
      "epoch 133 iter 41 loss=0.02635541930794716\n",
      "epoch 133 iter 42 loss=0.043968163430690765\n",
      "epoch 133 iter 43 loss=0.03818831220269203\n",
      "epoch 133 iter 44 loss=0.056161120533943176\n",
      "epoch 133 iter 45 loss=0.02981669269502163\n",
      "epoch 133 iter 46 loss=0.03974073380231857\n",
      "epoch 133 iter 47 loss=0.06608311086893082\n",
      "epoch 133 iter 48 loss=0.04331668093800545\n",
      "epoch 133 iter 49 loss=0.028484582901000977\n",
      "epoch 133 iter 50 loss=0.040232401341199875\n",
      "epoch 133 iter 51 loss=0.03475816920399666\n",
      "epoch 133 iter 52 loss=0.04619615525007248\n",
      "epoch 133 iter 53 loss=0.026556476950645447\n",
      "epoch 133 iter 54 loss=0.039636317640542984\n",
      "epoch 133 iter 55 loss=0.03119666501879692\n",
      "epoch 133 iter 56 loss=0.05171685293316841\n",
      "epoch 133 iter 57 loss=0.04369433596730232\n",
      "epoch 133 iter 58 loss=0.02315911091864109\n",
      "epoch 133 iter 59 loss=0.0490444116294384\n",
      "epoch 133 iter 60 loss=0.04907437041401863\n",
      "epoch 133 iter 61 loss=0.029858198016881943\n",
      "epoch 133 iter 62 loss=0.039792828261852264\n",
      "epoch 133 iter 63 loss=0.035516753792762756\n",
      "epoch 133 iter 64 loss=0.04454762861132622\n",
      "epoch 133 iter 65 loss=0.04178900644183159\n",
      "epoch 133 iter 66 loss=0.04274973273277283\n",
      "epoch 133 iter 67 loss=0.04938192665576935\n",
      "epoch 133 iter 68 loss=0.06767481565475464\n",
      "epoch 133 iter 69 loss=0.031797800213098526\n",
      "epoch 133 iter 70 loss=0.06964500993490219\n",
      "epoch 133 iter 71 loss=0.03842749446630478\n",
      "epoch 133 iter 72 loss=0.03912094980478287\n",
      "epoch 133 iter 73 loss=0.04631587117910385\n",
      "epoch 133 iter 74 loss=0.022846639156341553\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3298.\n",
      "epoch 134 iter 0 loss=0.039059802889823914\n",
      "epoch 134 iter 1 loss=0.028016455471515656\n",
      "epoch 134 iter 2 loss=0.05501452833414078\n",
      "epoch 134 iter 3 loss=0.03074299357831478\n",
      "epoch 134 iter 4 loss=0.03336557000875473\n",
      "epoch 134 iter 5 loss=0.050904519855976105\n",
      "epoch 134 iter 6 loss=0.03790029138326645\n",
      "epoch 134 iter 7 loss=0.037058282643556595\n",
      "epoch 134 iter 8 loss=0.041494354605674744\n",
      "epoch 134 iter 9 loss=0.040389638394117355\n",
      "epoch 134 iter 10 loss=0.06710821390151978\n",
      "epoch 134 iter 11 loss=0.023618733510375023\n",
      "epoch 134 iter 12 loss=0.05375673249363899\n",
      "epoch 134 iter 13 loss=0.033283405005931854\n",
      "epoch 134 iter 14 loss=0.034813862293958664\n",
      "epoch 134 iter 15 loss=0.03031882271170616\n",
      "epoch 134 iter 16 loss=0.035148657858371735\n",
      "epoch 134 iter 17 loss=0.05426398292183876\n",
      "epoch 134 iter 18 loss=0.05924782529473305\n",
      "epoch 134 iter 19 loss=0.050312068313360214\n",
      "epoch 134 iter 20 loss=0.04235513135790825\n",
      "epoch 134 iter 21 loss=0.048766233026981354\n",
      "epoch 134 iter 22 loss=0.03893778845667839\n",
      "epoch 134 iter 23 loss=0.03427910804748535\n",
      "epoch 134 iter 24 loss=0.03300658240914345\n",
      "epoch 134 iter 25 loss=0.03514368087053299\n",
      "epoch 134 iter 26 loss=0.03791011869907379\n",
      "epoch 134 iter 27 loss=0.045900411903858185\n",
      "epoch 134 iter 28 loss=0.031284209340810776\n",
      "epoch 134 iter 29 loss=0.06378825008869171\n",
      "epoch 134 iter 30 loss=0.032412346452474594\n",
      "epoch 134 iter 31 loss=0.04004748538136482\n",
      "epoch 134 iter 32 loss=0.04071478545665741\n",
      "epoch 134 iter 33 loss=0.02351379580795765\n",
      "epoch 134 iter 34 loss=0.05683302506804466\n",
      "epoch 134 iter 35 loss=0.05586804449558258\n",
      "epoch 134 iter 36 loss=0.039792634546756744\n",
      "epoch 134 iter 37 loss=0.05697512626647949\n",
      "epoch 134 iter 38 loss=0.05946282297372818\n",
      "epoch 134 iter 39 loss=0.029474060982465744\n",
      "epoch 134 iter 40 loss=0.02925361692905426\n",
      "epoch 134 iter 41 loss=0.04790220782160759\n",
      "epoch 134 iter 42 loss=0.017581667751073837\n",
      "epoch 134 iter 43 loss=0.04098048806190491\n",
      "epoch 134 iter 44 loss=0.05226323753595352\n",
      "epoch 134 iter 45 loss=0.033324480056762695\n",
      "epoch 134 iter 46 loss=0.04445905238389969\n",
      "epoch 134 iter 47 loss=0.04020386561751366\n",
      "epoch 134 iter 48 loss=0.02431960590183735\n",
      "epoch 134 iter 49 loss=0.046249743551015854\n",
      "epoch 134 iter 50 loss=0.03544127196073532\n",
      "epoch 134 iter 51 loss=0.044689714908599854\n",
      "epoch 134 iter 52 loss=0.03170131519436836\n",
      "epoch 134 iter 53 loss=0.0456211194396019\n",
      "epoch 134 iter 54 loss=0.02603057771921158\n",
      "epoch 134 iter 55 loss=0.0709155797958374\n",
      "epoch 134 iter 56 loss=0.05684143677353859\n",
      "epoch 134 iter 57 loss=0.057923853397369385\n",
      "epoch 134 iter 58 loss=0.024346355348825455\n",
      "epoch 134 iter 59 loss=0.03203483670949936\n",
      "epoch 134 iter 60 loss=0.033643413335084915\n",
      "epoch 134 iter 61 loss=0.02756894752383232\n",
      "epoch 134 iter 62 loss=0.03166085481643677\n",
      "epoch 134 iter 63 loss=0.04630393534898758\n",
      "epoch 134 iter 64 loss=0.05882647633552551\n",
      "epoch 134 iter 65 loss=0.02748102694749832\n",
      "epoch 134 iter 66 loss=0.03024373948574066\n",
      "epoch 134 iter 67 loss=0.05160294845700264\n",
      "epoch 134 iter 68 loss=0.04668256267905235\n",
      "epoch 134 iter 69 loss=0.038717225193977356\n",
      "epoch 134 iter 70 loss=0.03381985053420067\n",
      "epoch 134 iter 71 loss=0.043044742196798325\n",
      "epoch 134 iter 72 loss=0.043282512575387955\n",
      "epoch 134 iter 73 loss=0.03312835842370987\n",
      "epoch 134 iter 74 loss=0.04050455614924431\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3274.\n",
      "epoch 135 iter 0 loss=0.04543789476156235\n",
      "epoch 135 iter 1 loss=0.03866339847445488\n",
      "epoch 135 iter 2 loss=0.024833854287862778\n",
      "epoch 135 iter 3 loss=0.047421835362911224\n",
      "epoch 135 iter 4 loss=0.03649461269378662\n",
      "epoch 135 iter 5 loss=0.03457311540842056\n",
      "epoch 135 iter 6 loss=0.041791412979364395\n",
      "epoch 135 iter 7 loss=0.021504487842321396\n",
      "epoch 135 iter 8 loss=0.05186814069747925\n",
      "epoch 135 iter 9 loss=0.0385100282728672\n",
      "epoch 135 iter 10 loss=0.05781120806932449\n",
      "epoch 135 iter 11 loss=0.03692211955785751\n",
      "epoch 135 iter 12 loss=0.038299694657325745\n",
      "epoch 135 iter 13 loss=0.03438303619623184\n",
      "epoch 135 iter 14 loss=0.04257305711507797\n",
      "epoch 135 iter 15 loss=0.024929741397500038\n",
      "epoch 135 iter 16 loss=0.04209911450743675\n",
      "epoch 135 iter 17 loss=0.05800607055425644\n",
      "epoch 135 iter 18 loss=0.03064059279859066\n",
      "epoch 135 iter 19 loss=0.04452219605445862\n",
      "epoch 135 iter 20 loss=0.052926935255527496\n",
      "epoch 135 iter 21 loss=0.0319635309278965\n",
      "epoch 135 iter 22 loss=0.025131048634648323\n",
      "epoch 135 iter 23 loss=0.036378469318151474\n",
      "epoch 135 iter 24 loss=0.06613177806138992\n",
      "epoch 135 iter 25 loss=0.04439954087138176\n",
      "epoch 135 iter 26 loss=0.037690673023462296\n",
      "epoch 135 iter 27 loss=0.050230417400598526\n",
      "epoch 135 iter 28 loss=0.016475142911076546\n",
      "epoch 135 iter 29 loss=0.030216649174690247\n",
      "epoch 135 iter 30 loss=0.05007674917578697\n",
      "epoch 135 iter 31 loss=0.05074648931622505\n",
      "epoch 135 iter 32 loss=0.04906952381134033\n",
      "epoch 135 iter 33 loss=0.053455956280231476\n",
      "epoch 135 iter 34 loss=0.02567974664270878\n",
      "epoch 135 iter 35 loss=0.05069458857178688\n",
      "epoch 135 iter 36 loss=0.053538527339696884\n",
      "epoch 135 iter 37 loss=0.046829208731651306\n",
      "epoch 135 iter 38 loss=0.039295997470617294\n",
      "epoch 135 iter 39 loss=0.0677151307463646\n",
      "epoch 135 iter 40 loss=0.025411665439605713\n",
      "epoch 135 iter 41 loss=0.03310611471533775\n",
      "epoch 135 iter 42 loss=0.04682941734790802\n",
      "epoch 135 iter 43 loss=0.03293617442250252\n",
      "epoch 135 iter 44 loss=0.03367282822728157\n",
      "epoch 135 iter 45 loss=0.0396370030939579\n",
      "epoch 135 iter 46 loss=0.022553814575076103\n",
      "epoch 135 iter 47 loss=0.03785175830125809\n",
      "epoch 135 iter 48 loss=0.049321260303258896\n",
      "epoch 135 iter 49 loss=0.03436149284243584\n",
      "epoch 135 iter 50 loss=0.0274367593228817\n",
      "epoch 135 iter 51 loss=0.03621058911085129\n",
      "epoch 135 iter 52 loss=0.06539417058229446\n",
      "epoch 135 iter 53 loss=0.04826292768120766\n",
      "epoch 135 iter 54 loss=0.03045915812253952\n",
      "epoch 135 iter 55 loss=0.05132550001144409\n",
      "epoch 135 iter 56 loss=0.025568325072526932\n",
      "epoch 135 iter 57 loss=0.05306491628289223\n",
      "epoch 135 iter 58 loss=0.02290486916899681\n",
      "epoch 135 iter 59 loss=0.03984095901250839\n",
      "epoch 135 iter 60 loss=0.03378279134631157\n",
      "epoch 135 iter 61 loss=0.053838834166526794\n",
      "epoch 135 iter 62 loss=0.039213601499795914\n",
      "epoch 135 iter 63 loss=0.034904591739177704\n",
      "epoch 135 iter 64 loss=0.0434047132730484\n",
      "epoch 135 iter 65 loss=0.03885817900300026\n",
      "epoch 135 iter 66 loss=0.040643129497766495\n",
      "epoch 135 iter 67 loss=0.04315103963017464\n",
      "epoch 135 iter 68 loss=0.037513796240091324\n",
      "epoch 135 iter 69 loss=0.04878659546375275\n",
      "epoch 135 iter 70 loss=0.03605111688375473\n",
      "epoch 135 iter 71 loss=0.02442202903330326\n",
      "epoch 135 iter 72 loss=0.0321606807410717\n",
      "epoch 135 iter 73 loss=0.03596636652946472\n",
      "epoch 135 iter 74 loss=0.05183775722980499\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3230.\n",
      "epoch 136 iter 0 loss=0.041958991438150406\n",
      "epoch 136 iter 1 loss=0.027078159153461456\n",
      "epoch 136 iter 2 loss=0.03909413889050484\n",
      "epoch 136 iter 3 loss=0.024432605132460594\n",
      "epoch 136 iter 4 loss=0.031127288937568665\n",
      "epoch 136 iter 5 loss=0.029968278482556343\n",
      "epoch 136 iter 6 loss=0.03400995582342148\n",
      "epoch 136 iter 7 loss=0.026124170050024986\n",
      "epoch 136 iter 8 loss=0.042374949902296066\n",
      "epoch 136 iter 9 loss=0.05259415879845619\n",
      "epoch 136 iter 10 loss=0.039694443345069885\n",
      "epoch 136 iter 11 loss=0.0363585464656353\n",
      "epoch 136 iter 12 loss=0.044446833431720734\n",
      "epoch 136 iter 13 loss=0.03196391463279724\n",
      "epoch 136 iter 14 loss=0.02517795003950596\n",
      "epoch 136 iter 15 loss=0.03139827772974968\n",
      "epoch 136 iter 16 loss=0.07276307046413422\n",
      "epoch 136 iter 17 loss=0.047130655497312546\n",
      "epoch 136 iter 18 loss=0.04081600159406662\n",
      "epoch 136 iter 19 loss=0.038165733218193054\n",
      "epoch 136 iter 20 loss=0.021323276683688164\n",
      "epoch 136 iter 21 loss=0.043161772191524506\n",
      "epoch 136 iter 22 loss=0.03198634833097458\n",
      "epoch 136 iter 23 loss=0.04818107560276985\n",
      "epoch 136 iter 24 loss=0.05960044637322426\n",
      "epoch 136 iter 25 loss=0.04362773150205612\n",
      "epoch 136 iter 26 loss=0.05622163042426109\n",
      "epoch 136 iter 27 loss=0.045321639627218246\n",
      "epoch 136 iter 28 loss=0.02108660340309143\n",
      "epoch 136 iter 29 loss=0.0500069297850132\n",
      "epoch 136 iter 30 loss=0.04799652844667435\n",
      "epoch 136 iter 31 loss=0.030180297791957855\n",
      "epoch 136 iter 32 loss=0.020984718576073647\n",
      "epoch 136 iter 33 loss=0.05733621120452881\n",
      "epoch 136 iter 34 loss=0.05354737490415573\n",
      "epoch 136 iter 35 loss=0.03301764279603958\n",
      "epoch 136 iter 36 loss=0.057950425893068314\n",
      "epoch 136 iter 37 loss=0.04069727286696434\n",
      "epoch 136 iter 38 loss=0.042979851365089417\n",
      "epoch 136 iter 39 loss=0.044686201959848404\n",
      "epoch 136 iter 40 loss=0.03016294725239277\n",
      "epoch 136 iter 41 loss=0.04931848123669624\n",
      "epoch 136 iter 42 loss=0.04076143354177475\n",
      "epoch 136 iter 43 loss=0.04176079481840134\n",
      "epoch 136 iter 44 loss=0.022228799760341644\n",
      "epoch 136 iter 45 loss=0.053284768015146255\n",
      "epoch 136 iter 46 loss=0.05483083054423332\n",
      "epoch 136 iter 47 loss=0.030897200107574463\n",
      "epoch 136 iter 48 loss=0.03516751527786255\n",
      "epoch 136 iter 49 loss=0.029473884031176567\n",
      "epoch 136 iter 50 loss=0.06270952522754669\n",
      "epoch 136 iter 51 loss=0.03341154009103775\n",
      "epoch 136 iter 52 loss=0.037859633564949036\n",
      "epoch 136 iter 53 loss=0.03273601084947586\n",
      "epoch 136 iter 54 loss=0.04604791849851608\n",
      "epoch 136 iter 55 loss=0.043604109436273575\n",
      "epoch 136 iter 56 loss=0.04032231867313385\n",
      "epoch 136 iter 57 loss=0.021132970228791237\n",
      "epoch 136 iter 58 loss=0.028741024434566498\n",
      "epoch 136 iter 59 loss=0.03271552920341492\n",
      "epoch 136 iter 60 loss=0.0651593878865242\n",
      "epoch 136 iter 61 loss=0.042968910187482834\n",
      "epoch 136 iter 62 loss=0.03452304005622864\n",
      "epoch 136 iter 63 loss=0.03992552310228348\n",
      "epoch 136 iter 64 loss=0.035554733127355576\n",
      "epoch 136 iter 65 loss=0.03595323488116264\n",
      "epoch 136 iter 66 loss=0.0411902479827404\n",
      "epoch 136 iter 67 loss=0.044218532741069794\n",
      "epoch 136 iter 68 loss=0.03474365174770355\n",
      "epoch 136 iter 69 loss=0.039896465837955475\n",
      "epoch 136 iter 70 loss=0.02722984179854393\n",
      "epoch 136 iter 71 loss=0.044049233198165894\n",
      "epoch 136 iter 72 loss=0.03842223435640335\n",
      "epoch 136 iter 73 loss=0.04452986270189285\n",
      "epoch 136 iter 74 loss=0.04564668610692024\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3180.\n",
      "epoch 137 iter 0 loss=0.02065047062933445\n",
      "epoch 137 iter 1 loss=0.03936334699392319\n",
      "epoch 137 iter 2 loss=0.04541805386543274\n",
      "epoch 137 iter 3 loss=0.030651090666651726\n",
      "epoch 137 iter 4 loss=0.032236337661743164\n",
      "epoch 137 iter 5 loss=0.03843121603131294\n",
      "epoch 137 iter 6 loss=0.05040478706359863\n",
      "epoch 137 iter 7 loss=0.034682076424360275\n",
      "epoch 137 iter 8 loss=0.05480356141924858\n",
      "epoch 137 iter 9 loss=0.053675562143325806\n",
      "epoch 137 iter 10 loss=0.03502720966935158\n",
      "epoch 137 iter 11 loss=0.02186938002705574\n",
      "epoch 137 iter 12 loss=0.0490540973842144\n",
      "epoch 137 iter 13 loss=0.026617402210831642\n",
      "epoch 137 iter 14 loss=0.02175620011985302\n",
      "epoch 137 iter 15 loss=0.04635642096400261\n",
      "epoch 137 iter 16 loss=0.023234453052282333\n",
      "epoch 137 iter 17 loss=0.030765023082494736\n",
      "epoch 137 iter 18 loss=0.040033843368291855\n",
      "epoch 137 iter 19 loss=0.0455932654440403\n",
      "epoch 137 iter 20 loss=0.03615532070398331\n",
      "epoch 137 iter 21 loss=0.05006173625588417\n",
      "epoch 137 iter 22 loss=0.03438691422343254\n",
      "epoch 137 iter 23 loss=0.04234295338392258\n",
      "epoch 137 iter 24 loss=0.029409432783722878\n",
      "epoch 137 iter 25 loss=0.06531110405921936\n",
      "epoch 137 iter 26 loss=0.03181090205907822\n",
      "epoch 137 iter 27 loss=0.04132574796676636\n",
      "epoch 137 iter 28 loss=0.04849562793970108\n",
      "epoch 137 iter 29 loss=0.051704246550798416\n",
      "epoch 137 iter 30 loss=0.041563697159290314\n",
      "epoch 137 iter 31 loss=0.02479804866015911\n",
      "epoch 137 iter 32 loss=0.036447666585445404\n",
      "epoch 137 iter 33 loss=0.03782867267727852\n",
      "epoch 137 iter 34 loss=0.020658230409026146\n",
      "epoch 137 iter 35 loss=0.04788493737578392\n",
      "epoch 137 iter 36 loss=0.020261671394109726\n",
      "epoch 137 iter 37 loss=0.06430014967918396\n",
      "epoch 137 iter 38 loss=0.030651014298200607\n",
      "epoch 137 iter 39 loss=0.06299687922000885\n",
      "epoch 137 iter 40 loss=0.04001474380493164\n",
      "epoch 137 iter 41 loss=0.04351203516125679\n",
      "epoch 137 iter 42 loss=0.030531257390975952\n",
      "epoch 137 iter 43 loss=0.037356339395046234\n",
      "epoch 137 iter 44 loss=0.043698232620954514\n",
      "epoch 137 iter 45 loss=0.03370225057005882\n",
      "epoch 137 iter 46 loss=0.03898168355226517\n",
      "epoch 137 iter 47 loss=0.03174708038568497\n",
      "epoch 137 iter 48 loss=0.04881269857287407\n",
      "epoch 137 iter 49 loss=0.03227716311812401\n",
      "epoch 137 iter 50 loss=0.03214850649237633\n",
      "epoch 137 iter 51 loss=0.03914015367627144\n",
      "epoch 137 iter 52 loss=0.06568513810634613\n",
      "epoch 137 iter 53 loss=0.031033683568239212\n",
      "epoch 137 iter 54 loss=0.041042931377887726\n",
      "epoch 137 iter 55 loss=0.05760881304740906\n",
      "epoch 137 iter 56 loss=0.044152531772851944\n",
      "epoch 137 iter 57 loss=0.03873201087117195\n",
      "epoch 137 iter 58 loss=0.03946356102824211\n",
      "epoch 137 iter 59 loss=0.0640496015548706\n",
      "epoch 137 iter 60 loss=0.03867098316550255\n",
      "epoch 137 iter 61 loss=0.03686065226793289\n",
      "epoch 137 iter 62 loss=0.03911667317152023\n",
      "epoch 137 iter 63 loss=0.02010878175497055\n",
      "epoch 137 iter 64 loss=0.05918159708380699\n",
      "epoch 137 iter 65 loss=0.039501044899225235\n",
      "epoch 137 iter 66 loss=0.05469050258398056\n",
      "epoch 137 iter 67 loss=0.026068517938256264\n",
      "epoch 137 iter 68 loss=0.05187935382127762\n",
      "epoch 137 iter 69 loss=0.06176374480128288\n",
      "epoch 137 iter 70 loss=0.030585970729589462\n",
      "epoch 137 iter 71 loss=0.053494952619075775\n",
      "epoch 137 iter 72 loss=0.03348241746425629\n",
      "epoch 137 iter 73 loss=0.028375035151839256\n",
      "epoch 137 iter 74 loss=0.05178321897983551\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3180.\n",
      "epoch 138 iter 0 loss=0.031520381569862366\n",
      "epoch 138 iter 1 loss=0.03164277970790863\n",
      "epoch 138 iter 2 loss=0.04355054721236229\n",
      "epoch 138 iter 3 loss=0.03438857942819595\n",
      "epoch 138 iter 4 loss=0.028304893523454666\n",
      "epoch 138 iter 5 loss=0.04768742620944977\n",
      "epoch 138 iter 6 loss=0.02610396035015583\n",
      "epoch 138 iter 7 loss=0.034160763025283813\n",
      "epoch 138 iter 8 loss=0.04983704909682274\n",
      "epoch 138 iter 9 loss=0.03999794274568558\n",
      "epoch 138 iter 10 loss=0.02438252605497837\n",
      "epoch 138 iter 11 loss=0.030280357226729393\n",
      "epoch 138 iter 12 loss=0.04574595391750336\n",
      "epoch 138 iter 13 loss=0.03568103536963463\n",
      "epoch 138 iter 14 loss=0.03547370806336403\n",
      "epoch 138 iter 15 loss=0.05545802786946297\n",
      "epoch 138 iter 16 loss=0.0484638512134552\n",
      "epoch 138 iter 17 loss=0.05032971128821373\n",
      "epoch 138 iter 18 loss=0.030719691887497902\n",
      "epoch 138 iter 19 loss=0.06032727658748627\n",
      "epoch 138 iter 20 loss=0.04009845480322838\n",
      "epoch 138 iter 21 loss=0.03190939128398895\n",
      "epoch 138 iter 22 loss=0.03342165797948837\n",
      "epoch 138 iter 23 loss=0.031956177204847336\n",
      "epoch 138 iter 24 loss=0.057756032794713974\n",
      "epoch 138 iter 25 loss=0.050567876547575\n",
      "epoch 138 iter 26 loss=0.05498521029949188\n",
      "epoch 138 iter 27 loss=0.04683370888233185\n",
      "epoch 138 iter 28 loss=0.03412916511297226\n",
      "epoch 138 iter 29 loss=0.03851300850510597\n",
      "epoch 138 iter 30 loss=0.032698724418878555\n",
      "epoch 138 iter 31 loss=0.02721426449716091\n",
      "epoch 138 iter 32 loss=0.05980079248547554\n",
      "epoch 138 iter 33 loss=0.05610194057226181\n",
      "epoch 138 iter 34 loss=0.03289290517568588\n",
      "epoch 138 iter 35 loss=0.04023972153663635\n",
      "epoch 138 iter 36 loss=0.033397503197193146\n",
      "epoch 138 iter 37 loss=0.020761532709002495\n",
      "epoch 138 iter 38 loss=0.03650694712996483\n",
      "epoch 138 iter 39 loss=0.040604084730148315\n",
      "epoch 138 iter 40 loss=0.03968552500009537\n",
      "epoch 138 iter 41 loss=0.05170431733131409\n",
      "epoch 138 iter 42 loss=0.04558975622057915\n",
      "epoch 138 iter 43 loss=0.03241937607526779\n",
      "epoch 138 iter 44 loss=0.023653851822018623\n",
      "epoch 138 iter 45 loss=0.03044707328081131\n",
      "epoch 138 iter 46 loss=0.022608455270528793\n",
      "epoch 138 iter 47 loss=0.02769744209945202\n",
      "epoch 138 iter 48 loss=0.031349726021289825\n",
      "epoch 138 iter 49 loss=0.027092618867754936\n",
      "epoch 138 iter 50 loss=0.03954168036580086\n",
      "epoch 138 iter 51 loss=0.02651963196694851\n",
      "epoch 138 iter 52 loss=0.07656362652778625\n",
      "epoch 138 iter 53 loss=0.04174220934510231\n",
      "epoch 138 iter 54 loss=0.05706077069044113\n",
      "epoch 138 iter 55 loss=0.04034512862563133\n",
      "epoch 138 iter 56 loss=0.029809530824422836\n",
      "epoch 138 iter 57 loss=0.027818981558084488\n",
      "epoch 138 iter 58 loss=0.03259693831205368\n",
      "epoch 138 iter 59 loss=0.05362065136432648\n",
      "epoch 138 iter 60 loss=0.04638716205954552\n",
      "epoch 138 iter 61 loss=0.04584302380681038\n",
      "epoch 138 iter 62 loss=0.02360999956727028\n",
      "epoch 138 iter 63 loss=0.07239526510238647\n",
      "epoch 138 iter 64 loss=0.0382782444357872\n",
      "epoch 138 iter 65 loss=0.030792919918894768\n",
      "epoch 138 iter 66 loss=0.05209347978234291\n",
      "epoch 138 iter 67 loss=0.05349324271082878\n",
      "epoch 138 iter 68 loss=0.042198121547698975\n",
      "epoch 138 iter 69 loss=0.04927319660782814\n",
      "epoch 138 iter 70 loss=0.03241834044456482\n",
      "epoch 138 iter 71 loss=0.03576243296265602\n",
      "epoch 138 iter 72 loss=0.04636261612176895\n",
      "epoch 138 iter 73 loss=0.01906217634677887\n",
      "epoch 138 iter 74 loss=0.04700688272714615\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3173.\n",
      "epoch 139 iter 0 loss=0.025932513177394867\n",
      "epoch 139 iter 1 loss=0.026047127321362495\n",
      "epoch 139 iter 2 loss=0.043463949114084244\n",
      "epoch 139 iter 3 loss=0.033610809594392776\n",
      "epoch 139 iter 4 loss=0.0503479540348053\n",
      "epoch 139 iter 5 loss=0.036170169711112976\n",
      "epoch 139 iter 6 loss=0.07887592166662216\n",
      "epoch 139 iter 7 loss=0.023813337087631226\n",
      "epoch 139 iter 8 loss=0.05245149880647659\n",
      "epoch 139 iter 9 loss=0.04672173410654068\n",
      "epoch 139 iter 10 loss=0.05506531521677971\n",
      "epoch 139 iter 11 loss=0.03642411530017853\n",
      "epoch 139 iter 12 loss=0.0415102019906044\n",
      "epoch 139 iter 13 loss=0.028946418315172195\n",
      "epoch 139 iter 14 loss=0.049166448414325714\n",
      "epoch 139 iter 15 loss=0.05632728338241577\n",
      "epoch 139 iter 16 loss=0.040803708136081696\n",
      "epoch 139 iter 17 loss=0.04230827838182449\n",
      "epoch 139 iter 18 loss=0.030401932075619698\n",
      "epoch 139 iter 19 loss=0.03425980731844902\n",
      "epoch 139 iter 20 loss=0.0431365929543972\n",
      "epoch 139 iter 21 loss=0.0450628325343132\n",
      "epoch 139 iter 22 loss=0.046616289764642715\n",
      "epoch 139 iter 23 loss=0.028582818806171417\n",
      "epoch 139 iter 24 loss=0.029573984444141388\n",
      "epoch 139 iter 25 loss=0.025979166850447655\n",
      "epoch 139 iter 26 loss=0.020200951024889946\n",
      "epoch 139 iter 27 loss=0.06496690213680267\n",
      "epoch 139 iter 28 loss=0.046128567308187485\n",
      "epoch 139 iter 29 loss=0.03853325918316841\n",
      "epoch 139 iter 30 loss=0.030617449432611465\n",
      "epoch 139 iter 31 loss=0.031219065189361572\n",
      "epoch 139 iter 32 loss=0.025529753416776657\n",
      "epoch 139 iter 33 loss=0.04281546175479889\n",
      "epoch 139 iter 34 loss=0.031341034919023514\n",
      "epoch 139 iter 35 loss=0.02521631494164467\n",
      "epoch 139 iter 36 loss=0.06884343922138214\n",
      "epoch 139 iter 37 loss=0.02993786893785\n",
      "epoch 139 iter 38 loss=0.05747324600815773\n",
      "epoch 139 iter 39 loss=0.04077660292387009\n",
      "epoch 139 iter 40 loss=0.05468970537185669\n",
      "epoch 139 iter 41 loss=0.053804825991392136\n",
      "epoch 139 iter 42 loss=0.0343254953622818\n",
      "epoch 139 iter 43 loss=0.04270784929394722\n",
      "epoch 139 iter 44 loss=0.03546982258558273\n",
      "epoch 139 iter 45 loss=0.03688546642661095\n",
      "epoch 139 iter 46 loss=0.03781095892190933\n",
      "epoch 139 iter 47 loss=0.048814088106155396\n",
      "epoch 139 iter 48 loss=0.059806060045957565\n",
      "epoch 139 iter 49 loss=0.037821315228939056\n",
      "epoch 139 iter 50 loss=0.046679139137268066\n",
      "epoch 139 iter 51 loss=0.030719997361302376\n",
      "epoch 139 iter 52 loss=0.036716993898153305\n",
      "epoch 139 iter 53 loss=0.05234295874834061\n",
      "epoch 139 iter 54 loss=0.023048553615808487\n",
      "epoch 139 iter 55 loss=0.03960597887635231\n",
      "epoch 139 iter 56 loss=0.03972778096795082\n",
      "epoch 139 iter 57 loss=0.03577079623937607\n",
      "epoch 139 iter 58 loss=0.046287212520837784\n",
      "epoch 139 iter 59 loss=0.03563065081834793\n",
      "epoch 139 iter 60 loss=0.06637025624513626\n",
      "epoch 139 iter 61 loss=0.018306201323866844\n",
      "epoch 139 iter 62 loss=0.052015166729688644\n",
      "epoch 139 iter 63 loss=0.034959111362695694\n",
      "epoch 139 iter 64 loss=0.032227542251348495\n",
      "epoch 139 iter 65 loss=0.045794527977705\n",
      "epoch 139 iter 66 loss=0.022417129948735237\n",
      "epoch 139 iter 67 loss=0.04337327927350998\n",
      "epoch 139 iter 68 loss=0.028342848643660545\n",
      "epoch 139 iter 69 loss=0.02908850461244583\n",
      "epoch 139 iter 70 loss=0.03683164343237877\n",
      "epoch 139 iter 71 loss=0.035406358540058136\n",
      "epoch 139 iter 72 loss=0.026856662705540657\n",
      "epoch 139 iter 73 loss=0.0424538217484951\n",
      "epoch 139 iter 74 loss=0.03775874897837639\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3201.\n",
      "epoch 140 iter 0 loss=0.0375443734228611\n",
      "epoch 140 iter 1 loss=0.036153510212898254\n",
      "epoch 140 iter 2 loss=0.033984698355197906\n",
      "epoch 140 iter 3 loss=0.03614988550543785\n",
      "epoch 140 iter 4 loss=0.041849974542856216\n",
      "epoch 140 iter 5 loss=0.03550947085022926\n",
      "epoch 140 iter 6 loss=0.053923457860946655\n",
      "epoch 140 iter 7 loss=0.03690527379512787\n",
      "epoch 140 iter 8 loss=0.03431129455566406\n",
      "epoch 140 iter 9 loss=0.038499847054481506\n",
      "epoch 140 iter 10 loss=0.0547226257622242\n",
      "epoch 140 iter 11 loss=0.07110561430454254\n",
      "epoch 140 iter 12 loss=0.028551025316119194\n",
      "epoch 140 iter 13 loss=0.024839190766215324\n",
      "epoch 140 iter 14 loss=0.034868791699409485\n",
      "epoch 140 iter 15 loss=0.038853198289871216\n",
      "epoch 140 iter 16 loss=0.03744039684534073\n",
      "epoch 140 iter 17 loss=0.03817906603217125\n",
      "epoch 140 iter 18 loss=0.04580775275826454\n",
      "epoch 140 iter 19 loss=0.057686466723680496\n",
      "epoch 140 iter 20 loss=0.029927995055913925\n",
      "epoch 140 iter 21 loss=0.023650918155908585\n",
      "epoch 140 iter 22 loss=0.037612736225128174\n",
      "epoch 140 iter 23 loss=0.057378944009542465\n",
      "epoch 140 iter 24 loss=0.0387563481926918\n",
      "epoch 140 iter 25 loss=0.0610714852809906\n",
      "epoch 140 iter 26 loss=0.06670371443033218\n",
      "epoch 140 iter 27 loss=0.03950588032603264\n",
      "epoch 140 iter 28 loss=0.021278370171785355\n",
      "epoch 140 iter 29 loss=0.03271983191370964\n",
      "epoch 140 iter 30 loss=0.045873403549194336\n",
      "epoch 140 iter 31 loss=0.042177774012088776\n",
      "epoch 140 iter 32 loss=0.035963527858257294\n",
      "epoch 140 iter 33 loss=0.0393415130674839\n",
      "epoch 140 iter 34 loss=0.03611069545149803\n",
      "epoch 140 iter 35 loss=0.030971083790063858\n",
      "epoch 140 iter 36 loss=0.039152685552835464\n",
      "epoch 140 iter 37 loss=0.04060464724898338\n",
      "epoch 140 iter 38 loss=0.03424927592277527\n",
      "epoch 140 iter 39 loss=0.027407124638557434\n",
      "epoch 140 iter 40 loss=0.038041647523641586\n",
      "epoch 140 iter 41 loss=0.03356538712978363\n",
      "epoch 140 iter 42 loss=0.020300060510635376\n",
      "epoch 140 iter 43 loss=0.031416717916727066\n",
      "epoch 140 iter 44 loss=0.04801898077130318\n",
      "epoch 140 iter 45 loss=0.02507457137107849\n",
      "epoch 140 iter 46 loss=0.036740992218256\n",
      "epoch 140 iter 47 loss=0.03757666423916817\n",
      "epoch 140 iter 48 loss=0.042603034526109695\n",
      "epoch 140 iter 49 loss=0.057237062603235245\n",
      "epoch 140 iter 50 loss=0.04276768118143082\n",
      "epoch 140 iter 51 loss=0.025948887690901756\n",
      "epoch 140 iter 52 loss=0.051713503897190094\n",
      "epoch 140 iter 53 loss=0.039730533957481384\n",
      "epoch 140 iter 54 loss=0.03423481434583664\n",
      "epoch 140 iter 55 loss=0.026425648480653763\n",
      "epoch 140 iter 56 loss=0.05976292863488197\n",
      "epoch 140 iter 57 loss=0.039265330880880356\n",
      "epoch 140 iter 58 loss=0.033676065504550934\n",
      "epoch 140 iter 59 loss=0.03070608340203762\n",
      "epoch 140 iter 60 loss=0.03696707636117935\n",
      "epoch 140 iter 61 loss=0.017474476248025894\n",
      "epoch 140 iter 62 loss=0.018188893795013428\n",
      "epoch 140 iter 63 loss=0.044073887169361115\n",
      "epoch 140 iter 64 loss=0.03947831690311432\n",
      "epoch 140 iter 65 loss=0.022843457758426666\n",
      "epoch 140 iter 66 loss=0.045127224177122116\n",
      "epoch 140 iter 67 loss=0.0422297865152359\n",
      "epoch 140 iter 68 loss=0.045754749327898026\n",
      "epoch 140 iter 69 loss=0.03884672373533249\n",
      "epoch 140 iter 70 loss=0.050830237567424774\n",
      "epoch 140 iter 71 loss=0.022838938981294632\n",
      "epoch 140 iter 72 loss=0.033853381872177124\n",
      "epoch 140 iter 73 loss=0.037849050015211105\n",
      "epoch 140 iter 74 loss=0.06873970478773117\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3207.\n",
      "epoch 141 iter 0 loss=0.02588474005460739\n",
      "epoch 141 iter 1 loss=0.05235588178038597\n",
      "epoch 141 iter 2 loss=0.0453590527176857\n",
      "epoch 141 iter 3 loss=0.03795153275132179\n",
      "epoch 141 iter 4 loss=0.05711136385798454\n",
      "epoch 141 iter 5 loss=0.030298104509711266\n",
      "epoch 141 iter 6 loss=0.022837094962596893\n",
      "epoch 141 iter 7 loss=0.033185508102178574\n",
      "epoch 141 iter 8 loss=0.03826878219842911\n",
      "epoch 141 iter 9 loss=0.04366186633706093\n",
      "epoch 141 iter 10 loss=0.03032802604138851\n",
      "epoch 141 iter 11 loss=0.05779355391860008\n",
      "epoch 141 iter 12 loss=0.030780622735619545\n",
      "epoch 141 iter 13 loss=0.04901638254523277\n",
      "epoch 141 iter 14 loss=0.035354506224393845\n",
      "epoch 141 iter 15 loss=0.03347781300544739\n",
      "epoch 141 iter 16 loss=0.02891131117939949\n",
      "epoch 141 iter 17 loss=0.06431399285793304\n",
      "epoch 141 iter 18 loss=0.032190218567848206\n",
      "epoch 141 iter 19 loss=0.029319291934370995\n",
      "epoch 141 iter 20 loss=0.04594465717673302\n",
      "epoch 141 iter 21 loss=0.035074569284915924\n",
      "epoch 141 iter 22 loss=0.040664829313755035\n",
      "epoch 141 iter 23 loss=0.038731373846530914\n",
      "epoch 141 iter 24 loss=0.03574150800704956\n",
      "epoch 141 iter 25 loss=0.04832396283745766\n",
      "epoch 141 iter 26 loss=0.04295407235622406\n",
      "epoch 141 iter 27 loss=0.03340498358011246\n",
      "epoch 141 iter 28 loss=0.03359760344028473\n",
      "epoch 141 iter 29 loss=0.01901036873459816\n",
      "epoch 141 iter 30 loss=0.045876871794462204\n",
      "epoch 141 iter 31 loss=0.043717652559280396\n",
      "epoch 141 iter 32 loss=0.025461377575993538\n",
      "epoch 141 iter 33 loss=0.029120126739144325\n",
      "epoch 141 iter 34 loss=0.04058901593089104\n",
      "epoch 141 iter 35 loss=0.04148508235812187\n",
      "epoch 141 iter 36 loss=0.038291241973638535\n",
      "epoch 141 iter 37 loss=0.03869517147541046\n",
      "epoch 141 iter 38 loss=0.04804220795631409\n",
      "epoch 141 iter 39 loss=0.048976168036460876\n",
      "epoch 141 iter 40 loss=0.044812411069869995\n",
      "epoch 141 iter 41 loss=0.040253788232803345\n",
      "epoch 141 iter 42 loss=0.050017356872558594\n",
      "epoch 141 iter 43 loss=0.032293692231178284\n",
      "epoch 141 iter 44 loss=0.050275519490242004\n",
      "epoch 141 iter 45 loss=0.05368608981370926\n",
      "epoch 141 iter 46 loss=0.04135223105549812\n",
      "epoch 141 iter 47 loss=0.029240088537335396\n",
      "epoch 141 iter 48 loss=0.038528334349393845\n",
      "epoch 141 iter 49 loss=0.03130215406417847\n",
      "epoch 141 iter 50 loss=0.032102908939123154\n",
      "epoch 141 iter 51 loss=0.03678945079445839\n",
      "epoch 141 iter 52 loss=0.05124317854642868\n",
      "epoch 141 iter 53 loss=0.025411594659090042\n",
      "epoch 141 iter 54 loss=0.03601999953389168\n",
      "epoch 141 iter 55 loss=0.04933326318860054\n",
      "epoch 141 iter 56 loss=0.02367888018488884\n",
      "epoch 141 iter 57 loss=0.036318760365247726\n",
      "epoch 141 iter 58 loss=0.027666611596941948\n",
      "epoch 141 iter 59 loss=0.02677031233906746\n",
      "epoch 141 iter 60 loss=0.03878319635987282\n",
      "epoch 141 iter 61 loss=0.02460893802344799\n",
      "epoch 141 iter 62 loss=0.028454873710870743\n",
      "epoch 141 iter 63 loss=0.03817905858159065\n",
      "epoch 141 iter 64 loss=0.04547199606895447\n",
      "epoch 141 iter 65 loss=0.051629748195409775\n",
      "epoch 141 iter 66 loss=0.03271539509296417\n",
      "epoch 141 iter 67 loss=0.03509080782532692\n",
      "epoch 141 iter 68 loss=0.04998732730746269\n",
      "epoch 141 iter 69 loss=0.045846689492464066\n",
      "epoch 141 iter 70 loss=0.047240354120731354\n",
      "epoch 141 iter 71 loss=0.023300208151340485\n",
      "epoch 141 iter 72 loss=0.04348134249448776\n",
      "epoch 141 iter 73 loss=0.04135406017303467\n",
      "epoch 141 iter 74 loss=0.02346356213092804\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3202.\n",
      "epoch 142 iter 0 loss=0.03117121011018753\n",
      "epoch 142 iter 1 loss=0.04201725497841835\n",
      "epoch 142 iter 2 loss=0.028591908514499664\n",
      "epoch 142 iter 3 loss=0.04110053554177284\n",
      "epoch 142 iter 4 loss=0.05426320806145668\n",
      "epoch 142 iter 5 loss=0.03215291351079941\n",
      "epoch 142 iter 6 loss=0.037835635244846344\n",
      "epoch 142 iter 7 loss=0.06261242181062698\n",
      "epoch 142 iter 8 loss=0.03591006249189377\n",
      "epoch 142 iter 9 loss=0.04586024582386017\n",
      "epoch 142 iter 10 loss=0.030912019312381744\n",
      "epoch 142 iter 11 loss=0.05346843600273132\n",
      "epoch 142 iter 12 loss=0.03646266832947731\n",
      "epoch 142 iter 13 loss=0.026259958744049072\n",
      "epoch 142 iter 14 loss=0.033987827599048615\n",
      "epoch 142 iter 15 loss=0.02352491393685341\n",
      "epoch 142 iter 16 loss=0.04490848630666733\n",
      "epoch 142 iter 17 loss=0.04023068770766258\n",
      "epoch 142 iter 18 loss=0.047047436237335205\n",
      "epoch 142 iter 19 loss=0.030269693583250046\n",
      "epoch 142 iter 20 loss=0.04575226455926895\n",
      "epoch 142 iter 21 loss=0.04549553617835045\n",
      "epoch 142 iter 22 loss=0.024867625907063484\n",
      "epoch 142 iter 23 loss=0.0492483414709568\n",
      "epoch 142 iter 24 loss=0.029735634103417397\n",
      "epoch 142 iter 25 loss=0.035928577184677124\n",
      "epoch 142 iter 26 loss=0.06032659113407135\n",
      "epoch 142 iter 27 loss=0.05086466670036316\n",
      "epoch 142 iter 28 loss=0.048591360449790955\n",
      "epoch 142 iter 29 loss=0.03522016853094101\n",
      "epoch 142 iter 30 loss=0.030896974727511406\n",
      "epoch 142 iter 31 loss=0.049326252192258835\n",
      "epoch 142 iter 32 loss=0.039632637053728104\n",
      "epoch 142 iter 33 loss=0.034520916640758514\n",
      "epoch 142 iter 34 loss=0.06961707025766373\n",
      "epoch 142 iter 35 loss=0.03622303158044815\n",
      "epoch 142 iter 36 loss=0.023792104795575142\n",
      "epoch 142 iter 37 loss=0.05047698691487312\n",
      "epoch 142 iter 38 loss=0.027752624824643135\n",
      "epoch 142 iter 39 loss=0.048078011721372604\n",
      "epoch 142 iter 40 loss=0.028603389859199524\n",
      "epoch 142 iter 41 loss=0.03875454515218735\n",
      "epoch 142 iter 42 loss=0.03965756669640541\n",
      "epoch 142 iter 43 loss=0.04221624881029129\n",
      "epoch 142 iter 44 loss=0.03981844708323479\n",
      "epoch 142 iter 45 loss=0.046039044857025146\n",
      "epoch 142 iter 46 loss=0.03400886431336403\n",
      "epoch 142 iter 47 loss=0.026640750467777252\n",
      "epoch 142 iter 48 loss=0.05640943720936775\n",
      "epoch 142 iter 49 loss=0.042228907346725464\n",
      "epoch 142 iter 50 loss=0.021973026916384697\n",
      "epoch 142 iter 51 loss=0.021920954808592796\n",
      "epoch 142 iter 52 loss=0.031830865889787674\n",
      "epoch 142 iter 53 loss=0.040662866085767746\n",
      "epoch 142 iter 54 loss=0.04742685332894325\n",
      "epoch 142 iter 55 loss=0.03313569724559784\n",
      "epoch 142 iter 56 loss=0.04468217119574547\n",
      "epoch 142 iter 57 loss=0.03755798935890198\n",
      "epoch 142 iter 58 loss=0.017324501648545265\n",
      "epoch 142 iter 59 loss=0.028220871463418007\n",
      "epoch 142 iter 60 loss=0.051858749240636826\n",
      "epoch 142 iter 61 loss=0.028441451489925385\n",
      "epoch 142 iter 62 loss=0.04082299768924713\n",
      "epoch 142 iter 63 loss=0.05237992852926254\n",
      "epoch 142 iter 64 loss=0.03496133163571358\n",
      "epoch 142 iter 65 loss=0.02036138065159321\n",
      "epoch 142 iter 66 loss=0.031149830669164658\n",
      "epoch 142 iter 67 loss=0.028413871303200722\n",
      "epoch 142 iter 68 loss=0.03601023927330971\n",
      "epoch 142 iter 69 loss=0.04020722955465317\n",
      "epoch 142 iter 70 loss=0.046771127730607986\n",
      "epoch 142 iter 71 loss=0.026680253446102142\n",
      "epoch 142 iter 72 loss=0.03515725955367088\n",
      "epoch 142 iter 73 loss=0.06446585059165955\n",
      "epoch 142 iter 74 loss=0.021313544362783432\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3205.\n",
      "epoch 143 iter 0 loss=0.04311032220721245\n",
      "epoch 143 iter 1 loss=0.024819498881697655\n",
      "epoch 143 iter 2 loss=0.02399456314742565\n",
      "epoch 143 iter 3 loss=0.039005544036626816\n",
      "epoch 143 iter 4 loss=0.03808796778321266\n",
      "epoch 143 iter 5 loss=0.027827156707644463\n",
      "epoch 143 iter 6 loss=0.04142108932137489\n",
      "epoch 143 iter 7 loss=0.02754831872880459\n",
      "epoch 143 iter 8 loss=0.049247752875089645\n",
      "epoch 143 iter 9 loss=0.043163493275642395\n",
      "epoch 143 iter 10 loss=0.03270319104194641\n",
      "epoch 143 iter 11 loss=0.042408887296915054\n",
      "epoch 143 iter 12 loss=0.034755490720272064\n",
      "epoch 143 iter 13 loss=0.03008100762963295\n",
      "epoch 143 iter 14 loss=0.030607787892222404\n",
      "epoch 143 iter 15 loss=0.0421125702559948\n",
      "epoch 143 iter 16 loss=0.019888704642653465\n",
      "epoch 143 iter 17 loss=0.05914032831788063\n",
      "epoch 143 iter 18 loss=0.028188472613692284\n",
      "epoch 143 iter 19 loss=0.04965711385011673\n",
      "epoch 143 iter 20 loss=0.043336182832717896\n",
      "epoch 143 iter 21 loss=0.04451798275113106\n",
      "epoch 143 iter 22 loss=0.055791791528463364\n",
      "epoch 143 iter 23 loss=0.04079224541783333\n",
      "epoch 143 iter 24 loss=0.03797996789216995\n",
      "epoch 143 iter 25 loss=0.040472473949193954\n",
      "epoch 143 iter 26 loss=0.046135272830724716\n",
      "epoch 143 iter 27 loss=0.0466957613825798\n",
      "epoch 143 iter 28 loss=0.04155780002474785\n",
      "epoch 143 iter 29 loss=0.04432598128914833\n",
      "epoch 143 iter 30 loss=0.041303638368844986\n",
      "epoch 143 iter 31 loss=0.041556719690561295\n",
      "epoch 143 iter 32 loss=0.03899846598505974\n",
      "epoch 143 iter 33 loss=0.04383836314082146\n",
      "epoch 143 iter 34 loss=0.03149237111210823\n",
      "epoch 143 iter 35 loss=0.05010223761200905\n",
      "epoch 143 iter 36 loss=0.03492923453450203\n",
      "epoch 143 iter 37 loss=0.03613593429327011\n",
      "epoch 143 iter 38 loss=0.01758411154150963\n",
      "epoch 143 iter 39 loss=0.023380961269140244\n",
      "epoch 143 iter 40 loss=0.04838157445192337\n",
      "epoch 143 iter 41 loss=0.04214998707175255\n",
      "epoch 143 iter 42 loss=0.02124539203941822\n",
      "epoch 143 iter 43 loss=0.040644723922014236\n",
      "epoch 143 iter 44 loss=0.050909675657749176\n",
      "epoch 143 iter 45 loss=0.0360313318669796\n",
      "epoch 143 iter 46 loss=0.06343299150466919\n",
      "epoch 143 iter 47 loss=0.04213249683380127\n",
      "epoch 143 iter 48 loss=0.03676333278417587\n",
      "epoch 143 iter 49 loss=0.045321814715862274\n",
      "epoch 143 iter 50 loss=0.03602398559451103\n",
      "epoch 143 iter 51 loss=0.05528438091278076\n",
      "epoch 143 iter 52 loss=0.05913207307457924\n",
      "epoch 143 iter 53 loss=0.0521567203104496\n",
      "epoch 143 iter 54 loss=0.04509531334042549\n",
      "epoch 143 iter 55 loss=0.03540472313761711\n",
      "epoch 143 iter 56 loss=0.039477117359638214\n",
      "epoch 143 iter 57 loss=0.04255901277065277\n",
      "epoch 143 iter 58 loss=0.04644398391246796\n",
      "epoch 143 iter 59 loss=0.03839220479130745\n",
      "epoch 143 iter 60 loss=0.031086737290024757\n",
      "epoch 143 iter 61 loss=0.03964393585920334\n",
      "epoch 143 iter 62 loss=0.02020914852619171\n",
      "epoch 143 iter 63 loss=0.0424351841211319\n",
      "epoch 143 iter 64 loss=0.03384430706501007\n",
      "epoch 143 iter 65 loss=0.02277432754635811\n",
      "epoch 143 iter 66 loss=0.03834535926580429\n",
      "epoch 143 iter 67 loss=0.039114441722631454\n",
      "epoch 143 iter 68 loss=0.04478619620203972\n",
      "epoch 143 iter 69 loss=0.019101759418845177\n",
      "epoch 143 iter 70 loss=0.03552207350730896\n",
      "epoch 143 iter 71 loss=0.025336066260933876\n",
      "epoch 143 iter 72 loss=0.021260006353259087\n",
      "epoch 143 iter 73 loss=0.028465131297707558\n",
      "epoch 143 iter 74 loss=0.035513270646333694\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3260.\n",
      "epoch 144 iter 0 loss=0.04132784157991409\n",
      "epoch 144 iter 1 loss=0.03342285007238388\n",
      "epoch 144 iter 2 loss=0.025110559538006783\n",
      "epoch 144 iter 3 loss=0.02132248692214489\n",
      "epoch 144 iter 4 loss=0.02525683306157589\n",
      "epoch 144 iter 5 loss=0.03466261550784111\n",
      "epoch 144 iter 6 loss=0.015567220747470856\n",
      "epoch 144 iter 7 loss=0.02175496146082878\n",
      "epoch 144 iter 8 loss=0.036161139607429504\n",
      "epoch 144 iter 9 loss=0.028876034542918205\n",
      "epoch 144 iter 10 loss=0.05061860755085945\n",
      "epoch 144 iter 11 loss=0.058730144053697586\n",
      "epoch 144 iter 12 loss=0.05909663438796997\n",
      "epoch 144 iter 13 loss=0.05104324221611023\n",
      "epoch 144 iter 14 loss=0.044350240379571915\n",
      "epoch 144 iter 15 loss=0.03429470211267471\n",
      "epoch 144 iter 16 loss=0.0470217764377594\n",
      "epoch 144 iter 17 loss=0.02538791298866272\n",
      "epoch 144 iter 18 loss=0.036605071276426315\n",
      "epoch 144 iter 19 loss=0.03713025152683258\n",
      "epoch 144 iter 20 loss=0.03959830850362778\n",
      "epoch 144 iter 21 loss=0.047420892864465714\n",
      "epoch 144 iter 22 loss=0.04032180458307266\n",
      "epoch 144 iter 23 loss=0.03106648474931717\n",
      "epoch 144 iter 24 loss=0.04278905689716339\n",
      "epoch 144 iter 25 loss=0.028972048312425613\n",
      "epoch 144 iter 26 loss=0.03211448714137077\n",
      "epoch 144 iter 27 loss=0.031339533627033234\n",
      "epoch 144 iter 28 loss=0.03653720021247864\n",
      "epoch 144 iter 29 loss=0.034416865557432175\n",
      "epoch 144 iter 30 loss=0.04354240000247955\n",
      "epoch 144 iter 31 loss=0.03610880300402641\n",
      "epoch 144 iter 32 loss=0.03965719789266586\n",
      "epoch 144 iter 33 loss=0.023577943444252014\n",
      "epoch 144 iter 34 loss=0.03391057625412941\n",
      "epoch 144 iter 35 loss=0.04030149057507515\n",
      "epoch 144 iter 36 loss=0.04257199540734291\n",
      "epoch 144 iter 37 loss=0.04549933969974518\n",
      "epoch 144 iter 38 loss=0.04016309604048729\n",
      "epoch 144 iter 39 loss=0.04085913673043251\n",
      "epoch 144 iter 40 loss=0.04982255771756172\n",
      "epoch 144 iter 41 loss=0.04905502125620842\n",
      "epoch 144 iter 42 loss=0.032005906105041504\n",
      "epoch 144 iter 43 loss=0.06153291463851929\n",
      "epoch 144 iter 44 loss=0.03795592859387398\n",
      "epoch 144 iter 45 loss=0.048558421432971954\n",
      "epoch 144 iter 46 loss=0.028814485296607018\n",
      "epoch 144 iter 47 loss=0.02760212868452072\n",
      "epoch 144 iter 48 loss=0.04506085440516472\n",
      "epoch 144 iter 49 loss=0.0634298026561737\n",
      "epoch 144 iter 50 loss=0.03522967919707298\n",
      "epoch 144 iter 51 loss=0.03224216401576996\n",
      "epoch 144 iter 52 loss=0.02833571471273899\n",
      "epoch 144 iter 53 loss=0.02500985935330391\n",
      "epoch 144 iter 54 loss=0.05076971277594566\n",
      "epoch 144 iter 55 loss=0.02498166263103485\n",
      "epoch 144 iter 56 loss=0.02623424492776394\n",
      "epoch 144 iter 57 loss=0.03352522477507591\n",
      "epoch 144 iter 58 loss=0.04027611017227173\n",
      "epoch 144 iter 59 loss=0.04829717054963112\n",
      "epoch 144 iter 60 loss=0.0560079924762249\n",
      "epoch 144 iter 61 loss=0.04180851951241493\n",
      "epoch 144 iter 62 loss=0.04078157618641853\n",
      "epoch 144 iter 63 loss=0.046551335602998734\n",
      "epoch 144 iter 64 loss=0.04222436994314194\n",
      "epoch 144 iter 65 loss=0.038929373025894165\n",
      "epoch 144 iter 66 loss=0.03593950718641281\n",
      "epoch 144 iter 67 loss=0.03996990993618965\n",
      "epoch 144 iter 68 loss=0.03316757082939148\n",
      "epoch 144 iter 69 loss=0.029625853523612022\n",
      "epoch 144 iter 70 loss=0.033432502299547195\n",
      "epoch 144 iter 71 loss=0.02495260164141655\n",
      "epoch 144 iter 72 loss=0.04001894220709801\n",
      "epoch 144 iter 73 loss=0.052629053592681885\n",
      "epoch 144 iter 74 loss=0.040548499673604965\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3190.\n",
      "epoch 145 iter 0 loss=0.038909777998924255\n",
      "epoch 145 iter 1 loss=0.03847143054008484\n",
      "epoch 145 iter 2 loss=0.04770608991384506\n",
      "epoch 145 iter 3 loss=0.024901116266846657\n",
      "epoch 145 iter 4 loss=0.031146634370088577\n",
      "epoch 145 iter 5 loss=0.04534272477030754\n",
      "epoch 145 iter 6 loss=0.05064874887466431\n",
      "epoch 145 iter 7 loss=0.05366932973265648\n",
      "epoch 145 iter 8 loss=0.03941196948289871\n",
      "epoch 145 iter 9 loss=0.023425837978720665\n",
      "epoch 145 iter 10 loss=0.06971719115972519\n",
      "epoch 145 iter 11 loss=0.04361845925450325\n",
      "epoch 145 iter 12 loss=0.022905753925442696\n",
      "epoch 145 iter 13 loss=0.049234628677368164\n",
      "epoch 145 iter 14 loss=0.03416689485311508\n",
      "epoch 145 iter 15 loss=0.03546427935361862\n",
      "epoch 145 iter 16 loss=0.05874742195010185\n",
      "epoch 145 iter 17 loss=0.051858820021152496\n",
      "epoch 145 iter 18 loss=0.03535674884915352\n",
      "epoch 145 iter 19 loss=0.029176140204072\n",
      "epoch 145 iter 20 loss=0.05201106145977974\n",
      "epoch 145 iter 21 loss=0.04714502766728401\n",
      "epoch 145 iter 22 loss=0.038387998938560486\n",
      "epoch 145 iter 23 loss=0.01906862109899521\n",
      "epoch 145 iter 24 loss=0.03675477206707001\n",
      "epoch 145 iter 25 loss=0.036027535796165466\n",
      "epoch 145 iter 26 loss=0.031058188527822495\n",
      "epoch 145 iter 27 loss=0.020833680406212807\n",
      "epoch 145 iter 28 loss=0.05345754697918892\n",
      "epoch 145 iter 29 loss=0.022287685424089432\n",
      "epoch 145 iter 30 loss=0.06198187917470932\n",
      "epoch 145 iter 31 loss=0.036356061697006226\n",
      "epoch 145 iter 32 loss=0.027575869113206863\n",
      "epoch 145 iter 33 loss=0.03217921406030655\n",
      "epoch 145 iter 34 loss=0.027157874777913094\n",
      "epoch 145 iter 35 loss=0.034116894006729126\n",
      "epoch 145 iter 36 loss=0.0593942254781723\n",
      "epoch 145 iter 37 loss=0.026422644034028053\n",
      "epoch 145 iter 38 loss=0.03238585591316223\n",
      "epoch 145 iter 39 loss=0.029518386349081993\n",
      "epoch 145 iter 40 loss=0.03584175929427147\n",
      "epoch 145 iter 41 loss=0.03962241858243942\n",
      "epoch 145 iter 42 loss=0.02201988734304905\n",
      "epoch 145 iter 43 loss=0.02299727126955986\n",
      "epoch 145 iter 44 loss=0.040061499923467636\n",
      "epoch 145 iter 45 loss=0.035845499485731125\n",
      "epoch 145 iter 46 loss=0.04175624996423721\n",
      "epoch 145 iter 47 loss=0.037482473999261856\n",
      "epoch 145 iter 48 loss=0.04698207229375839\n",
      "epoch 145 iter 49 loss=0.044789452105760574\n",
      "epoch 145 iter 50 loss=0.03311115503311157\n",
      "epoch 145 iter 51 loss=0.03756006434559822\n",
      "epoch 145 iter 52 loss=0.03737223520874977\n",
      "epoch 145 iter 53 loss=0.039999835193157196\n",
      "epoch 145 iter 54 loss=0.04004903882741928\n",
      "epoch 145 iter 55 loss=0.03854324668645859\n",
      "epoch 145 iter 56 loss=0.02475951984524727\n",
      "epoch 145 iter 57 loss=0.042976949363946915\n",
      "epoch 145 iter 58 loss=0.04888813942670822\n",
      "epoch 145 iter 59 loss=0.03158427029848099\n",
      "epoch 145 iter 60 loss=0.05522554740309715\n",
      "epoch 145 iter 61 loss=0.047360170632600784\n",
      "epoch 145 iter 62 loss=0.034735921770334244\n",
      "epoch 145 iter 63 loss=0.057876117527484894\n",
      "epoch 145 iter 64 loss=0.021584033966064453\n",
      "epoch 145 iter 65 loss=0.031309906393289566\n",
      "epoch 145 iter 66 loss=0.038038622587919235\n",
      "epoch 145 iter 67 loss=0.02869797870516777\n",
      "epoch 145 iter 68 loss=0.031199485063552856\n",
      "epoch 145 iter 69 loss=0.04083067923784256\n",
      "epoch 145 iter 70 loss=0.0554230622947216\n",
      "epoch 145 iter 71 loss=0.026668593287467957\n",
      "epoch 145 iter 72 loss=0.04415830224752426\n",
      "epoch 145 iter 73 loss=0.03575807437300682\n",
      "epoch 145 iter 74 loss=0.031527552753686905\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3186.\n",
      "epoch 146 iter 0 loss=0.03713206201791763\n",
      "epoch 146 iter 1 loss=0.047528427094221115\n",
      "epoch 146 iter 2 loss=0.030219189822673798\n",
      "epoch 146 iter 3 loss=0.023735160008072853\n",
      "epoch 146 iter 4 loss=0.030360717326402664\n",
      "epoch 146 iter 5 loss=0.013191762380301952\n",
      "epoch 146 iter 6 loss=0.01943543367087841\n",
      "epoch 146 iter 7 loss=0.038646962493658066\n",
      "epoch 146 iter 8 loss=0.046441543847322464\n",
      "epoch 146 iter 9 loss=0.03278448060154915\n",
      "epoch 146 iter 10 loss=0.0321805477142334\n",
      "epoch 146 iter 11 loss=0.029949087649583817\n",
      "epoch 146 iter 12 loss=0.06818374246358871\n",
      "epoch 146 iter 13 loss=0.0510014183819294\n",
      "epoch 146 iter 14 loss=0.04484681040048599\n",
      "epoch 146 iter 15 loss=0.03833325579762459\n",
      "epoch 146 iter 16 loss=0.05131509155035019\n",
      "epoch 146 iter 17 loss=0.05587486922740936\n",
      "epoch 146 iter 18 loss=0.04082157090306282\n",
      "epoch 146 iter 19 loss=0.04004911333322525\n",
      "epoch 146 iter 20 loss=0.03730509430170059\n",
      "epoch 146 iter 21 loss=0.042177632451057434\n",
      "epoch 146 iter 22 loss=0.0319056510925293\n",
      "epoch 146 iter 23 loss=0.05312880873680115\n",
      "epoch 146 iter 24 loss=0.0321313701570034\n",
      "epoch 146 iter 25 loss=0.024583008140325546\n",
      "epoch 146 iter 26 loss=0.039410579949617386\n",
      "epoch 146 iter 27 loss=0.05410674586892128\n",
      "epoch 146 iter 28 loss=0.029044900089502335\n",
      "epoch 146 iter 29 loss=0.045588839799165726\n",
      "epoch 146 iter 30 loss=0.046900730580091476\n",
      "epoch 146 iter 31 loss=0.03292493149638176\n",
      "epoch 146 iter 32 loss=0.05107394605875015\n",
      "epoch 146 iter 33 loss=0.027911923825740814\n",
      "epoch 146 iter 34 loss=0.04734533652663231\n",
      "epoch 146 iter 35 loss=0.04794957488775253\n",
      "epoch 146 iter 36 loss=0.033690016716718674\n",
      "epoch 146 iter 37 loss=0.02894083596765995\n",
      "epoch 146 iter 38 loss=0.021687274798750877\n",
      "epoch 146 iter 39 loss=0.03176001086831093\n",
      "epoch 146 iter 40 loss=0.03845415636897087\n",
      "epoch 146 iter 41 loss=0.02713351882994175\n",
      "epoch 146 iter 42 loss=0.07379179447889328\n",
      "epoch 146 iter 43 loss=0.033622659742832184\n",
      "epoch 146 iter 44 loss=0.03628789260983467\n",
      "epoch 146 iter 45 loss=0.02210540696978569\n",
      "epoch 146 iter 46 loss=0.03362424671649933\n",
      "epoch 146 iter 47 loss=0.029338113963603973\n",
      "epoch 146 iter 48 loss=0.04956173151731491\n",
      "epoch 146 iter 49 loss=0.059241004288196564\n",
      "epoch 146 iter 50 loss=0.026909345760941505\n",
      "epoch 146 iter 51 loss=0.02269037440419197\n",
      "epoch 146 iter 52 loss=0.0312875360250473\n",
      "epoch 146 iter 53 loss=0.035354841500520706\n",
      "epoch 146 iter 54 loss=0.024114152416586876\n",
      "epoch 146 iter 55 loss=0.039490438997745514\n",
      "epoch 146 iter 56 loss=0.054062362760305405\n",
      "epoch 146 iter 57 loss=0.06605816632509232\n",
      "epoch 146 iter 58 loss=0.03214263543486595\n",
      "epoch 146 iter 59 loss=0.032713633030653\n",
      "epoch 146 iter 60 loss=0.03794808313250542\n",
      "epoch 146 iter 61 loss=0.026490474119782448\n",
      "epoch 146 iter 62 loss=0.03993365913629532\n",
      "epoch 146 iter 63 loss=0.025772161781787872\n",
      "epoch 146 iter 64 loss=0.04255828633904457\n",
      "epoch 146 iter 65 loss=0.04165910556912422\n",
      "epoch 146 iter 66 loss=0.028278661891818047\n",
      "epoch 146 iter 67 loss=0.06045972928404808\n",
      "epoch 146 iter 68 loss=0.032531626522541046\n",
      "epoch 146 iter 69 loss=0.03792246803641319\n",
      "epoch 146 iter 70 loss=0.036207642406225204\n",
      "epoch 146 iter 71 loss=0.03843621164560318\n",
      "epoch 146 iter 72 loss=0.038757678121328354\n",
      "epoch 146 iter 73 loss=0.03157826513051987\n",
      "epoch 146 iter 74 loss=0.043731726706027985\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3216.\n",
      "epoch 147 iter 0 loss=0.027220040559768677\n",
      "epoch 147 iter 1 loss=0.04617350548505783\n",
      "epoch 147 iter 2 loss=0.04436212405562401\n",
      "epoch 147 iter 3 loss=0.02124796248972416\n",
      "epoch 147 iter 4 loss=0.02008548378944397\n",
      "epoch 147 iter 5 loss=0.03253740072250366\n",
      "epoch 147 iter 6 loss=0.032013244926929474\n",
      "epoch 147 iter 7 loss=0.04984002187848091\n",
      "epoch 147 iter 8 loss=0.05489566922187805\n",
      "epoch 147 iter 9 loss=0.023860815912485123\n",
      "epoch 147 iter 10 loss=0.031615711748600006\n",
      "epoch 147 iter 11 loss=0.03381374850869179\n",
      "epoch 147 iter 12 loss=0.04305388778448105\n",
      "epoch 147 iter 13 loss=0.04556500166654587\n",
      "epoch 147 iter 14 loss=0.03779127821326256\n",
      "epoch 147 iter 15 loss=0.022535983473062515\n",
      "epoch 147 iter 16 loss=0.03178131580352783\n",
      "epoch 147 iter 17 loss=0.048843689262866974\n",
      "epoch 147 iter 18 loss=0.026907917112112045\n",
      "epoch 147 iter 19 loss=0.022114966064691544\n",
      "epoch 147 iter 20 loss=0.02761569619178772\n",
      "epoch 147 iter 21 loss=0.039551109075546265\n",
      "epoch 147 iter 22 loss=0.045188020914793015\n",
      "epoch 147 iter 23 loss=0.025742994621396065\n",
      "epoch 147 iter 24 loss=0.025366002693772316\n",
      "epoch 147 iter 25 loss=0.03106239065527916\n",
      "epoch 147 iter 26 loss=0.042184483259916306\n",
      "epoch 147 iter 27 loss=0.059628259390592575\n",
      "epoch 147 iter 28 loss=0.05464775115251541\n",
      "epoch 147 iter 29 loss=0.06596878916025162\n",
      "epoch 147 iter 30 loss=0.03529944643378258\n",
      "epoch 147 iter 31 loss=0.045374006032943726\n",
      "epoch 147 iter 32 loss=0.029734289273619652\n",
      "epoch 147 iter 33 loss=0.026679834350943565\n",
      "epoch 147 iter 34 loss=0.034282997250556946\n",
      "epoch 147 iter 35 loss=0.02509075216948986\n",
      "epoch 147 iter 36 loss=0.026959901675581932\n",
      "epoch 147 iter 37 loss=0.022398196160793304\n",
      "epoch 147 iter 38 loss=0.04165744036436081\n",
      "epoch 147 iter 39 loss=0.03709530085325241\n",
      "epoch 147 iter 40 loss=0.03678654506802559\n",
      "epoch 147 iter 41 loss=0.030766213312745094\n",
      "epoch 147 iter 42 loss=0.044443901628255844\n",
      "epoch 147 iter 43 loss=0.025294147431850433\n",
      "epoch 147 iter 44 loss=0.0474867969751358\n",
      "epoch 147 iter 45 loss=0.0419539213180542\n",
      "epoch 147 iter 46 loss=0.053387049585580826\n",
      "epoch 147 iter 47 loss=0.04320367053151131\n",
      "epoch 147 iter 48 loss=0.04056220129132271\n",
      "epoch 147 iter 49 loss=0.026843754574656487\n",
      "epoch 147 iter 50 loss=0.0205902811139822\n",
      "epoch 147 iter 51 loss=0.04712129384279251\n",
      "epoch 147 iter 52 loss=0.05803123489022255\n",
      "epoch 147 iter 53 loss=0.045180995017290115\n",
      "epoch 147 iter 54 loss=0.047089122235774994\n",
      "epoch 147 iter 55 loss=0.021511679515242577\n",
      "epoch 147 iter 56 loss=0.03966136276721954\n",
      "epoch 147 iter 57 loss=0.02848982997238636\n",
      "epoch 147 iter 58 loss=0.03547346219420433\n",
      "epoch 147 iter 59 loss=0.04607124254107475\n",
      "epoch 147 iter 60 loss=0.037266604602336884\n",
      "epoch 147 iter 61 loss=0.036028776317834854\n",
      "epoch 147 iter 62 loss=0.028230026364326477\n",
      "epoch 147 iter 63 loss=0.04396815970540047\n",
      "epoch 147 iter 64 loss=0.03445650264620781\n",
      "epoch 147 iter 65 loss=0.037849992513656616\n",
      "epoch 147 iter 66 loss=0.05113755911588669\n",
      "epoch 147 iter 67 loss=0.04067991301417351\n",
      "epoch 147 iter 68 loss=0.041954975575208664\n",
      "epoch 147 iter 69 loss=0.028835101053118706\n",
      "epoch 147 iter 70 loss=0.03567207232117653\n",
      "epoch 147 iter 71 loss=0.027628354728221893\n",
      "epoch 147 iter 72 loss=0.047178201377391815\n",
      "epoch 147 iter 73 loss=0.030830081552267075\n",
      "epoch 147 iter 74 loss=0.05231951177120209\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3194.\n",
      "epoch 148 iter 0 loss=0.06122714281082153\n",
      "epoch 148 iter 1 loss=0.058732371777296066\n",
      "epoch 148 iter 2 loss=0.03893039748072624\n",
      "epoch 148 iter 3 loss=0.04963834956288338\n",
      "epoch 148 iter 4 loss=0.04762168973684311\n",
      "epoch 148 iter 5 loss=0.020367491990327835\n",
      "epoch 148 iter 6 loss=0.02243838831782341\n",
      "epoch 148 iter 7 loss=0.03618011996150017\n",
      "epoch 148 iter 8 loss=0.02656100131571293\n",
      "epoch 148 iter 9 loss=0.032697893679142\n",
      "epoch 148 iter 10 loss=0.03638877719640732\n",
      "epoch 148 iter 11 loss=0.03405878692865372\n",
      "epoch 148 iter 12 loss=0.04083794727921486\n",
      "epoch 148 iter 13 loss=0.03205921873450279\n",
      "epoch 148 iter 14 loss=0.039741575717926025\n",
      "epoch 148 iter 15 loss=0.048416197299957275\n",
      "epoch 148 iter 16 loss=0.026729058474302292\n",
      "epoch 148 iter 17 loss=0.030778760090470314\n",
      "epoch 148 iter 18 loss=0.04661417007446289\n",
      "epoch 148 iter 19 loss=0.05357751622796059\n",
      "epoch 148 iter 20 loss=0.04068005084991455\n",
      "epoch 148 iter 21 loss=0.025331519544124603\n",
      "epoch 148 iter 22 loss=0.03500320389866829\n",
      "epoch 148 iter 23 loss=0.041535090655088425\n",
      "epoch 148 iter 24 loss=0.032791778445243835\n",
      "epoch 148 iter 25 loss=0.03672394901514053\n",
      "epoch 148 iter 26 loss=0.029380882158875465\n",
      "epoch 148 iter 27 loss=0.049438297748565674\n",
      "epoch 148 iter 28 loss=0.023733794689178467\n",
      "epoch 148 iter 29 loss=0.048467524349689484\n",
      "epoch 148 iter 30 loss=0.02723556011915207\n",
      "epoch 148 iter 31 loss=0.021710224449634552\n",
      "epoch 148 iter 32 loss=0.04625937342643738\n",
      "epoch 148 iter 33 loss=0.04269057512283325\n",
      "epoch 148 iter 34 loss=0.04156035929918289\n",
      "epoch 148 iter 35 loss=0.03854329138994217\n",
      "epoch 148 iter 36 loss=0.0431436263024807\n",
      "epoch 148 iter 37 loss=0.038285814225673676\n",
      "epoch 148 iter 38 loss=0.03277064114809036\n",
      "epoch 148 iter 39 loss=0.03133298084139824\n",
      "epoch 148 iter 40 loss=0.05208481848239899\n",
      "epoch 148 iter 41 loss=0.0713789239525795\n",
      "epoch 148 iter 42 loss=0.022584808990359306\n",
      "epoch 148 iter 43 loss=0.04107551649212837\n",
      "epoch 148 iter 44 loss=0.04873092845082283\n",
      "epoch 148 iter 45 loss=0.024163132533431053\n",
      "epoch 148 iter 46 loss=0.019425218924880028\n",
      "epoch 148 iter 47 loss=0.0409875214099884\n",
      "epoch 148 iter 48 loss=0.049015406519174576\n",
      "epoch 148 iter 49 loss=0.038203027099370956\n",
      "epoch 148 iter 50 loss=0.03730588033795357\n",
      "epoch 148 iter 51 loss=0.03618858754634857\n",
      "epoch 148 iter 52 loss=0.026122217997908592\n",
      "epoch 148 iter 53 loss=0.04502078518271446\n",
      "epoch 148 iter 54 loss=0.04807811602950096\n",
      "epoch 148 iter 55 loss=0.020894855260849\n",
      "epoch 148 iter 56 loss=0.02219979837536812\n",
      "epoch 148 iter 57 loss=0.029705945402383804\n",
      "epoch 148 iter 58 loss=0.02374931052327156\n",
      "epoch 148 iter 59 loss=0.024223744869232178\n",
      "epoch 148 iter 60 loss=0.06314170360565186\n",
      "epoch 148 iter 61 loss=0.046421803534030914\n",
      "epoch 148 iter 62 loss=0.039606597274541855\n",
      "epoch 148 iter 63 loss=0.029068810865283012\n",
      "epoch 148 iter 64 loss=0.018006274476647377\n",
      "epoch 148 iter 65 loss=0.032487884163856506\n",
      "epoch 148 iter 66 loss=0.05891454964876175\n",
      "epoch 148 iter 67 loss=0.0276296716183424\n",
      "epoch 148 iter 68 loss=0.04005232825875282\n",
      "epoch 148 iter 69 loss=0.02880740724503994\n",
      "epoch 148 iter 70 loss=0.046838968992233276\n",
      "epoch 148 iter 71 loss=0.02531350590288639\n",
      "epoch 148 iter 72 loss=0.03919019177556038\n",
      "epoch 148 iter 73 loss=0.028616677969694138\n",
      "epoch 148 iter 74 loss=0.046298056840896606\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3195.\n",
      "epoch 149 iter 0 loss=0.03286931291222572\n",
      "epoch 149 iter 1 loss=0.04658305644989014\n",
      "epoch 149 iter 2 loss=0.054507702589035034\n",
      "epoch 149 iter 3 loss=0.021782197058200836\n",
      "epoch 149 iter 4 loss=0.03257458657026291\n",
      "epoch 149 iter 5 loss=0.03383972868323326\n",
      "epoch 149 iter 6 loss=0.041552986949682236\n",
      "epoch 149 iter 7 loss=0.033971697092056274\n",
      "epoch 149 iter 8 loss=0.04699140414595604\n",
      "epoch 149 iter 9 loss=0.029833732172846794\n",
      "epoch 149 iter 10 loss=0.03624404966831207\n",
      "epoch 149 iter 11 loss=0.05581469461321831\n",
      "epoch 149 iter 12 loss=0.029541363939642906\n",
      "epoch 149 iter 13 loss=0.028545193374156952\n",
      "epoch 149 iter 14 loss=0.04122908040881157\n",
      "epoch 149 iter 15 loss=0.049901995807886124\n",
      "epoch 149 iter 16 loss=0.0317278653383255\n",
      "epoch 149 iter 17 loss=0.036597125232219696\n",
      "epoch 149 iter 18 loss=0.02427498996257782\n",
      "epoch 149 iter 19 loss=0.031194563955068588\n",
      "epoch 149 iter 20 loss=0.028804345056414604\n",
      "epoch 149 iter 21 loss=0.05208718776702881\n",
      "epoch 149 iter 22 loss=0.038102779537439346\n",
      "epoch 149 iter 23 loss=0.0538766048848629\n",
      "epoch 149 iter 24 loss=0.03074537217617035\n",
      "epoch 149 iter 25 loss=0.04274831339716911\n",
      "epoch 149 iter 26 loss=0.014859392307698727\n",
      "epoch 149 iter 27 loss=0.044498082250356674\n",
      "epoch 149 iter 28 loss=0.04292956739664078\n",
      "epoch 149 iter 29 loss=0.027518315240740776\n",
      "epoch 149 iter 30 loss=0.034008387476205826\n",
      "epoch 149 iter 31 loss=0.03189782798290253\n",
      "epoch 149 iter 32 loss=0.04170221835374832\n",
      "epoch 149 iter 33 loss=0.028125446289777756\n",
      "epoch 149 iter 34 loss=0.03500913828611374\n",
      "epoch 149 iter 35 loss=0.04055147245526314\n",
      "epoch 149 iter 36 loss=0.04976031184196472\n",
      "epoch 149 iter 37 loss=0.05027756467461586\n",
      "epoch 149 iter 38 loss=0.01822446472942829\n",
      "epoch 149 iter 39 loss=0.052798181772232056\n",
      "epoch 149 iter 40 loss=0.015212874859571457\n",
      "epoch 149 iter 41 loss=0.029279951006174088\n",
      "epoch 149 iter 42 loss=0.04088054224848747\n",
      "epoch 149 iter 43 loss=0.0297651719301939\n",
      "epoch 149 iter 44 loss=0.040442340075969696\n",
      "epoch 149 iter 45 loss=0.034835923463106155\n",
      "epoch 149 iter 46 loss=0.043539293110370636\n",
      "epoch 149 iter 47 loss=0.033615611493587494\n",
      "epoch 149 iter 48 loss=0.026510726660490036\n",
      "epoch 149 iter 49 loss=0.03223313018679619\n",
      "epoch 149 iter 50 loss=0.037554480135440826\n",
      "epoch 149 iter 51 loss=0.0472986102104187\n",
      "epoch 149 iter 52 loss=0.054109226912260056\n",
      "epoch 149 iter 53 loss=0.024836694821715355\n",
      "epoch 149 iter 54 loss=0.03566747158765793\n",
      "epoch 149 iter 55 loss=0.04179103299975395\n",
      "epoch 149 iter 56 loss=0.04058530554175377\n",
      "epoch 149 iter 57 loss=0.035518668591976166\n",
      "epoch 149 iter 58 loss=0.02263963408768177\n",
      "epoch 149 iter 59 loss=0.02188642881810665\n",
      "epoch 149 iter 60 loss=0.03547929227352142\n",
      "epoch 149 iter 61 loss=0.046539828181266785\n",
      "epoch 149 iter 62 loss=0.04404193535447121\n",
      "epoch 149 iter 63 loss=0.029443925246596336\n",
      "epoch 149 iter 64 loss=0.03906011953949928\n",
      "epoch 149 iter 65 loss=0.040526919066905975\n",
      "epoch 149 iter 66 loss=0.051439639180898666\n",
      "epoch 149 iter 67 loss=0.040471915155649185\n",
      "epoch 149 iter 68 loss=0.04340706765651703\n",
      "epoch 149 iter 69 loss=0.04801979288458824\n",
      "epoch 149 iter 70 loss=0.03433450311422348\n",
      "epoch 149 iter 71 loss=0.03490680456161499\n",
      "epoch 149 iter 72 loss=0.045834675431251526\n",
      "epoch 149 iter 73 loss=0.023086706176400185\n",
      "epoch 149 iter 74 loss=0.052574507892131805\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3180.\n",
      "epoch 150 iter 0 loss=0.021511919796466827\n",
      "epoch 150 iter 1 loss=0.03555523231625557\n",
      "epoch 150 iter 2 loss=0.04114173352718353\n",
      "epoch 150 iter 3 loss=0.04397706314921379\n",
      "epoch 150 iter 4 loss=0.04700113832950592\n",
      "epoch 150 iter 5 loss=0.039829008281230927\n",
      "epoch 150 iter 6 loss=0.03001835010945797\n",
      "epoch 150 iter 7 loss=0.042329784482717514\n",
      "epoch 150 iter 8 loss=0.068385548889637\n",
      "epoch 150 iter 9 loss=0.04774061590433121\n",
      "epoch 150 iter 10 loss=0.0406668558716774\n",
      "epoch 150 iter 11 loss=0.029412245377898216\n",
      "epoch 150 iter 12 loss=0.04770035669207573\n",
      "epoch 150 iter 13 loss=0.03935351222753525\n",
      "epoch 150 iter 14 loss=0.03790490701794624\n",
      "epoch 150 iter 15 loss=0.04316585883498192\n",
      "epoch 150 iter 16 loss=0.048804767429828644\n",
      "epoch 150 iter 17 loss=0.04080561548471451\n",
      "epoch 150 iter 18 loss=0.04881229251623154\n",
      "epoch 150 iter 19 loss=0.030650369822978973\n",
      "epoch 150 iter 20 loss=0.05297226458787918\n",
      "epoch 150 iter 21 loss=0.021128149703145027\n",
      "epoch 150 iter 22 loss=0.0296652652323246\n",
      "epoch 150 iter 23 loss=0.03963436558842659\n",
      "epoch 150 iter 24 loss=0.023947590962052345\n",
      "epoch 150 iter 25 loss=0.04438670352101326\n",
      "epoch 150 iter 26 loss=0.03736706078052521\n",
      "epoch 150 iter 27 loss=0.029314227402210236\n",
      "epoch 150 iter 28 loss=0.021584780886769295\n",
      "epoch 150 iter 29 loss=0.04580923169851303\n",
      "epoch 150 iter 30 loss=0.03100620023906231\n",
      "epoch 150 iter 31 loss=0.021637631580233574\n",
      "epoch 150 iter 32 loss=0.032811786979436874\n",
      "epoch 150 iter 33 loss=0.03710141032934189\n",
      "epoch 150 iter 34 loss=0.033901944756507874\n",
      "epoch 150 iter 35 loss=0.051084719598293304\n",
      "epoch 150 iter 36 loss=0.047578420490026474\n",
      "epoch 150 iter 37 loss=0.018342887982726097\n",
      "epoch 150 iter 38 loss=0.02187262289226055\n",
      "epoch 150 iter 39 loss=0.031276073306798935\n",
      "epoch 150 iter 40 loss=0.02922165021300316\n",
      "epoch 150 iter 41 loss=0.03212801367044449\n",
      "epoch 150 iter 42 loss=0.04909949377179146\n",
      "epoch 150 iter 43 loss=0.034522879868745804\n",
      "epoch 150 iter 44 loss=0.03616071492433548\n",
      "epoch 150 iter 45 loss=0.03932040184736252\n",
      "epoch 150 iter 46 loss=0.04997387155890465\n",
      "epoch 150 iter 47 loss=0.04340054467320442\n",
      "epoch 150 iter 48 loss=0.026282604783773422\n",
      "epoch 150 iter 49 loss=0.05348316207528114\n",
      "epoch 150 iter 50 loss=0.026463765650987625\n",
      "epoch 150 iter 51 loss=0.030585220083594322\n",
      "epoch 150 iter 52 loss=0.03862962871789932\n",
      "epoch 150 iter 53 loss=0.016983721405267715\n",
      "epoch 150 iter 54 loss=0.029707113280892372\n",
      "epoch 150 iter 55 loss=0.03512458875775337\n",
      "epoch 150 iter 56 loss=0.03088427148759365\n",
      "epoch 150 iter 57 loss=0.02801697887480259\n",
      "epoch 150 iter 58 loss=0.03616464510560036\n",
      "epoch 150 iter 59 loss=0.04378124326467514\n",
      "epoch 150 iter 60 loss=0.03868826478719711\n",
      "epoch 150 iter 61 loss=0.03952909633517265\n",
      "epoch 150 iter 62 loss=0.04932861775159836\n",
      "epoch 150 iter 63 loss=0.0466977134346962\n",
      "epoch 150 iter 64 loss=0.042676400393247604\n",
      "epoch 150 iter 65 loss=0.030988803133368492\n",
      "epoch 150 iter 66 loss=0.03150201961398125\n",
      "epoch 150 iter 67 loss=0.04032214730978012\n",
      "epoch 150 iter 68 loss=0.031833477318286896\n",
      "epoch 150 iter 69 loss=0.042481210082769394\n",
      "epoch 150 iter 70 loss=0.04298814386129379\n",
      "epoch 150 iter 71 loss=0.033786557614803314\n",
      "epoch 150 iter 72 loss=0.03740654140710831\n",
      "epoch 150 iter 73 loss=0.040440235286951065\n",
      "epoch 150 iter 74 loss=0.03999046981334686\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3214.\n",
      "epoch 151 iter 0 loss=0.060301922261714935\n",
      "epoch 151 iter 1 loss=0.04142357409000397\n",
      "epoch 151 iter 2 loss=0.0314946249127388\n",
      "epoch 151 iter 3 loss=0.041504934430122375\n",
      "epoch 151 iter 4 loss=0.018871953710913658\n",
      "epoch 151 iter 5 loss=0.03905509039759636\n",
      "epoch 151 iter 6 loss=0.03542995825409889\n",
      "epoch 151 iter 7 loss=0.03199241682887077\n",
      "epoch 151 iter 8 loss=0.031068220734596252\n",
      "epoch 151 iter 9 loss=0.026088448241353035\n",
      "epoch 151 iter 10 loss=0.033189404755830765\n",
      "epoch 151 iter 11 loss=0.018314635381102562\n",
      "epoch 151 iter 12 loss=0.037972841411828995\n",
      "epoch 151 iter 13 loss=0.04174187779426575\n",
      "epoch 151 iter 14 loss=0.04163770005106926\n",
      "epoch 151 iter 15 loss=0.026131294667720795\n",
      "epoch 151 iter 16 loss=0.027408666908740997\n",
      "epoch 151 iter 17 loss=0.03362146019935608\n",
      "epoch 151 iter 18 loss=0.037433404475450516\n",
      "epoch 151 iter 19 loss=0.043775539845228195\n",
      "epoch 151 iter 20 loss=0.029459869489073753\n",
      "epoch 151 iter 21 loss=0.038422372192144394\n",
      "epoch 151 iter 22 loss=0.028621744364500046\n",
      "epoch 151 iter 23 loss=0.01794658973813057\n",
      "epoch 151 iter 24 loss=0.04123686999082565\n",
      "epoch 151 iter 25 loss=0.03250613808631897\n",
      "epoch 151 iter 26 loss=0.02965286560356617\n",
      "epoch 151 iter 27 loss=0.028818335384130478\n",
      "epoch 151 iter 28 loss=0.03566265478730202\n",
      "epoch 151 iter 29 loss=0.045234594494104385\n",
      "epoch 151 iter 30 loss=0.06521948426961899\n",
      "epoch 151 iter 31 loss=0.04022753983736038\n",
      "epoch 151 iter 32 loss=0.02728043869137764\n",
      "epoch 151 iter 33 loss=0.04417969658970833\n",
      "epoch 151 iter 34 loss=0.03703916817903519\n",
      "epoch 151 iter 35 loss=0.05753302201628685\n",
      "epoch 151 iter 36 loss=0.03616517037153244\n",
      "epoch 151 iter 37 loss=0.025713933631777763\n",
      "epoch 151 iter 38 loss=0.02913374826312065\n",
      "epoch 151 iter 39 loss=0.05003130063414574\n",
      "epoch 151 iter 40 loss=0.04687700420618057\n",
      "epoch 151 iter 41 loss=0.030385099351406097\n",
      "epoch 151 iter 42 loss=0.027126658707857132\n",
      "epoch 151 iter 43 loss=0.04054074361920357\n",
      "epoch 151 iter 44 loss=0.06331615149974823\n",
      "epoch 151 iter 45 loss=0.031900081783533096\n",
      "epoch 151 iter 46 loss=0.04057717323303223\n",
      "epoch 151 iter 47 loss=0.03930779919028282\n",
      "epoch 151 iter 48 loss=0.042868807911872864\n",
      "epoch 151 iter 49 loss=0.025023121386766434\n",
      "epoch 151 iter 50 loss=0.02592507377266884\n",
      "epoch 151 iter 51 loss=0.048382718116045\n",
      "epoch 151 iter 52 loss=0.04732414707541466\n",
      "epoch 151 iter 53 loss=0.03165404498577118\n",
      "epoch 151 iter 54 loss=0.027122987434267998\n",
      "epoch 151 iter 55 loss=0.04838188737630844\n",
      "epoch 151 iter 56 loss=0.05848035588860512\n",
      "epoch 151 iter 57 loss=0.0322306752204895\n",
      "epoch 151 iter 58 loss=0.040663063526153564\n",
      "epoch 151 iter 59 loss=0.04456526041030884\n",
      "epoch 151 iter 60 loss=0.027539700269699097\n",
      "epoch 151 iter 61 loss=0.037406302988529205\n",
      "epoch 151 iter 62 loss=0.04003852233290672\n",
      "epoch 151 iter 63 loss=0.015954643487930298\n",
      "epoch 151 iter 64 loss=0.04634722322225571\n",
      "epoch 151 iter 65 loss=0.04256688803434372\n",
      "epoch 151 iter 66 loss=0.051747627556324005\n",
      "epoch 151 iter 67 loss=0.027464348822832108\n",
      "epoch 151 iter 68 loss=0.019471049308776855\n",
      "epoch 151 iter 69 loss=0.03718485310673714\n",
      "epoch 151 iter 70 loss=0.02819216065108776\n",
      "epoch 151 iter 71 loss=0.04541406035423279\n",
      "epoch 151 iter 72 loss=0.04225550591945648\n",
      "epoch 151 iter 73 loss=0.059570275247097015\n",
      "epoch 151 iter 74 loss=0.03012958914041519\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3161.\n",
      "epoch 152 iter 0 loss=0.028368571773171425\n",
      "epoch 152 iter 1 loss=0.05332868918776512\n",
      "epoch 152 iter 2 loss=0.035158369690179825\n",
      "epoch 152 iter 3 loss=0.03308452293276787\n",
      "epoch 152 iter 4 loss=0.029055237770080566\n",
      "epoch 152 iter 5 loss=0.022685911506414413\n",
      "epoch 152 iter 6 loss=0.03993578255176544\n",
      "epoch 152 iter 7 loss=0.023480892181396484\n",
      "epoch 152 iter 8 loss=0.06169077008962631\n",
      "epoch 152 iter 9 loss=0.0270671546459198\n",
      "epoch 152 iter 10 loss=0.03734111413359642\n",
      "epoch 152 iter 11 loss=0.0383276529610157\n",
      "epoch 152 iter 12 loss=0.03694246709346771\n",
      "epoch 152 iter 13 loss=0.05591193959116936\n",
      "epoch 152 iter 14 loss=0.030256055295467377\n",
      "epoch 152 iter 15 loss=0.04611968621611595\n",
      "epoch 152 iter 16 loss=0.038710836321115494\n",
      "epoch 152 iter 17 loss=0.03508473560214043\n",
      "epoch 152 iter 18 loss=0.04558221623301506\n",
      "epoch 152 iter 19 loss=0.01875375211238861\n",
      "epoch 152 iter 20 loss=0.02478710189461708\n",
      "epoch 152 iter 21 loss=0.0457611083984375\n",
      "epoch 152 iter 22 loss=0.03673696517944336\n",
      "epoch 152 iter 23 loss=0.043429043143987656\n",
      "epoch 152 iter 24 loss=0.03166249021887779\n",
      "epoch 152 iter 25 loss=0.027696875855326653\n",
      "epoch 152 iter 26 loss=0.052330516278743744\n",
      "epoch 152 iter 27 loss=0.020455580204725266\n",
      "epoch 152 iter 28 loss=0.02454870566725731\n",
      "epoch 152 iter 29 loss=0.03830587863922119\n",
      "epoch 152 iter 30 loss=0.04625524580478668\n",
      "epoch 152 iter 31 loss=0.05843094736337662\n",
      "epoch 152 iter 32 loss=0.03308625519275665\n",
      "epoch 152 iter 33 loss=0.031321074813604355\n",
      "epoch 152 iter 34 loss=0.021349044516682625\n",
      "epoch 152 iter 35 loss=0.04628177732229233\n",
      "epoch 152 iter 36 loss=0.030931266024708748\n",
      "epoch 152 iter 37 loss=0.04056321457028389\n",
      "epoch 152 iter 38 loss=0.0371570810675621\n",
      "epoch 152 iter 39 loss=0.027125930413603783\n",
      "epoch 152 iter 40 loss=0.03639101982116699\n",
      "epoch 152 iter 41 loss=0.04349707439541817\n",
      "epoch 152 iter 42 loss=0.024017175659537315\n",
      "epoch 152 iter 43 loss=0.027020687237381935\n",
      "epoch 152 iter 44 loss=0.04407096281647682\n",
      "epoch 152 iter 45 loss=0.029555661603808403\n",
      "epoch 152 iter 46 loss=0.04937123879790306\n",
      "epoch 152 iter 47 loss=0.05338394269347191\n",
      "epoch 152 iter 48 loss=0.028184665367007256\n",
      "epoch 152 iter 49 loss=0.024280399084091187\n",
      "epoch 152 iter 50 loss=0.04313676059246063\n",
      "epoch 152 iter 51 loss=0.026732761412858963\n",
      "epoch 152 iter 52 loss=0.04302401840686798\n",
      "epoch 152 iter 53 loss=0.050802215933799744\n",
      "epoch 152 iter 54 loss=0.04616290330886841\n",
      "epoch 152 iter 55 loss=0.0321786142885685\n",
      "epoch 152 iter 56 loss=0.03781735152006149\n",
      "epoch 152 iter 57 loss=0.03911212459206581\n",
      "epoch 152 iter 58 loss=0.04295976459980011\n",
      "epoch 152 iter 59 loss=0.04111660271883011\n",
      "epoch 152 iter 60 loss=0.026742735877633095\n",
      "epoch 152 iter 61 loss=0.04744957014918327\n",
      "epoch 152 iter 62 loss=0.0381881445646286\n",
      "epoch 152 iter 63 loss=0.04179602116346359\n",
      "epoch 152 iter 64 loss=0.05852024629712105\n",
      "epoch 152 iter 65 loss=0.018736887723207474\n",
      "epoch 152 iter 66 loss=0.02913607656955719\n",
      "epoch 152 iter 67 loss=0.029707808047533035\n",
      "epoch 152 iter 68 loss=0.03069608472287655\n",
      "epoch 152 iter 69 loss=0.04420434683561325\n",
      "epoch 152 iter 70 loss=0.02653743512928486\n",
      "epoch 152 iter 71 loss=0.05133623257279396\n",
      "epoch 152 iter 72 loss=0.04124746099114418\n",
      "epoch 152 iter 73 loss=0.034782521426677704\n",
      "epoch 152 iter 74 loss=0.0427728109061718\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3203.\n",
      "epoch 153 iter 0 loss=0.017936058342456818\n",
      "epoch 153 iter 1 loss=0.03192828223109245\n",
      "epoch 153 iter 2 loss=0.029566485434770584\n",
      "epoch 153 iter 3 loss=0.0473436638712883\n",
      "epoch 153 iter 4 loss=0.04296433925628662\n",
      "epoch 153 iter 5 loss=0.029266489669680595\n",
      "epoch 153 iter 6 loss=0.05600830912590027\n",
      "epoch 153 iter 7 loss=0.03354279324412346\n",
      "epoch 153 iter 8 loss=0.026554102078080177\n",
      "epoch 153 iter 9 loss=0.05305442214012146\n",
      "epoch 153 iter 10 loss=0.018102115020155907\n",
      "epoch 153 iter 11 loss=0.042994432151317596\n",
      "epoch 153 iter 12 loss=0.03011384978890419\n",
      "epoch 153 iter 13 loss=0.05892150476574898\n",
      "epoch 153 iter 14 loss=0.03601709380745888\n",
      "epoch 153 iter 15 loss=0.04486452043056488\n",
      "epoch 153 iter 16 loss=0.03453722596168518\n",
      "epoch 153 iter 17 loss=0.03125276044011116\n",
      "epoch 153 iter 18 loss=0.04185986518859863\n",
      "epoch 153 iter 19 loss=0.030865857377648354\n",
      "epoch 153 iter 20 loss=0.01766381599009037\n",
      "epoch 153 iter 21 loss=0.03885677456855774\n",
      "epoch 153 iter 22 loss=0.04069770500063896\n",
      "epoch 153 iter 23 loss=0.03685006499290466\n",
      "epoch 153 iter 24 loss=0.03787955269217491\n",
      "epoch 153 iter 25 loss=0.04091177508234978\n",
      "epoch 153 iter 26 loss=0.04866456612944603\n",
      "epoch 153 iter 27 loss=0.04576830565929413\n",
      "epoch 153 iter 28 loss=0.022527413442730904\n",
      "epoch 153 iter 29 loss=0.046040598303079605\n",
      "epoch 153 iter 30 loss=0.04992791265249252\n",
      "epoch 153 iter 31 loss=0.0366365946829319\n",
      "epoch 153 iter 32 loss=0.022583087906241417\n",
      "epoch 153 iter 33 loss=0.042493078857660294\n",
      "epoch 153 iter 34 loss=0.033578358590602875\n",
      "epoch 153 iter 35 loss=0.030072759836912155\n",
      "epoch 153 iter 36 loss=0.031704504042863846\n",
      "epoch 153 iter 37 loss=0.039648622274398804\n",
      "epoch 153 iter 38 loss=0.038845572620630264\n",
      "epoch 153 iter 39 loss=0.04233745113015175\n",
      "epoch 153 iter 40 loss=0.028998536989092827\n",
      "epoch 153 iter 41 loss=0.04974731430411339\n",
      "epoch 153 iter 42 loss=0.020400485023856163\n",
      "epoch 153 iter 43 loss=0.0346185527741909\n",
      "epoch 153 iter 44 loss=0.03949284553527832\n",
      "epoch 153 iter 45 loss=0.01970123127102852\n",
      "epoch 153 iter 46 loss=0.03723620995879173\n",
      "epoch 153 iter 47 loss=0.04378670081496239\n",
      "epoch 153 iter 48 loss=0.04697756469249725\n",
      "epoch 153 iter 49 loss=0.031414344906806946\n",
      "epoch 153 iter 50 loss=0.044453609734773636\n",
      "epoch 153 iter 51 loss=0.04064963757991791\n",
      "epoch 153 iter 52 loss=0.03415068984031677\n",
      "epoch 153 iter 53 loss=0.03729696571826935\n",
      "epoch 153 iter 54 loss=0.03081437386572361\n",
      "epoch 153 iter 55 loss=0.029474083334207535\n",
      "epoch 153 iter 56 loss=0.03192896768450737\n",
      "epoch 153 iter 57 loss=0.05673454701900482\n",
      "epoch 153 iter 58 loss=0.025694923475384712\n",
      "epoch 153 iter 59 loss=0.03386332467198372\n",
      "epoch 153 iter 60 loss=0.03547828271985054\n",
      "epoch 153 iter 61 loss=0.052131667733192444\n",
      "epoch 153 iter 62 loss=0.058983445167541504\n",
      "epoch 153 iter 63 loss=0.042232487350702286\n",
      "epoch 153 iter 64 loss=0.03089733049273491\n",
      "epoch 153 iter 65 loss=0.027667947113513947\n",
      "epoch 153 iter 66 loss=0.05744317173957825\n",
      "epoch 153 iter 67 loss=0.06512555480003357\n",
      "epoch 153 iter 68 loss=0.03554458171129227\n",
      "epoch 153 iter 69 loss=0.029171979054808617\n",
      "epoch 153 iter 70 loss=0.029378080740571022\n",
      "epoch 153 iter 71 loss=0.026817459613084793\n",
      "epoch 153 iter 72 loss=0.02643105573952198\n",
      "epoch 153 iter 73 loss=0.03151873126626015\n",
      "epoch 153 iter 74 loss=0.03310364484786987\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3209.\n",
      "epoch 154 iter 0 loss=0.03196671977639198\n",
      "epoch 154 iter 1 loss=0.04287213459610939\n",
      "epoch 154 iter 2 loss=0.02674836665391922\n",
      "epoch 154 iter 3 loss=0.03225024417042732\n",
      "epoch 154 iter 4 loss=0.023794926702976227\n",
      "epoch 154 iter 5 loss=0.044090740382671356\n",
      "epoch 154 iter 6 loss=0.029448075219988823\n",
      "epoch 154 iter 7 loss=0.03876481205224991\n",
      "epoch 154 iter 8 loss=0.05057854577898979\n",
      "epoch 154 iter 9 loss=0.027350910007953644\n",
      "epoch 154 iter 10 loss=0.02881910651922226\n",
      "epoch 154 iter 11 loss=0.037847843021154404\n",
      "epoch 154 iter 12 loss=0.022472048178315163\n",
      "epoch 154 iter 13 loss=0.028364580124616623\n",
      "epoch 154 iter 14 loss=0.016283370554447174\n",
      "epoch 154 iter 15 loss=0.05122661218047142\n",
      "epoch 154 iter 16 loss=0.02983533963561058\n",
      "epoch 154 iter 17 loss=0.020785503089427948\n",
      "epoch 154 iter 18 loss=0.03611091151833534\n",
      "epoch 154 iter 19 loss=0.027273477986454964\n",
      "epoch 154 iter 20 loss=0.04728510230779648\n",
      "epoch 154 iter 21 loss=0.025805648416280746\n",
      "epoch 154 iter 22 loss=0.04610354080796242\n",
      "epoch 154 iter 23 loss=0.04381882771849632\n",
      "epoch 154 iter 24 loss=0.029519060626626015\n",
      "epoch 154 iter 25 loss=0.04035288840532303\n",
      "epoch 154 iter 26 loss=0.024945203214883804\n",
      "epoch 154 iter 27 loss=0.0436108261346817\n",
      "epoch 154 iter 28 loss=0.03558146581053734\n",
      "epoch 154 iter 29 loss=0.01910221390426159\n",
      "epoch 154 iter 30 loss=0.04474542662501335\n",
      "epoch 154 iter 31 loss=0.03780446574091911\n",
      "epoch 154 iter 32 loss=0.04680787771940231\n",
      "epoch 154 iter 33 loss=0.034502867609262466\n",
      "epoch 154 iter 34 loss=0.03353538736701012\n",
      "epoch 154 iter 35 loss=0.03099161386489868\n",
      "epoch 154 iter 36 loss=0.03246363252401352\n",
      "epoch 154 iter 37 loss=0.02146671712398529\n",
      "epoch 154 iter 38 loss=0.05751309171319008\n",
      "epoch 154 iter 39 loss=0.030118800699710846\n",
      "epoch 154 iter 40 loss=0.03607277199625969\n",
      "epoch 154 iter 41 loss=0.037291429936885834\n",
      "epoch 154 iter 42 loss=0.025819849222898483\n",
      "epoch 154 iter 43 loss=0.05385645106434822\n",
      "epoch 154 iter 44 loss=0.03918302059173584\n",
      "epoch 154 iter 45 loss=0.03615625575184822\n",
      "epoch 154 iter 46 loss=0.026474593207240105\n",
      "epoch 154 iter 47 loss=0.03093506209552288\n",
      "epoch 154 iter 48 loss=0.03551207855343819\n",
      "epoch 154 iter 49 loss=0.04944707825779915\n",
      "epoch 154 iter 50 loss=0.06276723742485046\n",
      "epoch 154 iter 51 loss=0.03072941303253174\n",
      "epoch 154 iter 52 loss=0.03966554254293442\n",
      "epoch 154 iter 53 loss=0.05135919526219368\n",
      "epoch 154 iter 54 loss=0.03418996185064316\n",
      "epoch 154 iter 55 loss=0.041380830109119415\n",
      "epoch 154 iter 56 loss=0.03816882148385048\n",
      "epoch 154 iter 57 loss=0.051032889634370804\n",
      "epoch 154 iter 58 loss=0.032051119953393936\n",
      "epoch 154 iter 59 loss=0.04672842472791672\n",
      "epoch 154 iter 60 loss=0.06461972743272781\n",
      "epoch 154 iter 61 loss=0.02255992777645588\n",
      "epoch 154 iter 62 loss=0.04406009986996651\n",
      "epoch 154 iter 63 loss=0.03220851346850395\n",
      "epoch 154 iter 64 loss=0.02062513865530491\n",
      "epoch 154 iter 65 loss=0.042719319462776184\n",
      "epoch 154 iter 66 loss=0.03320286050438881\n",
      "epoch 154 iter 67 loss=0.06487356126308441\n",
      "epoch 154 iter 68 loss=0.03647352755069733\n",
      "epoch 154 iter 69 loss=0.027632921934127808\n",
      "epoch 154 iter 70 loss=0.021819664165377617\n",
      "epoch 154 iter 71 loss=0.05373334884643555\n",
      "epoch 154 iter 72 loss=0.03176034241914749\n",
      "epoch 154 iter 73 loss=0.047331128269433975\n",
      "epoch 154 iter 74 loss=0.02545660361647606\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3157.\n",
      "epoch 155 iter 0 loss=0.023512862622737885\n",
      "epoch 155 iter 1 loss=0.04084767401218414\n",
      "epoch 155 iter 2 loss=0.048391882330179214\n",
      "epoch 155 iter 3 loss=0.0269863810390234\n",
      "epoch 155 iter 4 loss=0.037871282547712326\n",
      "epoch 155 iter 5 loss=0.0374281220138073\n",
      "epoch 155 iter 6 loss=0.04141679033637047\n",
      "epoch 155 iter 7 loss=0.0192982517182827\n",
      "epoch 155 iter 8 loss=0.02852083556354046\n",
      "epoch 155 iter 9 loss=0.03504046052694321\n",
      "epoch 155 iter 10 loss=0.02426893450319767\n",
      "epoch 155 iter 11 loss=0.048850879073143005\n",
      "epoch 155 iter 12 loss=0.02776733972132206\n",
      "epoch 155 iter 13 loss=0.0455741286277771\n",
      "epoch 155 iter 14 loss=0.02398558519780636\n",
      "epoch 155 iter 15 loss=0.03339400514960289\n",
      "epoch 155 iter 16 loss=0.018673304468393326\n",
      "epoch 155 iter 17 loss=0.04212477058172226\n",
      "epoch 155 iter 18 loss=0.03918282687664032\n",
      "epoch 155 iter 19 loss=0.03818243369460106\n",
      "epoch 155 iter 20 loss=0.03787031024694443\n",
      "epoch 155 iter 21 loss=0.03970884904265404\n",
      "epoch 155 iter 22 loss=0.04855165630578995\n",
      "epoch 155 iter 23 loss=0.031809430569410324\n",
      "epoch 155 iter 24 loss=0.02564646676182747\n",
      "epoch 155 iter 25 loss=0.04303388670086861\n",
      "epoch 155 iter 26 loss=0.02972778119146824\n",
      "epoch 155 iter 27 loss=0.026629865169525146\n",
      "epoch 155 iter 28 loss=0.027483316138386726\n",
      "epoch 155 iter 29 loss=0.03095865249633789\n",
      "epoch 155 iter 30 loss=0.04415864497423172\n",
      "epoch 155 iter 31 loss=0.027408132329583168\n",
      "epoch 155 iter 32 loss=0.04851072281599045\n",
      "epoch 155 iter 33 loss=0.03720245882868767\n",
      "epoch 155 iter 34 loss=0.018807606771588326\n",
      "epoch 155 iter 35 loss=0.04720553755760193\n",
      "epoch 155 iter 36 loss=0.04127342998981476\n",
      "epoch 155 iter 37 loss=0.04012424126267433\n",
      "epoch 155 iter 38 loss=0.044236257672309875\n",
      "epoch 155 iter 39 loss=0.036111850291490555\n",
      "epoch 155 iter 40 loss=0.05159014090895653\n",
      "epoch 155 iter 41 loss=0.0369357094168663\n",
      "epoch 155 iter 42 loss=0.03426937758922577\n",
      "epoch 155 iter 43 loss=0.04509466886520386\n",
      "epoch 155 iter 44 loss=0.01820034720003605\n",
      "epoch 155 iter 45 loss=0.0311888474971056\n",
      "epoch 155 iter 46 loss=0.03706609085202217\n",
      "epoch 155 iter 47 loss=0.04154155030846596\n",
      "epoch 155 iter 48 loss=0.03959110379219055\n",
      "epoch 155 iter 49 loss=0.027146359905600548\n",
      "epoch 155 iter 50 loss=0.02771189622581005\n",
      "epoch 155 iter 51 loss=0.02330469898879528\n",
      "epoch 155 iter 52 loss=0.032165706157684326\n",
      "epoch 155 iter 53 loss=0.03284628316760063\n",
      "epoch 155 iter 54 loss=0.0614614337682724\n",
      "epoch 155 iter 55 loss=0.03486178070306778\n",
      "epoch 155 iter 56 loss=0.02910333126783371\n",
      "epoch 155 iter 57 loss=0.06621014326810837\n",
      "epoch 155 iter 58 loss=0.03628600761294365\n",
      "epoch 155 iter 59 loss=0.03492378816008568\n",
      "epoch 155 iter 60 loss=0.04258738085627556\n",
      "epoch 155 iter 61 loss=0.044350311160087585\n",
      "epoch 155 iter 62 loss=0.019447050988674164\n",
      "epoch 155 iter 63 loss=0.03806094080209732\n",
      "epoch 155 iter 64 loss=0.054942578077316284\n",
      "epoch 155 iter 65 loss=0.04137963429093361\n",
      "epoch 155 iter 66 loss=0.03251250833272934\n",
      "epoch 155 iter 67 loss=0.05756562948226929\n",
      "epoch 155 iter 68 loss=0.04292776808142662\n",
      "epoch 155 iter 69 loss=0.047645628452301025\n",
      "epoch 155 iter 70 loss=0.028594383969902992\n",
      "epoch 155 iter 71 loss=0.03470795601606369\n",
      "epoch 155 iter 72 loss=0.020811406895518303\n",
      "epoch 155 iter 73 loss=0.05193518474698067\n",
      "epoch 155 iter 74 loss=0.030679861083626747\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3203.\n",
      "epoch 156 iter 0 loss=0.04203955829143524\n",
      "epoch 156 iter 1 loss=0.0434778556227684\n",
      "epoch 156 iter 2 loss=0.01854880340397358\n",
      "epoch 156 iter 3 loss=0.03364528715610504\n",
      "epoch 156 iter 4 loss=0.039996881037950516\n",
      "epoch 156 iter 5 loss=0.03352799639105797\n",
      "epoch 156 iter 6 loss=0.029386363923549652\n",
      "epoch 156 iter 7 loss=0.03317384049296379\n",
      "epoch 156 iter 8 loss=0.0363764762878418\n",
      "epoch 156 iter 9 loss=0.04225365072488785\n",
      "epoch 156 iter 10 loss=0.03481817618012428\n",
      "epoch 156 iter 11 loss=0.02075083740055561\n",
      "epoch 156 iter 12 loss=0.04124462604522705\n",
      "epoch 156 iter 13 loss=0.03696485608816147\n",
      "epoch 156 iter 14 loss=0.04605930671095848\n",
      "epoch 156 iter 15 loss=0.045156292617321014\n",
      "epoch 156 iter 16 loss=0.03318750113248825\n",
      "epoch 156 iter 17 loss=0.03888283297419548\n",
      "epoch 156 iter 18 loss=0.0328347384929657\n",
      "epoch 156 iter 19 loss=0.032875899225473404\n",
      "epoch 156 iter 20 loss=0.04928416758775711\n",
      "epoch 156 iter 21 loss=0.05128976330161095\n",
      "epoch 156 iter 22 loss=0.03822479024529457\n",
      "epoch 156 iter 23 loss=0.03794682025909424\n",
      "epoch 156 iter 24 loss=0.020600689575076103\n",
      "epoch 156 iter 25 loss=0.03133392333984375\n",
      "epoch 156 iter 26 loss=0.024042759090662003\n",
      "epoch 156 iter 27 loss=0.023993900045752525\n",
      "epoch 156 iter 28 loss=0.01951037533581257\n",
      "epoch 156 iter 29 loss=0.05291008576750755\n",
      "epoch 156 iter 30 loss=0.04050475358963013\n",
      "epoch 156 iter 31 loss=0.03833382576704025\n",
      "epoch 156 iter 32 loss=0.05354665592312813\n",
      "epoch 156 iter 33 loss=0.033925049006938934\n",
      "epoch 156 iter 34 loss=0.042000025510787964\n",
      "epoch 156 iter 35 loss=0.03592834249138832\n",
      "epoch 156 iter 36 loss=0.029417749494314194\n",
      "epoch 156 iter 37 loss=0.03785363957285881\n",
      "epoch 156 iter 38 loss=0.04008860141038895\n",
      "epoch 156 iter 39 loss=0.03185305744409561\n",
      "epoch 156 iter 40 loss=0.02134341560304165\n",
      "epoch 156 iter 41 loss=0.043024465441703796\n",
      "epoch 156 iter 42 loss=0.04988398030400276\n",
      "epoch 156 iter 43 loss=0.04335099458694458\n",
      "epoch 156 iter 44 loss=0.04596451669931412\n",
      "epoch 156 iter 45 loss=0.01953963004052639\n",
      "epoch 156 iter 46 loss=0.06492124497890472\n",
      "epoch 156 iter 47 loss=0.026732079684734344\n",
      "epoch 156 iter 48 loss=0.02642112225294113\n",
      "epoch 156 iter 49 loss=0.025770876556634903\n",
      "epoch 156 iter 50 loss=0.03690842166543007\n",
      "epoch 156 iter 51 loss=0.04056810215115547\n",
      "epoch 156 iter 52 loss=0.012842406518757343\n",
      "epoch 156 iter 53 loss=0.04007519781589508\n",
      "epoch 156 iter 54 loss=0.042885661125183105\n",
      "epoch 156 iter 55 loss=0.046575598418712616\n",
      "epoch 156 iter 56 loss=0.041584812104701996\n",
      "epoch 156 iter 57 loss=0.019958486780524254\n",
      "epoch 156 iter 58 loss=0.05292977765202522\n",
      "epoch 156 iter 59 loss=0.04042378440499306\n",
      "epoch 156 iter 60 loss=0.036618489772081375\n",
      "epoch 156 iter 61 loss=0.020108085125684738\n",
      "epoch 156 iter 62 loss=0.029199203476309776\n",
      "epoch 156 iter 63 loss=0.046069372445344925\n",
      "epoch 156 iter 64 loss=0.03196810930967331\n",
      "epoch 156 iter 65 loss=0.019238490611314774\n",
      "epoch 156 iter 66 loss=0.04660465195775032\n",
      "epoch 156 iter 67 loss=0.04110448434948921\n",
      "epoch 156 iter 68 loss=0.019369088113307953\n",
      "epoch 156 iter 69 loss=0.06928930431604385\n",
      "epoch 156 iter 70 loss=0.04673624411225319\n",
      "epoch 156 iter 71 loss=0.04104311019182205\n",
      "epoch 156 iter 72 loss=0.02715807594358921\n",
      "epoch 156 iter 73 loss=0.025886142626404762\n",
      "epoch 156 iter 74 loss=0.043873533606529236\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3179.\n",
      "epoch 157 iter 0 loss=0.03781379014253616\n",
      "epoch 157 iter 1 loss=0.031285375356674194\n",
      "epoch 157 iter 2 loss=0.03848790004849434\n",
      "epoch 157 iter 3 loss=0.021690191701054573\n",
      "epoch 157 iter 4 loss=0.035863518714904785\n",
      "epoch 157 iter 5 loss=0.040582720190286636\n",
      "epoch 157 iter 6 loss=0.03391417860984802\n",
      "epoch 157 iter 7 loss=0.03230002522468567\n",
      "epoch 157 iter 8 loss=0.0332903228700161\n",
      "epoch 157 iter 9 loss=0.04908204451203346\n",
      "epoch 157 iter 10 loss=0.040518637746572495\n",
      "epoch 157 iter 11 loss=0.03089143894612789\n",
      "epoch 157 iter 12 loss=0.039420776069164276\n",
      "epoch 157 iter 13 loss=0.03728516399860382\n",
      "epoch 157 iter 14 loss=0.041789017617702484\n",
      "epoch 157 iter 15 loss=0.03830941021442413\n",
      "epoch 157 iter 16 loss=0.037042710930109024\n",
      "epoch 157 iter 17 loss=0.038725681602954865\n",
      "epoch 157 iter 18 loss=0.04791705310344696\n",
      "epoch 157 iter 19 loss=0.03547022491693497\n",
      "epoch 157 iter 20 loss=0.03250591829419136\n",
      "epoch 157 iter 21 loss=0.03754691407084465\n",
      "epoch 157 iter 22 loss=0.039033062756061554\n",
      "epoch 157 iter 23 loss=0.034251585602760315\n",
      "epoch 157 iter 24 loss=0.036814503371715546\n",
      "epoch 157 iter 25 loss=0.04801757633686066\n",
      "epoch 157 iter 26 loss=0.03490312024950981\n",
      "epoch 157 iter 27 loss=0.031110048294067383\n",
      "epoch 157 iter 28 loss=0.02757710963487625\n",
      "epoch 157 iter 29 loss=0.03497285768389702\n",
      "epoch 157 iter 30 loss=0.042153291404247284\n",
      "epoch 157 iter 31 loss=0.04300525039434433\n",
      "epoch 157 iter 32 loss=0.03959420695900917\n",
      "epoch 157 iter 33 loss=0.047601424157619476\n",
      "epoch 157 iter 34 loss=0.02386009693145752\n",
      "epoch 157 iter 35 loss=0.030805641785264015\n",
      "epoch 157 iter 36 loss=0.03643418103456497\n",
      "epoch 157 iter 37 loss=0.041924331337213516\n",
      "epoch 157 iter 38 loss=0.03280378878116608\n",
      "epoch 157 iter 39 loss=0.027142062783241272\n",
      "epoch 157 iter 40 loss=0.03721124306321144\n",
      "epoch 157 iter 41 loss=0.0487644337117672\n",
      "epoch 157 iter 42 loss=0.03906406834721565\n",
      "epoch 157 iter 43 loss=0.06362515687942505\n",
      "epoch 157 iter 44 loss=0.0315614752471447\n",
      "epoch 157 iter 45 loss=0.031990036368370056\n",
      "epoch 157 iter 46 loss=0.04996797814965248\n",
      "epoch 157 iter 47 loss=0.028040438890457153\n",
      "epoch 157 iter 48 loss=0.01957455277442932\n",
      "epoch 157 iter 49 loss=0.02899836376309395\n",
      "epoch 157 iter 50 loss=0.01621447503566742\n",
      "epoch 157 iter 51 loss=0.029559385031461716\n",
      "epoch 157 iter 52 loss=0.05140966922044754\n",
      "epoch 157 iter 53 loss=0.028442885726690292\n",
      "epoch 157 iter 54 loss=0.019350115209817886\n",
      "epoch 157 iter 55 loss=0.03238058462738991\n",
      "epoch 157 iter 56 loss=0.01796109788119793\n",
      "epoch 157 iter 57 loss=0.045886874198913574\n",
      "epoch 157 iter 58 loss=0.05769310146570206\n",
      "epoch 157 iter 59 loss=0.04028839245438576\n",
      "epoch 157 iter 60 loss=0.024547545239329338\n",
      "epoch 157 iter 61 loss=0.03932056576013565\n",
      "epoch 157 iter 62 loss=0.05507677048444748\n",
      "epoch 157 iter 63 loss=0.029885996133089066\n",
      "epoch 157 iter 64 loss=0.03181438520550728\n",
      "epoch 157 iter 65 loss=0.050981950014829636\n",
      "epoch 157 iter 66 loss=0.03163653984665871\n",
      "epoch 157 iter 67 loss=0.02674410492181778\n",
      "epoch 157 iter 68 loss=0.039179909974336624\n",
      "epoch 157 iter 69 loss=0.03489365801215172\n",
      "epoch 157 iter 70 loss=0.019228527322411537\n",
      "epoch 157 iter 71 loss=0.04443531855940819\n",
      "epoch 157 iter 72 loss=0.040916234254837036\n",
      "epoch 157 iter 73 loss=0.042408592998981476\n",
      "epoch 157 iter 74 loss=0.03143606334924698\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3204.\n",
      "epoch 158 iter 0 loss=0.03075077012181282\n",
      "epoch 158 iter 1 loss=0.0193648561835289\n",
      "epoch 158 iter 2 loss=0.02985328622162342\n",
      "epoch 158 iter 3 loss=0.04487629234790802\n",
      "epoch 158 iter 4 loss=0.03557494655251503\n",
      "epoch 158 iter 5 loss=0.08726035058498383\n",
      "epoch 158 iter 6 loss=0.02548801898956299\n",
      "epoch 158 iter 7 loss=0.05276182293891907\n",
      "epoch 158 iter 8 loss=0.030905457213521004\n",
      "epoch 158 iter 9 loss=0.02825702540576458\n",
      "epoch 158 iter 10 loss=0.02769756317138672\n",
      "epoch 158 iter 11 loss=0.04734056070446968\n",
      "epoch 158 iter 12 loss=0.04102489724755287\n",
      "epoch 158 iter 13 loss=0.04169664904475212\n",
      "epoch 158 iter 14 loss=0.04550918564200401\n",
      "epoch 158 iter 15 loss=0.028904898092150688\n",
      "epoch 158 iter 16 loss=0.05511484667658806\n",
      "epoch 158 iter 17 loss=0.019540252164006233\n",
      "epoch 158 iter 18 loss=0.05018571391701698\n",
      "epoch 158 iter 19 loss=0.025376956909894943\n",
      "epoch 158 iter 20 loss=0.031143106520175934\n",
      "epoch 158 iter 21 loss=0.04318840801715851\n",
      "epoch 158 iter 22 loss=0.04509330168366432\n",
      "epoch 158 iter 23 loss=0.026446957141160965\n",
      "epoch 158 iter 24 loss=0.038021232932806015\n",
      "epoch 158 iter 25 loss=0.042612940073013306\n",
      "epoch 158 iter 26 loss=0.05077236890792847\n",
      "epoch 158 iter 27 loss=0.05240839719772339\n",
      "epoch 158 iter 28 loss=0.021790675818920135\n",
      "epoch 158 iter 29 loss=0.03592133894562721\n",
      "epoch 158 iter 30 loss=0.03904873877763748\n",
      "epoch 158 iter 31 loss=0.03494630753993988\n",
      "epoch 158 iter 32 loss=0.02142154611647129\n",
      "epoch 158 iter 33 loss=0.03980456665158272\n",
      "epoch 158 iter 34 loss=0.03557603806257248\n",
      "epoch 158 iter 35 loss=0.04164476320147514\n",
      "epoch 158 iter 36 loss=0.053185828030109406\n",
      "epoch 158 iter 37 loss=0.02493135631084442\n",
      "epoch 158 iter 38 loss=0.0392492339015007\n",
      "epoch 158 iter 39 loss=0.03444777429103851\n",
      "epoch 158 iter 40 loss=0.04244205355644226\n",
      "epoch 158 iter 41 loss=0.03501492366194725\n",
      "epoch 158 iter 42 loss=0.044945646077394485\n",
      "epoch 158 iter 43 loss=0.03635192662477493\n",
      "epoch 158 iter 44 loss=0.028112661093473434\n",
      "epoch 158 iter 45 loss=0.03435700759291649\n",
      "epoch 158 iter 46 loss=0.05392805486917496\n",
      "epoch 158 iter 47 loss=0.03597671538591385\n",
      "epoch 158 iter 48 loss=0.03646622598171234\n",
      "epoch 158 iter 49 loss=0.039101097732782364\n",
      "epoch 158 iter 50 loss=0.022346481680870056\n",
      "epoch 158 iter 51 loss=0.017608642578125\n",
      "epoch 158 iter 52 loss=0.02265855297446251\n",
      "epoch 158 iter 53 loss=0.03221992403268814\n",
      "epoch 158 iter 54 loss=0.02613659016788006\n",
      "epoch 158 iter 55 loss=0.030804259702563286\n",
      "epoch 158 iter 56 loss=0.03632010519504547\n",
      "epoch 158 iter 57 loss=0.050851497799158096\n",
      "epoch 158 iter 58 loss=0.0240732841193676\n",
      "epoch 158 iter 59 loss=0.04955310747027397\n",
      "epoch 158 iter 60 loss=0.026366835460066795\n",
      "epoch 158 iter 61 loss=0.06144415959715843\n",
      "epoch 158 iter 62 loss=0.051106780767440796\n",
      "epoch 158 iter 63 loss=0.021611681208014488\n",
      "epoch 158 iter 64 loss=0.02565919980406761\n",
      "epoch 158 iter 65 loss=0.03171485289931297\n",
      "epoch 158 iter 66 loss=0.02964359149336815\n",
      "epoch 158 iter 67 loss=0.02169298380613327\n",
      "epoch 158 iter 68 loss=0.03793112933635712\n",
      "epoch 158 iter 69 loss=0.017291827127337456\n",
      "epoch 158 iter 70 loss=0.052437424659729004\n",
      "epoch 158 iter 71 loss=0.03967953100800514\n",
      "epoch 158 iter 72 loss=0.0492076650261879\n",
      "epoch 158 iter 73 loss=0.040275938808918\n",
      "epoch 158 iter 74 loss=0.032264746725559235\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3208.\n",
      "epoch 159 iter 0 loss=0.043642379343509674\n",
      "epoch 159 iter 1 loss=0.032883819192647934\n",
      "epoch 159 iter 2 loss=0.05040735751390457\n",
      "epoch 159 iter 3 loss=0.04613533988595009\n",
      "epoch 159 iter 4 loss=0.03368419036269188\n",
      "epoch 159 iter 5 loss=0.0377453938126564\n",
      "epoch 159 iter 6 loss=0.028048334643244743\n",
      "epoch 159 iter 7 loss=0.03359128534793854\n",
      "epoch 159 iter 8 loss=0.037338737398386\n",
      "epoch 159 iter 9 loss=0.03317057713866234\n",
      "epoch 159 iter 10 loss=0.021648166701197624\n",
      "epoch 159 iter 11 loss=0.052294500172138214\n",
      "epoch 159 iter 12 loss=0.03826490417122841\n",
      "epoch 159 iter 13 loss=0.04086797684431076\n",
      "epoch 159 iter 14 loss=0.0315391980111599\n",
      "epoch 159 iter 15 loss=0.02656821720302105\n",
      "epoch 159 iter 16 loss=0.03835418447852135\n",
      "epoch 159 iter 17 loss=0.027229098603129387\n",
      "epoch 159 iter 18 loss=0.04227643832564354\n",
      "epoch 159 iter 19 loss=0.023896779865026474\n",
      "epoch 159 iter 20 loss=0.03379829227924347\n",
      "epoch 159 iter 21 loss=0.0414937362074852\n",
      "epoch 159 iter 22 loss=0.06226008012890816\n",
      "epoch 159 iter 23 loss=0.03781891614198685\n",
      "epoch 159 iter 24 loss=0.04056057706475258\n",
      "epoch 159 iter 25 loss=0.02860104851424694\n",
      "epoch 159 iter 26 loss=0.03613775596022606\n",
      "epoch 159 iter 27 loss=0.05767260119318962\n",
      "epoch 159 iter 28 loss=0.01813514344394207\n",
      "epoch 159 iter 29 loss=0.045264389365911484\n",
      "epoch 159 iter 30 loss=0.0405580960214138\n",
      "epoch 159 iter 31 loss=0.04003781080245972\n",
      "epoch 159 iter 32 loss=0.039063919335603714\n",
      "epoch 159 iter 33 loss=0.03702737018465996\n",
      "epoch 159 iter 34 loss=0.026142409071326256\n",
      "epoch 159 iter 35 loss=0.053844042122364044\n",
      "epoch 159 iter 36 loss=0.03739229589700699\n",
      "epoch 159 iter 37 loss=0.04620427265763283\n",
      "epoch 159 iter 38 loss=0.05419425293803215\n",
      "epoch 159 iter 39 loss=0.050695013254880905\n",
      "epoch 159 iter 40 loss=0.028282281011343002\n",
      "epoch 159 iter 41 loss=0.04283016175031662\n",
      "epoch 159 iter 42 loss=0.0428229458630085\n",
      "epoch 159 iter 43 loss=0.04662998765707016\n",
      "epoch 159 iter 44 loss=0.03190139681100845\n",
      "epoch 159 iter 45 loss=0.037084609270095825\n",
      "epoch 159 iter 46 loss=0.021811475977301598\n",
      "epoch 159 iter 47 loss=0.03610687702894211\n",
      "epoch 159 iter 48 loss=0.040974829345941544\n",
      "epoch 159 iter 49 loss=0.031533073633909225\n",
      "epoch 159 iter 50 loss=0.03155679255723953\n",
      "epoch 159 iter 51 loss=0.026664137840270996\n",
      "epoch 159 iter 52 loss=0.02006826177239418\n",
      "epoch 159 iter 53 loss=0.04435092583298683\n",
      "epoch 159 iter 54 loss=0.031418025493621826\n",
      "epoch 159 iter 55 loss=0.02022097259759903\n",
      "epoch 159 iter 56 loss=0.02368917688727379\n",
      "epoch 159 iter 57 loss=0.023534400388598442\n",
      "epoch 159 iter 58 loss=0.015033147297799587\n",
      "epoch 159 iter 59 loss=0.02567162550985813\n",
      "epoch 159 iter 60 loss=0.033021558076143265\n",
      "epoch 159 iter 61 loss=0.054135244339704514\n",
      "epoch 159 iter 62 loss=0.03468756005167961\n",
      "epoch 159 iter 63 loss=0.03209706395864487\n",
      "epoch 159 iter 64 loss=0.021806558594107628\n",
      "epoch 159 iter 65 loss=0.04861119017004967\n",
      "epoch 159 iter 66 loss=0.028699606657028198\n",
      "epoch 159 iter 67 loss=0.026944207027554512\n",
      "epoch 159 iter 68 loss=0.020331230014562607\n",
      "epoch 159 iter 69 loss=0.04882184788584709\n",
      "epoch 159 iter 70 loss=0.044244058430194855\n",
      "epoch 159 iter 71 loss=0.0481913797557354\n",
      "epoch 159 iter 72 loss=0.020142296329140663\n",
      "epoch 159 iter 73 loss=0.03018999472260475\n",
      "epoch 159 iter 74 loss=0.03715093433856964\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3168.\n",
      "epoch 160 iter 0 loss=0.04548060521483421\n",
      "epoch 160 iter 1 loss=0.03263716399669647\n",
      "epoch 160 iter 2 loss=0.03183664754033089\n",
      "epoch 160 iter 3 loss=0.026430178433656693\n",
      "epoch 160 iter 4 loss=0.03611752390861511\n",
      "epoch 160 iter 5 loss=0.026794323697686195\n",
      "epoch 160 iter 6 loss=0.03246234729886055\n",
      "epoch 160 iter 7 loss=0.04508880525827408\n",
      "epoch 160 iter 8 loss=0.046135105192661285\n",
      "epoch 160 iter 9 loss=0.04104705899953842\n",
      "epoch 160 iter 10 loss=0.028738053515553474\n",
      "epoch 160 iter 11 loss=0.0206432081758976\n",
      "epoch 160 iter 12 loss=0.03039725311100483\n",
      "epoch 160 iter 13 loss=0.032379016280174255\n",
      "epoch 160 iter 14 loss=0.02194788306951523\n",
      "epoch 160 iter 15 loss=0.046989548951387405\n",
      "epoch 160 iter 16 loss=0.041004009544849396\n",
      "epoch 160 iter 17 loss=0.03844258189201355\n",
      "epoch 160 iter 18 loss=0.047041669487953186\n",
      "epoch 160 iter 19 loss=0.04321533069014549\n",
      "epoch 160 iter 20 loss=0.02046903967857361\n",
      "epoch 160 iter 21 loss=0.04474812000989914\n",
      "epoch 160 iter 22 loss=0.045177049934864044\n",
      "epoch 160 iter 23 loss=0.03388400748372078\n",
      "epoch 160 iter 24 loss=0.047041155397892\n",
      "epoch 160 iter 25 loss=0.033994320780038834\n",
      "epoch 160 iter 26 loss=0.019159238785505295\n",
      "epoch 160 iter 27 loss=0.026196828112006187\n",
      "epoch 160 iter 28 loss=0.052019525319337845\n",
      "epoch 160 iter 29 loss=0.03795706853270531\n",
      "epoch 160 iter 30 loss=0.038553573191165924\n",
      "epoch 160 iter 31 loss=0.04331931099295616\n",
      "epoch 160 iter 32 loss=0.01990061067044735\n",
      "epoch 160 iter 33 loss=0.038069359958171844\n",
      "epoch 160 iter 34 loss=0.02059199847280979\n",
      "epoch 160 iter 35 loss=0.03785838186740875\n",
      "epoch 160 iter 36 loss=0.020128579810261726\n",
      "epoch 160 iter 37 loss=0.020799513906240463\n",
      "epoch 160 iter 38 loss=0.04024451971054077\n",
      "epoch 160 iter 39 loss=0.033303190022706985\n",
      "epoch 160 iter 40 loss=0.029178768396377563\n",
      "epoch 160 iter 41 loss=0.03778311610221863\n",
      "epoch 160 iter 42 loss=0.030161861330270767\n",
      "epoch 160 iter 43 loss=0.028295600786805153\n",
      "epoch 160 iter 44 loss=0.02914673089981079\n",
      "epoch 160 iter 45 loss=0.030679238960146904\n",
      "epoch 160 iter 46 loss=0.0360296294093132\n",
      "epoch 160 iter 47 loss=0.05049864575266838\n",
      "epoch 160 iter 48 loss=0.037271566689014435\n",
      "epoch 160 iter 49 loss=0.03625553101301193\n",
      "epoch 160 iter 50 loss=0.02532614767551422\n",
      "epoch 160 iter 51 loss=0.018493976444005966\n",
      "epoch 160 iter 52 loss=0.04165931046009064\n",
      "epoch 160 iter 53 loss=0.04594622179865837\n",
      "epoch 160 iter 54 loss=0.04531767964363098\n",
      "epoch 160 iter 55 loss=0.05590525642037392\n",
      "epoch 160 iter 56 loss=0.022194750607013702\n",
      "epoch 160 iter 57 loss=0.04831709340214729\n",
      "epoch 160 iter 58 loss=0.03197450563311577\n",
      "epoch 160 iter 59 loss=0.05302604287862778\n",
      "epoch 160 iter 60 loss=0.02895783632993698\n",
      "epoch 160 iter 61 loss=0.035004179924726486\n",
      "epoch 160 iter 62 loss=0.0308894831687212\n",
      "epoch 160 iter 63 loss=0.03944184258580208\n",
      "epoch 160 iter 64 loss=0.027337318286299706\n",
      "epoch 160 iter 65 loss=0.034469474107027054\n",
      "epoch 160 iter 66 loss=0.02792017161846161\n",
      "epoch 160 iter 67 loss=0.04148223623633385\n",
      "epoch 160 iter 68 loss=0.04424942284822464\n",
      "epoch 160 iter 69 loss=0.05576163902878761\n",
      "epoch 160 iter 70 loss=0.03362630307674408\n",
      "epoch 160 iter 71 loss=0.03302528336644173\n",
      "epoch 160 iter 72 loss=0.06061537191271782\n",
      "epoch 160 iter 73 loss=0.028588080778717995\n",
      "epoch 160 iter 74 loss=0.03376861661672592\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3178.\n",
      "epoch 161 iter 0 loss=0.026285849511623383\n",
      "epoch 161 iter 1 loss=0.042121291160583496\n",
      "epoch 161 iter 2 loss=0.03830333799123764\n",
      "epoch 161 iter 3 loss=0.0414116196334362\n",
      "epoch 161 iter 4 loss=0.0378451831638813\n",
      "epoch 161 iter 5 loss=0.038389209657907486\n",
      "epoch 161 iter 6 loss=0.02065684087574482\n",
      "epoch 161 iter 7 loss=0.038099128752946854\n",
      "epoch 161 iter 8 loss=0.03877861052751541\n",
      "epoch 161 iter 9 loss=0.017618009820580482\n",
      "epoch 161 iter 10 loss=0.0428326316177845\n",
      "epoch 161 iter 11 loss=0.037450846284627914\n",
      "epoch 161 iter 12 loss=0.02810131385922432\n",
      "epoch 161 iter 13 loss=0.049609117209911346\n",
      "epoch 161 iter 14 loss=0.03304925188422203\n",
      "epoch 161 iter 15 loss=0.0387842170894146\n",
      "epoch 161 iter 16 loss=0.04590464383363724\n",
      "epoch 161 iter 17 loss=0.030604682862758636\n",
      "epoch 161 iter 18 loss=0.027223676443099976\n",
      "epoch 161 iter 19 loss=0.033326178789138794\n",
      "epoch 161 iter 20 loss=0.0524350069463253\n",
      "epoch 161 iter 21 loss=0.05072958394885063\n",
      "epoch 161 iter 22 loss=0.04060374200344086\n",
      "epoch 161 iter 23 loss=0.02647308260202408\n",
      "epoch 161 iter 24 loss=0.03349756449460983\n",
      "epoch 161 iter 25 loss=0.021124722436070442\n",
      "epoch 161 iter 26 loss=0.040308546274900436\n",
      "epoch 161 iter 27 loss=0.060103628784418106\n",
      "epoch 161 iter 28 loss=0.023521002382040024\n",
      "epoch 161 iter 29 loss=0.044338732957839966\n",
      "epoch 161 iter 30 loss=0.029976293444633484\n",
      "epoch 161 iter 31 loss=0.031952742487192154\n",
      "epoch 161 iter 32 loss=0.04296500235795975\n",
      "epoch 161 iter 33 loss=0.03794201835989952\n",
      "epoch 161 iter 34 loss=0.025560282170772552\n",
      "epoch 161 iter 35 loss=0.022912191227078438\n",
      "epoch 161 iter 36 loss=0.01994546502828598\n",
      "epoch 161 iter 37 loss=0.02036724053323269\n",
      "epoch 161 iter 38 loss=0.024239052087068558\n",
      "epoch 161 iter 39 loss=0.0313427597284317\n",
      "epoch 161 iter 40 loss=0.028110435232520103\n",
      "epoch 161 iter 41 loss=0.04082808271050453\n",
      "epoch 161 iter 42 loss=0.052126821130514145\n",
      "epoch 161 iter 43 loss=0.04835427552461624\n",
      "epoch 161 iter 44 loss=0.023162761703133583\n",
      "epoch 161 iter 45 loss=0.04078629985451698\n",
      "epoch 161 iter 46 loss=0.038817089051008224\n",
      "epoch 161 iter 47 loss=0.03601706027984619\n",
      "epoch 161 iter 48 loss=0.021765287965536118\n",
      "epoch 161 iter 49 loss=0.040322039276361465\n",
      "epoch 161 iter 50 loss=0.0253332257270813\n",
      "epoch 161 iter 51 loss=0.03079419769346714\n",
      "epoch 161 iter 52 loss=0.0466986708343029\n",
      "epoch 161 iter 53 loss=0.05021095648407936\n",
      "epoch 161 iter 54 loss=0.04169530048966408\n",
      "epoch 161 iter 55 loss=0.03575821965932846\n",
      "epoch 161 iter 56 loss=0.03071637824177742\n",
      "epoch 161 iter 57 loss=0.029647059738636017\n",
      "epoch 161 iter 58 loss=0.040468670427799225\n",
      "epoch 161 iter 59 loss=0.026511648669838905\n",
      "epoch 161 iter 60 loss=0.046962179243564606\n",
      "epoch 161 iter 61 loss=0.03657341003417969\n",
      "epoch 161 iter 62 loss=0.04136468842625618\n",
      "epoch 161 iter 63 loss=0.02194264531135559\n",
      "epoch 161 iter 64 loss=0.028596604242920876\n",
      "epoch 161 iter 65 loss=0.05085210129618645\n",
      "epoch 161 iter 66 loss=0.04881921783089638\n",
      "epoch 161 iter 67 loss=0.029158249497413635\n",
      "epoch 161 iter 68 loss=0.032335154712200165\n",
      "epoch 161 iter 69 loss=0.02698519080877304\n",
      "epoch 161 iter 70 loss=0.029918530955910683\n",
      "epoch 161 iter 71 loss=0.03187429904937744\n",
      "epoch 161 iter 72 loss=0.05423716828227043\n",
      "epoch 161 iter 73 loss=0.031478457152843475\n",
      "epoch 161 iter 74 loss=0.05866748094558716\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3163.\n",
      "epoch 162 iter 0 loss=0.04279113933444023\n",
      "epoch 162 iter 1 loss=0.03941887989640236\n",
      "epoch 162 iter 2 loss=0.02119929902255535\n",
      "epoch 162 iter 3 loss=0.02975708618760109\n",
      "epoch 162 iter 4 loss=0.061566490679979324\n",
      "epoch 162 iter 5 loss=0.03873153775930405\n",
      "epoch 162 iter 6 loss=0.020860718563199043\n",
      "epoch 162 iter 7 loss=0.04625548794865608\n",
      "epoch 162 iter 8 loss=0.030939770862460136\n",
      "epoch 162 iter 9 loss=0.041114967316389084\n",
      "epoch 162 iter 10 loss=0.0328199677169323\n",
      "epoch 162 iter 11 loss=0.03461483120918274\n",
      "epoch 162 iter 12 loss=0.032212886959314346\n",
      "epoch 162 iter 13 loss=0.03805144876241684\n",
      "epoch 162 iter 14 loss=0.04532567039132118\n",
      "epoch 162 iter 15 loss=0.03858364000916481\n",
      "epoch 162 iter 16 loss=0.025715988129377365\n",
      "epoch 162 iter 17 loss=0.043173570185899734\n",
      "epoch 162 iter 18 loss=0.027657847851514816\n",
      "epoch 162 iter 19 loss=0.02483338676393032\n",
      "epoch 162 iter 20 loss=0.03628022223711014\n",
      "epoch 162 iter 21 loss=0.03593158349394798\n",
      "epoch 162 iter 22 loss=0.025318823754787445\n",
      "epoch 162 iter 23 loss=0.04637230187654495\n",
      "epoch 162 iter 24 loss=0.018673723563551903\n",
      "epoch 162 iter 25 loss=0.03952605649828911\n",
      "epoch 162 iter 26 loss=0.01963324472308159\n",
      "epoch 162 iter 27 loss=0.028916947543621063\n",
      "epoch 162 iter 28 loss=0.04494608938694\n",
      "epoch 162 iter 29 loss=0.04470939189195633\n",
      "epoch 162 iter 30 loss=0.025763994082808495\n",
      "epoch 162 iter 31 loss=0.022665629163384438\n",
      "epoch 162 iter 32 loss=0.03640802949666977\n",
      "epoch 162 iter 33 loss=0.030712442472577095\n",
      "epoch 162 iter 34 loss=0.026291199028491974\n",
      "epoch 162 iter 35 loss=0.024083785712718964\n",
      "epoch 162 iter 36 loss=0.04561375826597214\n",
      "epoch 162 iter 37 loss=0.04290032759308815\n",
      "epoch 162 iter 38 loss=0.03961143642663956\n",
      "epoch 162 iter 39 loss=0.03141004219651222\n",
      "epoch 162 iter 40 loss=0.05798031762242317\n",
      "epoch 162 iter 41 loss=0.03759613633155823\n",
      "epoch 162 iter 42 loss=0.052440859377384186\n",
      "epoch 162 iter 43 loss=0.04360739141702652\n",
      "epoch 162 iter 44 loss=0.028826186433434486\n",
      "epoch 162 iter 45 loss=0.030726265162229538\n",
      "epoch 162 iter 46 loss=0.029585719108581543\n",
      "epoch 162 iter 47 loss=0.03897342458367348\n",
      "epoch 162 iter 48 loss=0.03050532191991806\n",
      "epoch 162 iter 49 loss=0.02648494951426983\n",
      "epoch 162 iter 50 loss=0.024834657087922096\n",
      "epoch 162 iter 51 loss=0.02766210399568081\n",
      "epoch 162 iter 52 loss=0.03458954021334648\n",
      "epoch 162 iter 53 loss=0.01824996992945671\n",
      "epoch 162 iter 54 loss=0.06721683591604233\n",
      "epoch 162 iter 55 loss=0.03481118753552437\n",
      "epoch 162 iter 56 loss=0.036068130284547806\n",
      "epoch 162 iter 57 loss=0.05618662014603615\n",
      "epoch 162 iter 58 loss=0.026719195768237114\n",
      "epoch 162 iter 59 loss=0.03799744322896004\n",
      "epoch 162 iter 60 loss=0.04940250143408775\n",
      "epoch 162 iter 61 loss=0.03725021332502365\n",
      "epoch 162 iter 62 loss=0.03035874292254448\n",
      "epoch 162 iter 63 loss=0.0593680664896965\n",
      "epoch 162 iter 64 loss=0.026938078925013542\n",
      "epoch 162 iter 65 loss=0.023161720484495163\n",
      "epoch 162 iter 66 loss=0.040253348648548126\n",
      "epoch 162 iter 67 loss=0.04380287230014801\n",
      "epoch 162 iter 68 loss=0.05357365682721138\n",
      "epoch 162 iter 69 loss=0.022096147760748863\n",
      "epoch 162 iter 70 loss=0.040738239884376526\n",
      "epoch 162 iter 71 loss=0.03980611637234688\n",
      "epoch 162 iter 72 loss=0.03199693188071251\n",
      "epoch 162 iter 73 loss=0.0141756571829319\n",
      "epoch 162 iter 74 loss=0.028279084712266922\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3183.\n",
      "epoch 163 iter 0 loss=0.04230663925409317\n",
      "epoch 163 iter 1 loss=0.0503978431224823\n",
      "epoch 163 iter 2 loss=0.047585561871528625\n",
      "epoch 163 iter 3 loss=0.03160325065255165\n",
      "epoch 163 iter 4 loss=0.04023811221122742\n",
      "epoch 163 iter 5 loss=0.04243047162890434\n",
      "epoch 163 iter 6 loss=0.05491332337260246\n",
      "epoch 163 iter 7 loss=0.03506407141685486\n",
      "epoch 163 iter 8 loss=0.027273710817098618\n",
      "epoch 163 iter 9 loss=0.035389259457588196\n",
      "epoch 163 iter 10 loss=0.0360933355987072\n",
      "epoch 163 iter 11 loss=0.046126965433359146\n",
      "epoch 163 iter 12 loss=0.036995213478803635\n",
      "epoch 163 iter 13 loss=0.028026524931192398\n",
      "epoch 163 iter 14 loss=0.020524535328149796\n",
      "epoch 163 iter 15 loss=0.02976449579000473\n",
      "epoch 163 iter 16 loss=0.03738689050078392\n",
      "epoch 163 iter 17 loss=0.038793765008449554\n",
      "epoch 163 iter 18 loss=0.031856339424848557\n",
      "epoch 163 iter 19 loss=0.03477948158979416\n",
      "epoch 163 iter 20 loss=0.028284840285778046\n",
      "epoch 163 iter 21 loss=0.03368430957198143\n",
      "epoch 163 iter 22 loss=0.03455754742026329\n",
      "epoch 163 iter 23 loss=0.04132845625281334\n",
      "epoch 163 iter 24 loss=0.031307026743888855\n",
      "epoch 163 iter 25 loss=0.033471960574388504\n",
      "epoch 163 iter 26 loss=0.04031696170568466\n",
      "epoch 163 iter 27 loss=0.02963436394929886\n",
      "epoch 163 iter 28 loss=0.03117837943136692\n",
      "epoch 163 iter 29 loss=0.030358437448740005\n",
      "epoch 163 iter 30 loss=0.022269301116466522\n",
      "epoch 163 iter 31 loss=0.06448163837194443\n",
      "epoch 163 iter 32 loss=0.018994946032762527\n",
      "epoch 163 iter 33 loss=0.02993069402873516\n",
      "epoch 163 iter 34 loss=0.04740414023399353\n",
      "epoch 163 iter 35 loss=0.034292422235012054\n",
      "epoch 163 iter 36 loss=0.023629650473594666\n",
      "epoch 163 iter 37 loss=0.030207781121134758\n",
      "epoch 163 iter 38 loss=0.036443375051021576\n",
      "epoch 163 iter 39 loss=0.032724227756261826\n",
      "epoch 163 iter 40 loss=0.014939128421247005\n",
      "epoch 163 iter 41 loss=0.04521797597408295\n",
      "epoch 163 iter 42 loss=0.043554920703172684\n",
      "epoch 163 iter 43 loss=0.040719833225011826\n",
      "epoch 163 iter 44 loss=0.04105166345834732\n",
      "epoch 163 iter 45 loss=0.018010081723332405\n",
      "epoch 163 iter 46 loss=0.03754051402211189\n",
      "epoch 163 iter 47 loss=0.03164101392030716\n",
      "epoch 163 iter 48 loss=0.0358373299241066\n",
      "epoch 163 iter 49 loss=0.040229737758636475\n",
      "epoch 163 iter 50 loss=0.045330677181482315\n",
      "epoch 163 iter 51 loss=0.020721541717648506\n",
      "epoch 163 iter 52 loss=0.03575925901532173\n",
      "epoch 163 iter 53 loss=0.03746984153985977\n",
      "epoch 163 iter 54 loss=0.03746340423822403\n",
      "epoch 163 iter 55 loss=0.035490162670612335\n",
      "epoch 163 iter 56 loss=0.03164142370223999\n",
      "epoch 163 iter 57 loss=0.02715894766151905\n",
      "epoch 163 iter 58 loss=0.025695092976093292\n",
      "epoch 163 iter 59 loss=0.03595912083983421\n",
      "epoch 163 iter 60 loss=0.03526497259736061\n",
      "epoch 163 iter 61 loss=0.0295189768075943\n",
      "epoch 163 iter 62 loss=0.03061485104262829\n",
      "epoch 163 iter 63 loss=0.0379486121237278\n",
      "epoch 163 iter 64 loss=0.0235323254019022\n",
      "epoch 163 iter 65 loss=0.04727112129330635\n",
      "epoch 163 iter 66 loss=0.059768196195364\n",
      "epoch 163 iter 67 loss=0.04955563694238663\n",
      "epoch 163 iter 68 loss=0.05356277897953987\n",
      "epoch 163 iter 69 loss=0.024242840707302094\n",
      "epoch 163 iter 70 loss=0.037998657673597336\n",
      "epoch 163 iter 71 loss=0.02526373229920864\n",
      "epoch 163 iter 72 loss=0.03937441110610962\n",
      "epoch 163 iter 73 loss=0.038442134857177734\n",
      "epoch 163 iter 74 loss=0.043264444917440414\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3188.\n",
      "epoch 164 iter 0 loss=0.02678374946117401\n",
      "epoch 164 iter 1 loss=0.02737240493297577\n",
      "epoch 164 iter 2 loss=0.05416572466492653\n",
      "epoch 164 iter 3 loss=0.03338460996747017\n",
      "epoch 164 iter 4 loss=0.026070190593600273\n",
      "epoch 164 iter 5 loss=0.039365608245134354\n",
      "epoch 164 iter 6 loss=0.051587603986263275\n",
      "epoch 164 iter 7 loss=0.02568788081407547\n",
      "epoch 164 iter 8 loss=0.031340114772319794\n",
      "epoch 164 iter 9 loss=0.03466168791055679\n",
      "epoch 164 iter 10 loss=0.05771726742386818\n",
      "epoch 164 iter 11 loss=0.05371996387839317\n",
      "epoch 164 iter 12 loss=0.05858553573489189\n",
      "epoch 164 iter 13 loss=0.03099134750664234\n",
      "epoch 164 iter 14 loss=0.02405501715838909\n",
      "epoch 164 iter 15 loss=0.029499396681785583\n",
      "epoch 164 iter 16 loss=0.027590909972786903\n",
      "epoch 164 iter 17 loss=0.03490974009037018\n",
      "epoch 164 iter 18 loss=0.04202335700392723\n",
      "epoch 164 iter 19 loss=0.031931862235069275\n",
      "epoch 164 iter 20 loss=0.03617774322628975\n",
      "epoch 164 iter 21 loss=0.03878851979970932\n",
      "epoch 164 iter 22 loss=0.032016828656196594\n",
      "epoch 164 iter 23 loss=0.03646709769964218\n",
      "epoch 164 iter 24 loss=0.03082156553864479\n",
      "epoch 164 iter 25 loss=0.03783814236521721\n",
      "epoch 164 iter 26 loss=0.02001097984611988\n",
      "epoch 164 iter 27 loss=0.038431935012340546\n",
      "epoch 164 iter 28 loss=0.03593519702553749\n",
      "epoch 164 iter 29 loss=0.022062944248318672\n",
      "epoch 164 iter 30 loss=0.0340554341673851\n",
      "epoch 164 iter 31 loss=0.045245133340358734\n",
      "epoch 164 iter 32 loss=0.02216990292072296\n",
      "epoch 164 iter 33 loss=0.04286884516477585\n",
      "epoch 164 iter 34 loss=0.033343005925416946\n",
      "epoch 164 iter 35 loss=0.03141595050692558\n",
      "epoch 164 iter 36 loss=0.035178594291210175\n",
      "epoch 164 iter 37 loss=0.04396594315767288\n",
      "epoch 164 iter 38 loss=0.02589758113026619\n",
      "epoch 164 iter 39 loss=0.025634262710809708\n",
      "epoch 164 iter 40 loss=0.03630908206105232\n",
      "epoch 164 iter 41 loss=0.040439266711473465\n",
      "epoch 164 iter 42 loss=0.01790102943778038\n",
      "epoch 164 iter 43 loss=0.02558840624988079\n",
      "epoch 164 iter 44 loss=0.02078193612396717\n",
      "epoch 164 iter 45 loss=0.05225410684943199\n",
      "epoch 164 iter 46 loss=0.038083650171756744\n",
      "epoch 164 iter 47 loss=0.034606728702783585\n",
      "epoch 164 iter 48 loss=0.04178735613822937\n",
      "epoch 164 iter 49 loss=0.039395689964294434\n",
      "epoch 164 iter 50 loss=0.04163700342178345\n",
      "epoch 164 iter 51 loss=0.03650319576263428\n",
      "epoch 164 iter 52 loss=0.05606229603290558\n",
      "epoch 164 iter 53 loss=0.019389037042856216\n",
      "epoch 164 iter 54 loss=0.04459977149963379\n",
      "epoch 164 iter 55 loss=0.035793453454971313\n",
      "epoch 164 iter 56 loss=0.046403080224990845\n",
      "epoch 164 iter 57 loss=0.0396682471036911\n",
      "epoch 164 iter 58 loss=0.02125793881714344\n",
      "epoch 164 iter 59 loss=0.04142860323190689\n",
      "epoch 164 iter 60 loss=0.017092734575271606\n",
      "epoch 164 iter 61 loss=0.03748525306582451\n",
      "epoch 164 iter 62 loss=0.03317943960428238\n",
      "epoch 164 iter 63 loss=0.04317809268832207\n",
      "epoch 164 iter 64 loss=0.03212606534361839\n",
      "epoch 164 iter 65 loss=0.0253340732306242\n",
      "epoch 164 iter 66 loss=0.04834248498082161\n",
      "epoch 164 iter 67 loss=0.06481027603149414\n",
      "epoch 164 iter 68 loss=0.02835801988840103\n",
      "epoch 164 iter 69 loss=0.03161461651325226\n",
      "epoch 164 iter 70 loss=0.02443290501832962\n",
      "epoch 164 iter 71 loss=0.03584516420960426\n",
      "epoch 164 iter 72 loss=0.038511235266923904\n",
      "epoch 164 iter 73 loss=0.03971623256802559\n",
      "epoch 164 iter 74 loss=0.02250628173351288\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3177.\n",
      "epoch 165 iter 0 loss=0.03841523081064224\n",
      "epoch 165 iter 1 loss=0.03975089266896248\n",
      "epoch 165 iter 2 loss=0.047699276357889175\n",
      "epoch 165 iter 3 loss=0.03633393347263336\n",
      "epoch 165 iter 4 loss=0.0410039946436882\n",
      "epoch 165 iter 5 loss=0.046210289001464844\n",
      "epoch 165 iter 6 loss=0.03046363964676857\n",
      "epoch 165 iter 7 loss=0.04670053347945213\n",
      "epoch 165 iter 8 loss=0.02800603024661541\n",
      "epoch 165 iter 9 loss=0.025649424642324448\n",
      "epoch 165 iter 10 loss=0.03930680453777313\n",
      "epoch 165 iter 11 loss=0.04495905712246895\n",
      "epoch 165 iter 12 loss=0.02687571570277214\n",
      "epoch 165 iter 13 loss=0.03892461210489273\n",
      "epoch 165 iter 14 loss=0.05626828223466873\n",
      "epoch 165 iter 15 loss=0.037082768976688385\n",
      "epoch 165 iter 16 loss=0.035008229315280914\n",
      "epoch 165 iter 17 loss=0.04139893501996994\n",
      "epoch 165 iter 18 loss=0.03290559723973274\n",
      "epoch 165 iter 19 loss=0.037678591907024384\n",
      "epoch 165 iter 20 loss=0.03379908949136734\n",
      "epoch 165 iter 21 loss=0.022256944328546524\n",
      "epoch 165 iter 22 loss=0.025739040225744247\n",
      "epoch 165 iter 23 loss=0.016474327072501183\n",
      "epoch 165 iter 24 loss=0.03189070522785187\n",
      "epoch 165 iter 25 loss=0.021061167120933533\n",
      "epoch 165 iter 26 loss=0.039185602217912674\n",
      "epoch 165 iter 27 loss=0.03802945837378502\n",
      "epoch 165 iter 28 loss=0.04319985210895538\n",
      "epoch 165 iter 29 loss=0.022409804165363312\n",
      "epoch 165 iter 30 loss=0.049278661608695984\n",
      "epoch 165 iter 31 loss=0.035264160484075546\n",
      "epoch 165 iter 32 loss=0.03944536671042442\n",
      "epoch 165 iter 33 loss=0.03504480794072151\n",
      "epoch 165 iter 34 loss=0.028227269649505615\n",
      "epoch 165 iter 35 loss=0.035630691796541214\n",
      "epoch 165 iter 36 loss=0.0503782294690609\n",
      "epoch 165 iter 37 loss=0.019369367510080338\n",
      "epoch 165 iter 38 loss=0.026086997240781784\n",
      "epoch 165 iter 39 loss=0.03863148391246796\n",
      "epoch 165 iter 40 loss=0.030542634427547455\n",
      "epoch 165 iter 41 loss=0.04588129371404648\n",
      "epoch 165 iter 42 loss=0.03453422710299492\n",
      "epoch 165 iter 43 loss=0.028099918738007545\n",
      "epoch 165 iter 44 loss=0.030487190932035446\n",
      "epoch 165 iter 45 loss=0.05400518327951431\n",
      "epoch 165 iter 46 loss=0.044107187539339066\n",
      "epoch 165 iter 47 loss=0.052754756063222885\n",
      "epoch 165 iter 48 loss=0.037129443138837814\n",
      "epoch 165 iter 49 loss=0.024159550666809082\n",
      "epoch 165 iter 50 loss=0.0319562703371048\n",
      "epoch 165 iter 51 loss=0.05037536472082138\n",
      "epoch 165 iter 52 loss=0.016045020893216133\n",
      "epoch 165 iter 53 loss=0.04276295378804207\n",
      "epoch 165 iter 54 loss=0.02764994651079178\n",
      "epoch 165 iter 55 loss=0.018012339249253273\n",
      "epoch 165 iter 56 loss=0.042654503136873245\n",
      "epoch 165 iter 57 loss=0.03298050910234451\n",
      "epoch 165 iter 58 loss=0.031399138271808624\n",
      "epoch 165 iter 59 loss=0.04299607872962952\n",
      "epoch 165 iter 60 loss=0.03669533506035805\n",
      "epoch 165 iter 61 loss=0.03660816326737404\n",
      "epoch 165 iter 62 loss=0.04608596861362457\n",
      "epoch 165 iter 63 loss=0.032216574996709824\n",
      "epoch 165 iter 64 loss=0.038592394441366196\n",
      "epoch 165 iter 65 loss=0.05741475149989128\n",
      "epoch 165 iter 66 loss=0.021872352808713913\n",
      "epoch 165 iter 67 loss=0.025030797347426414\n",
      "epoch 165 iter 68 loss=0.041075799614191055\n",
      "epoch 165 iter 69 loss=0.03253970295190811\n",
      "epoch 165 iter 70 loss=0.02754170075058937\n",
      "epoch 165 iter 71 loss=0.04248996824026108\n",
      "epoch 165 iter 72 loss=0.0372123047709465\n",
      "epoch 165 iter 73 loss=0.029324574396014214\n",
      "epoch 165 iter 74 loss=0.03692418709397316\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3177.\n",
      "epoch 166 iter 0 loss=0.04116920381784439\n",
      "epoch 166 iter 1 loss=0.031713809818029404\n",
      "epoch 166 iter 2 loss=0.04955130070447922\n",
      "epoch 166 iter 3 loss=0.026646696031093597\n",
      "epoch 166 iter 4 loss=0.012641126289963722\n",
      "epoch 166 iter 5 loss=0.03621043264865875\n",
      "epoch 166 iter 6 loss=0.020913058891892433\n",
      "epoch 166 iter 7 loss=0.05228152126073837\n",
      "epoch 166 iter 8 loss=0.03122721239924431\n",
      "epoch 166 iter 9 loss=0.024874500930309296\n",
      "epoch 166 iter 10 loss=0.04237780719995499\n",
      "epoch 166 iter 11 loss=0.022002527490258217\n",
      "epoch 166 iter 12 loss=0.03514505922794342\n",
      "epoch 166 iter 13 loss=0.030323248356580734\n",
      "epoch 166 iter 14 loss=0.02830243855714798\n",
      "epoch 166 iter 15 loss=0.04567804932594299\n",
      "epoch 166 iter 16 loss=0.04365203157067299\n",
      "epoch 166 iter 17 loss=0.03666151314973831\n",
      "epoch 166 iter 18 loss=0.02844497747719288\n",
      "epoch 166 iter 19 loss=0.03569230064749718\n",
      "epoch 166 iter 20 loss=0.0335206612944603\n",
      "epoch 166 iter 21 loss=0.03523504361510277\n",
      "epoch 166 iter 22 loss=0.02372032403945923\n",
      "epoch 166 iter 23 loss=0.018227122724056244\n",
      "epoch 166 iter 24 loss=0.017623649910092354\n",
      "epoch 166 iter 25 loss=0.01601993665099144\n",
      "epoch 166 iter 26 loss=0.0352647602558136\n",
      "epoch 166 iter 27 loss=0.0501285083591938\n",
      "epoch 166 iter 28 loss=0.03164127469062805\n",
      "epoch 166 iter 29 loss=0.022346360608935356\n",
      "epoch 166 iter 30 loss=0.030803201720118523\n",
      "epoch 166 iter 31 loss=0.03381345421075821\n",
      "epoch 166 iter 32 loss=0.03382357954978943\n",
      "epoch 166 iter 33 loss=0.026505548506975174\n",
      "epoch 166 iter 34 loss=0.03697945922613144\n",
      "epoch 166 iter 35 loss=0.046845290809869766\n",
      "epoch 166 iter 36 loss=0.03676412254571915\n",
      "epoch 166 iter 37 loss=0.03287326917052269\n",
      "epoch 166 iter 38 loss=0.04611005261540413\n",
      "epoch 166 iter 39 loss=0.03164399415254593\n",
      "epoch 166 iter 40 loss=0.029646672308444977\n",
      "epoch 166 iter 41 loss=0.04337700828909874\n",
      "epoch 166 iter 42 loss=0.030123455449938774\n",
      "epoch 166 iter 43 loss=0.05623836815357208\n",
      "epoch 166 iter 44 loss=0.034825533628463745\n",
      "epoch 166 iter 45 loss=0.03764208033680916\n",
      "epoch 166 iter 46 loss=0.042271267622709274\n",
      "epoch 166 iter 47 loss=0.03308064490556717\n",
      "epoch 166 iter 48 loss=0.0491679385304451\n",
      "epoch 166 iter 49 loss=0.03112025558948517\n",
      "epoch 166 iter 50 loss=0.0383082740008831\n",
      "epoch 166 iter 51 loss=0.023447584360837936\n",
      "epoch 166 iter 52 loss=0.04052364081144333\n",
      "epoch 166 iter 53 loss=0.04060783237218857\n",
      "epoch 166 iter 54 loss=0.042547643184661865\n",
      "epoch 166 iter 55 loss=0.02747083082795143\n",
      "epoch 166 iter 56 loss=0.04242552071809769\n",
      "epoch 166 iter 57 loss=0.04810848459601402\n",
      "epoch 166 iter 58 loss=0.02830875664949417\n",
      "epoch 166 iter 59 loss=0.05160318315029144\n",
      "epoch 166 iter 60 loss=0.053305208683013916\n",
      "epoch 166 iter 61 loss=0.05190512537956238\n",
      "epoch 166 iter 62 loss=0.039234161376953125\n",
      "epoch 166 iter 63 loss=0.0458904393017292\n",
      "epoch 166 iter 64 loss=0.021732112392783165\n",
      "epoch 166 iter 65 loss=0.029381342232227325\n",
      "epoch 166 iter 66 loss=0.05168425291776657\n",
      "epoch 166 iter 67 loss=0.03070329688489437\n",
      "epoch 166 iter 68 loss=0.03452564403414726\n",
      "epoch 166 iter 69 loss=0.032824672758579254\n",
      "epoch 166 iter 70 loss=0.03492581099271774\n",
      "epoch 166 iter 71 loss=0.025923127308487892\n",
      "epoch 166 iter 72 loss=0.0375860221683979\n",
      "epoch 166 iter 73 loss=0.040291063487529755\n",
      "epoch 166 iter 74 loss=0.019152255728840828\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3179.\n",
      "epoch 167 iter 0 loss=0.0455482192337513\n",
      "epoch 167 iter 1 loss=0.03515017777681351\n",
      "epoch 167 iter 2 loss=0.0403042696416378\n",
      "epoch 167 iter 3 loss=0.02166570909321308\n",
      "epoch 167 iter 4 loss=0.042809754610061646\n",
      "epoch 167 iter 5 loss=0.030918365344405174\n",
      "epoch 167 iter 6 loss=0.028371483087539673\n",
      "epoch 167 iter 7 loss=0.02505079284310341\n",
      "epoch 167 iter 8 loss=0.02339194342494011\n",
      "epoch 167 iter 9 loss=0.030320197343826294\n",
      "epoch 167 iter 10 loss=0.04222045838832855\n",
      "epoch 167 iter 11 loss=0.022326594218611717\n",
      "epoch 167 iter 12 loss=0.030175667256116867\n",
      "epoch 167 iter 13 loss=0.05490725859999657\n",
      "epoch 167 iter 14 loss=0.02435375191271305\n",
      "epoch 167 iter 15 loss=0.0476682186126709\n",
      "epoch 167 iter 16 loss=0.036766622215509415\n",
      "epoch 167 iter 17 loss=0.06116824969649315\n",
      "epoch 167 iter 18 loss=0.03211626783013344\n",
      "epoch 167 iter 19 loss=0.0483875647187233\n",
      "epoch 167 iter 20 loss=0.024661056697368622\n",
      "epoch 167 iter 21 loss=0.05698153004050255\n",
      "epoch 167 iter 22 loss=0.0322921983897686\n",
      "epoch 167 iter 23 loss=0.05272908881306648\n",
      "epoch 167 iter 24 loss=0.03803235664963722\n",
      "epoch 167 iter 25 loss=0.04307050630450249\n",
      "epoch 167 iter 26 loss=0.025613591074943542\n",
      "epoch 167 iter 27 loss=0.019426897168159485\n",
      "epoch 167 iter 28 loss=0.022724254056811333\n",
      "epoch 167 iter 29 loss=0.03142797201871872\n",
      "epoch 167 iter 30 loss=0.03252043575048447\n",
      "epoch 167 iter 31 loss=0.05016437545418739\n",
      "epoch 167 iter 32 loss=0.033335473388433456\n",
      "epoch 167 iter 33 loss=0.04343486577272415\n",
      "epoch 167 iter 34 loss=0.020930202677845955\n",
      "epoch 167 iter 35 loss=0.02949412167072296\n",
      "epoch 167 iter 36 loss=0.021220233291387558\n",
      "epoch 167 iter 37 loss=0.02803122252225876\n",
      "epoch 167 iter 38 loss=0.025402013212442398\n",
      "epoch 167 iter 39 loss=0.03107711672782898\n",
      "epoch 167 iter 40 loss=0.038324590772390366\n",
      "epoch 167 iter 41 loss=0.03088236227631569\n",
      "epoch 167 iter 42 loss=0.027478227391839027\n",
      "epoch 167 iter 43 loss=0.04671739786863327\n",
      "epoch 167 iter 44 loss=0.030825907364487648\n",
      "epoch 167 iter 45 loss=0.04726158455014229\n",
      "epoch 167 iter 46 loss=0.03954343870282173\n",
      "epoch 167 iter 47 loss=0.046568699181079865\n",
      "epoch 167 iter 48 loss=0.029518824070692062\n",
      "epoch 167 iter 49 loss=0.021617889404296875\n",
      "epoch 167 iter 50 loss=0.036143336445093155\n",
      "epoch 167 iter 51 loss=0.016740603372454643\n",
      "epoch 167 iter 52 loss=0.022386016324162483\n",
      "epoch 167 iter 53 loss=0.03389333561062813\n",
      "epoch 167 iter 54 loss=0.04805551469326019\n",
      "epoch 167 iter 55 loss=0.03515079244971275\n",
      "epoch 167 iter 56 loss=0.05072226747870445\n",
      "epoch 167 iter 57 loss=0.05613316595554352\n",
      "epoch 167 iter 58 loss=0.030833572149276733\n",
      "epoch 167 iter 59 loss=0.03275743126869202\n",
      "epoch 167 iter 60 loss=0.023804178461432457\n",
      "epoch 167 iter 61 loss=0.021416431292891502\n",
      "epoch 167 iter 62 loss=0.025640394538640976\n",
      "epoch 167 iter 63 loss=0.027865638956427574\n",
      "epoch 167 iter 64 loss=0.030370278283953667\n",
      "epoch 167 iter 65 loss=0.049606066197156906\n",
      "epoch 167 iter 66 loss=0.03969888389110565\n",
      "epoch 167 iter 67 loss=0.045072369277477264\n",
      "epoch 167 iter 68 loss=0.053806986659765244\n",
      "epoch 167 iter 69 loss=0.027032094076275826\n",
      "epoch 167 iter 70 loss=0.04108932614326477\n",
      "epoch 167 iter 71 loss=0.03264417499303818\n",
      "epoch 167 iter 72 loss=0.04300449788570404\n",
      "epoch 167 iter 73 loss=0.04512983188033104\n",
      "epoch 167 iter 74 loss=0.039589397609233856\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3201.\n",
      "epoch 168 iter 0 loss=0.027667736634612083\n",
      "epoch 168 iter 1 loss=0.03310096636414528\n",
      "epoch 168 iter 2 loss=0.042480383068323135\n",
      "epoch 168 iter 3 loss=0.028394166380167007\n",
      "epoch 168 iter 4 loss=0.03049037605524063\n",
      "epoch 168 iter 5 loss=0.04486003518104553\n",
      "epoch 168 iter 6 loss=0.03499029949307442\n",
      "epoch 168 iter 7 loss=0.030750276520848274\n",
      "epoch 168 iter 8 loss=0.04281291365623474\n",
      "epoch 168 iter 9 loss=0.03147358447313309\n",
      "epoch 168 iter 10 loss=0.03553808107972145\n",
      "epoch 168 iter 11 loss=0.03839840739965439\n",
      "epoch 168 iter 12 loss=0.035278983414173126\n",
      "epoch 168 iter 13 loss=0.055414941161870956\n",
      "epoch 168 iter 14 loss=0.02717674896121025\n",
      "epoch 168 iter 15 loss=0.019511329010128975\n",
      "epoch 168 iter 16 loss=0.02634703367948532\n",
      "epoch 168 iter 17 loss=0.03224925696849823\n",
      "epoch 168 iter 18 loss=0.04623999446630478\n",
      "epoch 168 iter 19 loss=0.04187760874629021\n",
      "epoch 168 iter 20 loss=0.03736107796430588\n",
      "epoch 168 iter 21 loss=0.0631997212767601\n",
      "epoch 168 iter 22 loss=0.050140008330345154\n",
      "epoch 168 iter 23 loss=0.02423905022442341\n",
      "epoch 168 iter 24 loss=0.02652185596525669\n",
      "epoch 168 iter 25 loss=0.043957751244306564\n",
      "epoch 168 iter 26 loss=0.05002008378505707\n",
      "epoch 168 iter 27 loss=0.043885037302970886\n",
      "epoch 168 iter 28 loss=0.03137219697237015\n",
      "epoch 168 iter 29 loss=0.03865355625748634\n",
      "epoch 168 iter 30 loss=0.03627600893378258\n",
      "epoch 168 iter 31 loss=0.04652995988726616\n",
      "epoch 168 iter 32 loss=0.032810844480991364\n",
      "epoch 168 iter 33 loss=0.06283850967884064\n",
      "epoch 168 iter 34 loss=0.02952859364449978\n",
      "epoch 168 iter 35 loss=0.04782294109463692\n",
      "epoch 168 iter 36 loss=0.032614342868328094\n",
      "epoch 168 iter 37 loss=0.02810138836503029\n",
      "epoch 168 iter 38 loss=0.021361635997891426\n",
      "epoch 168 iter 39 loss=0.05116759240627289\n",
      "epoch 168 iter 40 loss=0.027841240167617798\n",
      "epoch 168 iter 41 loss=0.045616935938596725\n",
      "epoch 168 iter 42 loss=0.02851432003080845\n",
      "epoch 168 iter 43 loss=0.04617951437830925\n",
      "epoch 168 iter 44 loss=0.03552879020571709\n",
      "epoch 168 iter 45 loss=0.027118569239974022\n",
      "epoch 168 iter 46 loss=0.0364220067858696\n",
      "epoch 168 iter 47 loss=0.018021754920482635\n",
      "epoch 168 iter 48 loss=0.04873955622315407\n",
      "epoch 168 iter 49 loss=0.017597461119294167\n",
      "epoch 168 iter 50 loss=0.04354327917098999\n",
      "epoch 168 iter 51 loss=0.027210183441638947\n",
      "epoch 168 iter 52 loss=0.02424764074385166\n",
      "epoch 168 iter 53 loss=0.02207650989294052\n",
      "epoch 168 iter 54 loss=0.03365061804652214\n",
      "epoch 168 iter 55 loss=0.034221895039081573\n",
      "epoch 168 iter 56 loss=0.037683721631765366\n",
      "epoch 168 iter 57 loss=0.037439100444316864\n",
      "epoch 168 iter 58 loss=0.030426636338233948\n",
      "epoch 168 iter 59 loss=0.028776634484529495\n",
      "epoch 168 iter 60 loss=0.03566215559840202\n",
      "epoch 168 iter 61 loss=0.03684398531913757\n",
      "epoch 168 iter 62 loss=0.022446850314736366\n",
      "epoch 168 iter 63 loss=0.04101165384054184\n",
      "epoch 168 iter 64 loss=0.03164960443973541\n",
      "epoch 168 iter 65 loss=0.03943851590156555\n",
      "epoch 168 iter 66 loss=0.026601295918226242\n",
      "epoch 168 iter 67 loss=0.03085339069366455\n",
      "epoch 168 iter 68 loss=0.029490388929843903\n",
      "epoch 168 iter 69 loss=0.04690894857048988\n",
      "epoch 168 iter 70 loss=0.04109970107674599\n",
      "epoch 168 iter 71 loss=0.028301993384957314\n",
      "epoch 168 iter 72 loss=0.033705681562423706\n",
      "epoch 168 iter 73 loss=0.029928477481007576\n",
      "epoch 168 iter 74 loss=0.02025814726948738\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3225.\n",
      "epoch 169 iter 0 loss=0.03861891105771065\n",
      "epoch 169 iter 1 loss=0.02306048758327961\n",
      "epoch 169 iter 2 loss=0.023591816425323486\n",
      "epoch 169 iter 3 loss=0.04294133186340332\n",
      "epoch 169 iter 4 loss=0.04634822532534599\n",
      "epoch 169 iter 5 loss=0.037186767905950546\n",
      "epoch 169 iter 6 loss=0.03699636086821556\n",
      "epoch 169 iter 7 loss=0.020779365673661232\n",
      "epoch 169 iter 8 loss=0.05343131348490715\n",
      "epoch 169 iter 9 loss=0.02595289796590805\n",
      "epoch 169 iter 10 loss=0.04479670152068138\n",
      "epoch 169 iter 11 loss=0.03634351119399071\n",
      "epoch 169 iter 12 loss=0.03855324909090996\n",
      "epoch 169 iter 13 loss=0.028754493221640587\n",
      "epoch 169 iter 14 loss=0.031884368509054184\n",
      "epoch 169 iter 15 loss=0.01686999388039112\n",
      "epoch 169 iter 16 loss=0.031843576580286026\n",
      "epoch 169 iter 17 loss=0.03224637731909752\n",
      "epoch 169 iter 18 loss=0.02669168822467327\n",
      "epoch 169 iter 19 loss=0.02601844258606434\n",
      "epoch 169 iter 20 loss=0.02219964750111103\n",
      "epoch 169 iter 21 loss=0.030497152358293533\n",
      "epoch 169 iter 22 loss=0.04297603294253349\n",
      "epoch 169 iter 23 loss=0.03820677101612091\n",
      "epoch 169 iter 24 loss=0.03391361981630325\n",
      "epoch 169 iter 25 loss=0.04318511486053467\n",
      "epoch 169 iter 26 loss=0.04099997133016586\n",
      "epoch 169 iter 27 loss=0.025883089751005173\n",
      "epoch 169 iter 28 loss=0.045704178512096405\n",
      "epoch 169 iter 29 loss=0.041757311671972275\n",
      "epoch 169 iter 30 loss=0.028577180579304695\n",
      "epoch 169 iter 31 loss=0.023654665797948837\n",
      "epoch 169 iter 32 loss=0.02819470502436161\n",
      "epoch 169 iter 33 loss=0.03382129967212677\n",
      "epoch 169 iter 34 loss=0.03569610044360161\n",
      "epoch 169 iter 35 loss=0.03186319023370743\n",
      "epoch 169 iter 36 loss=0.02401467226445675\n",
      "epoch 169 iter 37 loss=0.04518485441803932\n",
      "epoch 169 iter 38 loss=0.03961951285600662\n",
      "epoch 169 iter 39 loss=0.031590256839990616\n",
      "epoch 169 iter 40 loss=0.03897443413734436\n",
      "epoch 169 iter 41 loss=0.05241497978568077\n",
      "epoch 169 iter 42 loss=0.038519807159900665\n",
      "epoch 169 iter 43 loss=0.036256130784749985\n",
      "epoch 169 iter 44 loss=0.035785771906375885\n",
      "epoch 169 iter 45 loss=0.040646862238645554\n",
      "epoch 169 iter 46 loss=0.04280992969870567\n",
      "epoch 169 iter 47 loss=0.028309501707553864\n",
      "epoch 169 iter 48 loss=0.06245896592736244\n",
      "epoch 169 iter 49 loss=0.04233916103839874\n",
      "epoch 169 iter 50 loss=0.04871189966797829\n",
      "epoch 169 iter 51 loss=0.04031998664140701\n",
      "epoch 169 iter 52 loss=0.047527994960546494\n",
      "epoch 169 iter 53 loss=0.03359288349747658\n",
      "epoch 169 iter 54 loss=0.029970085248351097\n",
      "epoch 169 iter 55 loss=0.033972978591918945\n",
      "epoch 169 iter 56 loss=0.02626568265259266\n",
      "epoch 169 iter 57 loss=0.03713409975171089\n",
      "epoch 169 iter 58 loss=0.029645679518580437\n",
      "epoch 169 iter 59 loss=0.04806124418973923\n",
      "epoch 169 iter 60 loss=0.05815832316875458\n",
      "epoch 169 iter 61 loss=0.029566600918769836\n",
      "epoch 169 iter 62 loss=0.03743897005915642\n",
      "epoch 169 iter 63 loss=0.03547850251197815\n",
      "epoch 169 iter 64 loss=0.03287548944354057\n",
      "epoch 169 iter 65 loss=0.019660603255033493\n",
      "epoch 169 iter 66 loss=0.03504140302538872\n",
      "epoch 169 iter 67 loss=0.031470105051994324\n",
      "epoch 169 iter 68 loss=0.03527170792222023\n",
      "epoch 169 iter 69 loss=0.03254644572734833\n",
      "epoch 169 iter 70 loss=0.026172980666160583\n",
      "epoch 169 iter 71 loss=0.026432424783706665\n",
      "epoch 169 iter 72 loss=0.04196186363697052\n",
      "epoch 169 iter 73 loss=0.042216572910547256\n",
      "epoch 169 iter 74 loss=0.020613010972738266\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3189.\n",
      "epoch 170 iter 0 loss=0.02684480883181095\n",
      "epoch 170 iter 1 loss=0.032029181718826294\n",
      "epoch 170 iter 2 loss=0.04769368842244148\n",
      "epoch 170 iter 3 loss=0.030631037428975105\n",
      "epoch 170 iter 4 loss=0.045182641595602036\n",
      "epoch 170 iter 5 loss=0.029825229197740555\n",
      "epoch 170 iter 6 loss=0.043876759707927704\n",
      "epoch 170 iter 7 loss=0.04803568497300148\n",
      "epoch 170 iter 8 loss=0.035004809498786926\n",
      "epoch 170 iter 9 loss=0.04759136214852333\n",
      "epoch 170 iter 10 loss=0.02625127136707306\n",
      "epoch 170 iter 11 loss=0.03615480288863182\n",
      "epoch 170 iter 12 loss=0.027663351967930794\n",
      "epoch 170 iter 13 loss=0.041668280959129333\n",
      "epoch 170 iter 14 loss=0.047276683151721954\n",
      "epoch 170 iter 15 loss=0.026065118610858917\n",
      "epoch 170 iter 16 loss=0.055467914789915085\n",
      "epoch 170 iter 17 loss=0.026236778125166893\n",
      "epoch 170 iter 18 loss=0.029713336378335953\n",
      "epoch 170 iter 19 loss=0.020621467381715775\n",
      "epoch 170 iter 20 loss=0.03917207568883896\n",
      "epoch 170 iter 21 loss=0.017896050587296486\n",
      "epoch 170 iter 22 loss=0.05275274068117142\n",
      "epoch 170 iter 23 loss=0.03261111304163933\n",
      "epoch 170 iter 24 loss=0.04084986075758934\n",
      "epoch 170 iter 25 loss=0.05529699847102165\n",
      "epoch 170 iter 26 loss=0.025931699201464653\n",
      "epoch 170 iter 27 loss=0.02197004295885563\n",
      "epoch 170 iter 28 loss=0.022044919431209564\n",
      "epoch 170 iter 29 loss=0.022075993940234184\n",
      "epoch 170 iter 30 loss=0.03116236813366413\n",
      "epoch 170 iter 31 loss=0.04031327739357948\n",
      "epoch 170 iter 32 loss=0.03714622184634209\n",
      "epoch 170 iter 33 loss=0.019993530586361885\n",
      "epoch 170 iter 34 loss=0.02957163192331791\n",
      "epoch 170 iter 35 loss=0.03493068739771843\n",
      "epoch 170 iter 36 loss=0.034420982003211975\n",
      "epoch 170 iter 37 loss=0.03287292644381523\n",
      "epoch 170 iter 38 loss=0.049070052802562714\n",
      "epoch 170 iter 39 loss=0.02995031140744686\n",
      "epoch 170 iter 40 loss=0.04153955355286598\n",
      "epoch 170 iter 41 loss=0.04223022237420082\n",
      "epoch 170 iter 42 loss=0.03720002621412277\n",
      "epoch 170 iter 43 loss=0.029373466968536377\n",
      "epoch 170 iter 44 loss=0.04177168384194374\n",
      "epoch 170 iter 45 loss=0.020631982013583183\n",
      "epoch 170 iter 46 loss=0.047470174729824066\n",
      "epoch 170 iter 47 loss=0.03371423855423927\n",
      "epoch 170 iter 48 loss=0.03621308133006096\n",
      "epoch 170 iter 49 loss=0.028466051444411278\n",
      "epoch 170 iter 50 loss=0.039266977459192276\n",
      "epoch 170 iter 51 loss=0.05203938111662865\n",
      "epoch 170 iter 52 loss=0.028735827654600143\n",
      "epoch 170 iter 53 loss=0.048887453973293304\n",
      "epoch 170 iter 54 loss=0.023856565356254578\n",
      "epoch 170 iter 55 loss=0.039740391075611115\n",
      "epoch 170 iter 56 loss=0.041512154042720795\n",
      "epoch 170 iter 57 loss=0.02058969996869564\n",
      "epoch 170 iter 58 loss=0.050246208906173706\n",
      "epoch 170 iter 59 loss=0.04080468788743019\n",
      "epoch 170 iter 60 loss=0.0392615832388401\n",
      "epoch 170 iter 61 loss=0.027660395950078964\n",
      "epoch 170 iter 62 loss=0.022396838292479515\n",
      "epoch 170 iter 63 loss=0.03897113353013992\n",
      "epoch 170 iter 64 loss=0.026442257687449455\n",
      "epoch 170 iter 65 loss=0.038494594395160675\n",
      "epoch 170 iter 66 loss=0.030468391254544258\n",
      "epoch 170 iter 67 loss=0.03875761479139328\n",
      "epoch 170 iter 68 loss=0.051575835794210434\n",
      "epoch 170 iter 69 loss=0.045038722455501556\n",
      "epoch 170 iter 70 loss=0.03722924366593361\n",
      "epoch 170 iter 71 loss=0.04035655036568642\n",
      "epoch 170 iter 72 loss=0.019252464175224304\n",
      "epoch 170 iter 73 loss=0.03856583312153816\n",
      "epoch 170 iter 74 loss=0.04056299850344658\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3174.\n",
      "epoch 171 iter 0 loss=0.026363782584667206\n",
      "epoch 171 iter 1 loss=0.04011513292789459\n",
      "epoch 171 iter 2 loss=0.027674099430441856\n",
      "epoch 171 iter 3 loss=0.022692881524562836\n",
      "epoch 171 iter 4 loss=0.037604544311761856\n",
      "epoch 171 iter 5 loss=0.03492026403546333\n",
      "epoch 171 iter 6 loss=0.03375044837594032\n",
      "epoch 171 iter 7 loss=0.05652917921543121\n",
      "epoch 171 iter 8 loss=0.03165411576628685\n",
      "epoch 171 iter 9 loss=0.03406187519431114\n",
      "epoch 171 iter 10 loss=0.03885636106133461\n",
      "epoch 171 iter 11 loss=0.037797752767801285\n",
      "epoch 171 iter 12 loss=0.036044489592313766\n",
      "epoch 171 iter 13 loss=0.030485693365335464\n",
      "epoch 171 iter 14 loss=0.04104255139827728\n",
      "epoch 171 iter 15 loss=0.037419334053993225\n",
      "epoch 171 iter 16 loss=0.043680593371391296\n",
      "epoch 171 iter 17 loss=0.02963319979608059\n",
      "epoch 171 iter 18 loss=0.0376213975250721\n",
      "epoch 171 iter 19 loss=0.06224270537495613\n",
      "epoch 171 iter 20 loss=0.029202815145254135\n",
      "epoch 171 iter 21 loss=0.040889542549848557\n",
      "epoch 171 iter 22 loss=0.037105102092027664\n",
      "epoch 171 iter 23 loss=0.043554600328207016\n",
      "epoch 171 iter 24 loss=0.02310895174741745\n",
      "epoch 171 iter 25 loss=0.041415031999349594\n",
      "epoch 171 iter 26 loss=0.03780395910143852\n",
      "epoch 171 iter 27 loss=0.02214631251990795\n",
      "epoch 171 iter 28 loss=0.023341823369264603\n",
      "epoch 171 iter 29 loss=0.03783039748668671\n",
      "epoch 171 iter 30 loss=0.03751993179321289\n",
      "epoch 171 iter 31 loss=0.02999274805188179\n",
      "epoch 171 iter 32 loss=0.025482913479208946\n",
      "epoch 171 iter 33 loss=0.028255188837647438\n",
      "epoch 171 iter 34 loss=0.03823890909552574\n",
      "epoch 171 iter 35 loss=0.052189458161592484\n",
      "epoch 171 iter 36 loss=0.056053005158901215\n",
      "epoch 171 iter 37 loss=0.03641301393508911\n",
      "epoch 171 iter 38 loss=0.017990129068493843\n",
      "epoch 171 iter 39 loss=0.025372611358761787\n",
      "epoch 171 iter 40 loss=0.02277716062963009\n",
      "epoch 171 iter 41 loss=0.023811832070350647\n",
      "epoch 171 iter 42 loss=0.04151853546500206\n",
      "epoch 171 iter 43 loss=0.02248018980026245\n",
      "epoch 171 iter 44 loss=0.045562081038951874\n",
      "epoch 171 iter 45 loss=0.04591787979006767\n",
      "epoch 171 iter 46 loss=0.03462718799710274\n",
      "epoch 171 iter 47 loss=0.026433883234858513\n",
      "epoch 171 iter 48 loss=0.04003365337848663\n",
      "epoch 171 iter 49 loss=0.041235748678445816\n",
      "epoch 171 iter 50 loss=0.025875085964798927\n",
      "epoch 171 iter 51 loss=0.0347839780151844\n",
      "epoch 171 iter 52 loss=0.036367569118738174\n",
      "epoch 171 iter 53 loss=0.051014244556427\n",
      "epoch 171 iter 54 loss=0.03675542771816254\n",
      "epoch 171 iter 55 loss=0.026472771540284157\n",
      "epoch 171 iter 56 loss=0.04387935623526573\n",
      "epoch 171 iter 57 loss=0.03974797576665878\n",
      "epoch 171 iter 58 loss=0.0402231365442276\n",
      "epoch 171 iter 59 loss=0.03316476568579674\n",
      "epoch 171 iter 60 loss=0.034845467656850815\n",
      "epoch 171 iter 61 loss=0.025732211768627167\n",
      "epoch 171 iter 62 loss=0.04713121056556702\n",
      "epoch 171 iter 63 loss=0.021247396245598793\n",
      "epoch 171 iter 64 loss=0.0359668992459774\n",
      "epoch 171 iter 65 loss=0.053071029484272\n",
      "epoch 171 iter 66 loss=0.029219262301921844\n",
      "epoch 171 iter 67 loss=0.04103235900402069\n",
      "epoch 171 iter 68 loss=0.021069684997200966\n",
      "epoch 171 iter 69 loss=0.04399963840842247\n",
      "epoch 171 iter 70 loss=0.04427843540906906\n",
      "epoch 171 iter 71 loss=0.048046454787254333\n",
      "epoch 171 iter 72 loss=0.018640611320734024\n",
      "epoch 171 iter 73 loss=0.03715766221284866\n",
      "epoch 171 iter 74 loss=0.020027147606015205\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3207.\n",
      "epoch 172 iter 0 loss=0.04942091181874275\n",
      "epoch 172 iter 1 loss=0.02549006976187229\n",
      "epoch 172 iter 2 loss=0.057612303644418716\n",
      "epoch 172 iter 3 loss=0.056581974029541016\n",
      "epoch 172 iter 4 loss=0.02469632588326931\n",
      "epoch 172 iter 5 loss=0.04550056532025337\n",
      "epoch 172 iter 6 loss=0.029116949066519737\n",
      "epoch 172 iter 7 loss=0.033859528601169586\n",
      "epoch 172 iter 8 loss=0.03598678112030029\n",
      "epoch 172 iter 9 loss=0.034502945840358734\n",
      "epoch 172 iter 10 loss=0.059853143990039825\n",
      "epoch 172 iter 11 loss=0.034155406057834625\n",
      "epoch 172 iter 12 loss=0.029567569494247437\n",
      "epoch 172 iter 13 loss=0.022153781726956367\n",
      "epoch 172 iter 14 loss=0.024698100984096527\n",
      "epoch 172 iter 15 loss=0.022602640092372894\n",
      "epoch 172 iter 16 loss=0.030786287039518356\n",
      "epoch 172 iter 17 loss=0.02117879129946232\n",
      "epoch 172 iter 18 loss=0.018315063789486885\n",
      "epoch 172 iter 19 loss=0.023458845913410187\n",
      "epoch 172 iter 20 loss=0.04196544736623764\n",
      "epoch 172 iter 21 loss=0.051013778895139694\n",
      "epoch 172 iter 22 loss=0.0611894428730011\n",
      "epoch 172 iter 23 loss=0.03444347158074379\n",
      "epoch 172 iter 24 loss=0.0301214586943388\n",
      "epoch 172 iter 25 loss=0.025874506682157516\n",
      "epoch 172 iter 26 loss=0.03322042152285576\n",
      "epoch 172 iter 27 loss=0.051248036324977875\n",
      "epoch 172 iter 28 loss=0.02078612707555294\n",
      "epoch 172 iter 29 loss=0.019228480756282806\n",
      "epoch 172 iter 30 loss=0.040350448340177536\n",
      "epoch 172 iter 31 loss=0.04001708701252937\n",
      "epoch 172 iter 32 loss=0.026852872222661972\n",
      "epoch 172 iter 33 loss=0.04584198445081711\n",
      "epoch 172 iter 34 loss=0.029933582991361618\n",
      "epoch 172 iter 35 loss=0.036796119064092636\n",
      "epoch 172 iter 36 loss=0.021408136934041977\n",
      "epoch 172 iter 37 loss=0.04472501203417778\n",
      "epoch 172 iter 38 loss=0.050614673644304276\n",
      "epoch 172 iter 39 loss=0.02710089646279812\n",
      "epoch 172 iter 40 loss=0.04032464325428009\n",
      "epoch 172 iter 41 loss=0.04535195231437683\n",
      "epoch 172 iter 42 loss=0.040032293647527695\n",
      "epoch 172 iter 43 loss=0.035843852907419205\n",
      "epoch 172 iter 44 loss=0.0263139046728611\n",
      "epoch 172 iter 45 loss=0.028885161504149437\n",
      "epoch 172 iter 46 loss=0.042091745883226395\n",
      "epoch 172 iter 47 loss=0.03725450113415718\n",
      "epoch 172 iter 48 loss=0.041225891560316086\n",
      "epoch 172 iter 49 loss=0.05817057937383652\n",
      "epoch 172 iter 50 loss=0.032882992178201675\n",
      "epoch 172 iter 51 loss=0.02385942079126835\n",
      "epoch 172 iter 52 loss=0.04024052992463112\n",
      "epoch 172 iter 53 loss=0.019958512857556343\n",
      "epoch 172 iter 54 loss=0.03727399557828903\n",
      "epoch 172 iter 55 loss=0.03390823304653168\n",
      "epoch 172 iter 56 loss=0.03374003991484642\n",
      "epoch 172 iter 57 loss=0.036829061806201935\n",
      "epoch 172 iter 58 loss=0.035749293863773346\n",
      "epoch 172 iter 59 loss=0.03212927654385567\n",
      "epoch 172 iter 60 loss=0.014878138899803162\n",
      "epoch 172 iter 61 loss=0.02586555667221546\n",
      "epoch 172 iter 62 loss=0.038446538150310516\n",
      "epoch 172 iter 63 loss=0.033075928688049316\n",
      "epoch 172 iter 64 loss=0.020665926858782768\n",
      "epoch 172 iter 65 loss=0.029712483286857605\n",
      "epoch 172 iter 66 loss=0.031082293018698692\n",
      "epoch 172 iter 67 loss=0.03513899818062782\n",
      "epoch 172 iter 68 loss=0.036529816687107086\n",
      "epoch 172 iter 69 loss=0.04315152391791344\n",
      "epoch 172 iter 70 loss=0.04268748685717583\n",
      "epoch 172 iter 71 loss=0.04441293701529503\n",
      "epoch 172 iter 72 loss=0.04850023239850998\n",
      "epoch 172 iter 73 loss=0.025359347462654114\n",
      "epoch 172 iter 74 loss=0.04614892974495888\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3197.\n",
      "epoch 173 iter 0 loss=0.035041242837905884\n",
      "epoch 173 iter 1 loss=0.05509626492857933\n",
      "epoch 173 iter 2 loss=0.025782687589526176\n",
      "epoch 173 iter 3 loss=0.02870587632060051\n",
      "epoch 173 iter 4 loss=0.04702756553888321\n",
      "epoch 173 iter 5 loss=0.04462460055947304\n",
      "epoch 173 iter 6 loss=0.021232929080724716\n",
      "epoch 173 iter 7 loss=0.016015052795410156\n",
      "epoch 173 iter 8 loss=0.019660672172904015\n",
      "epoch 173 iter 9 loss=0.045380279421806335\n",
      "epoch 173 iter 10 loss=0.03648056462407112\n",
      "epoch 173 iter 11 loss=0.018003739416599274\n",
      "epoch 173 iter 12 loss=0.03172466158866882\n",
      "epoch 173 iter 13 loss=0.0431547574698925\n",
      "epoch 173 iter 14 loss=0.05082855746150017\n",
      "epoch 173 iter 15 loss=0.02715384215116501\n",
      "epoch 173 iter 16 loss=0.04375341162085533\n",
      "epoch 173 iter 17 loss=0.04942646622657776\n",
      "epoch 173 iter 18 loss=0.022713353857398033\n",
      "epoch 173 iter 19 loss=0.03457524999976158\n",
      "epoch 173 iter 20 loss=0.035175371915102005\n",
      "epoch 173 iter 21 loss=0.04480200633406639\n",
      "epoch 173 iter 22 loss=0.03653811290860176\n",
      "epoch 173 iter 23 loss=0.029537944123148918\n",
      "epoch 173 iter 24 loss=0.03754545375704765\n",
      "epoch 173 iter 25 loss=0.03665966913104057\n",
      "epoch 173 iter 26 loss=0.025937022641301155\n",
      "epoch 173 iter 27 loss=0.02372119203209877\n",
      "epoch 173 iter 28 loss=0.02495303377509117\n",
      "epoch 173 iter 29 loss=0.028368672356009483\n",
      "epoch 173 iter 30 loss=0.04980236664414406\n",
      "epoch 173 iter 31 loss=0.028573639690876007\n",
      "epoch 173 iter 32 loss=0.040589287877082825\n",
      "epoch 173 iter 33 loss=0.031132420524954796\n",
      "epoch 173 iter 34 loss=0.021030254662036896\n",
      "epoch 173 iter 35 loss=0.028713896870613098\n",
      "epoch 173 iter 36 loss=0.054067954421043396\n",
      "epoch 173 iter 37 loss=0.05192334204912186\n",
      "epoch 173 iter 38 loss=0.02710418589413166\n",
      "epoch 173 iter 39 loss=0.039879847317934036\n",
      "epoch 173 iter 40 loss=0.02921433001756668\n",
      "epoch 173 iter 41 loss=0.02212723344564438\n",
      "epoch 173 iter 42 loss=0.03550078719854355\n",
      "epoch 173 iter 43 loss=0.03212838992476463\n",
      "epoch 173 iter 44 loss=0.03980088606476784\n",
      "epoch 173 iter 45 loss=0.052843883633613586\n",
      "epoch 173 iter 46 loss=0.027652759104967117\n",
      "epoch 173 iter 47 loss=0.021717943251132965\n",
      "epoch 173 iter 48 loss=0.040953390300273895\n",
      "epoch 173 iter 49 loss=0.058434680104255676\n",
      "epoch 173 iter 50 loss=0.034742217510938644\n",
      "epoch 173 iter 51 loss=0.02987832948565483\n",
      "epoch 173 iter 52 loss=0.024277793243527412\n",
      "epoch 173 iter 53 loss=0.033107127994298935\n",
      "epoch 173 iter 54 loss=0.0411013588309288\n",
      "epoch 173 iter 55 loss=0.03652850165963173\n",
      "epoch 173 iter 56 loss=0.030144765973091125\n",
      "epoch 173 iter 57 loss=0.032802581787109375\n",
      "epoch 173 iter 58 loss=0.028394563123583794\n",
      "epoch 173 iter 59 loss=0.05809316039085388\n",
      "epoch 173 iter 60 loss=0.029745204374194145\n",
      "epoch 173 iter 61 loss=0.043451402336359024\n",
      "epoch 173 iter 62 loss=0.045661911368370056\n",
      "epoch 173 iter 63 loss=0.03515329584479332\n",
      "epoch 173 iter 64 loss=0.0313863642513752\n",
      "epoch 173 iter 65 loss=0.034014321863651276\n",
      "epoch 173 iter 66 loss=0.043399468064308167\n",
      "epoch 173 iter 67 loss=0.0345093309879303\n",
      "epoch 173 iter 68 loss=0.03519979119300842\n",
      "epoch 173 iter 69 loss=0.04333958029747009\n",
      "epoch 173 iter 70 loss=0.02858046069741249\n",
      "epoch 173 iter 71 loss=0.0415448322892189\n",
      "epoch 173 iter 72 loss=0.02265862189233303\n",
      "epoch 173 iter 73 loss=0.033876895904541016\n",
      "epoch 173 iter 74 loss=0.030628765001893044\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3199.\n",
      "epoch 174 iter 0 loss=0.014912820421159267\n",
      "epoch 174 iter 1 loss=0.05147586017847061\n",
      "epoch 174 iter 2 loss=0.0238125491887331\n",
      "epoch 174 iter 3 loss=0.0389280766248703\n",
      "epoch 174 iter 4 loss=0.023271815851330757\n",
      "epoch 174 iter 5 loss=0.04297357052564621\n",
      "epoch 174 iter 6 loss=0.0535409078001976\n",
      "epoch 174 iter 7 loss=0.037428680807352066\n",
      "epoch 174 iter 8 loss=0.034502942115068436\n",
      "epoch 174 iter 9 loss=0.029373524710536003\n",
      "epoch 174 iter 10 loss=0.036128267645835876\n",
      "epoch 174 iter 11 loss=0.028809845447540283\n",
      "epoch 174 iter 12 loss=0.04171553626656532\n",
      "epoch 174 iter 13 loss=0.044141385704278946\n",
      "epoch 174 iter 14 loss=0.025130294263362885\n",
      "epoch 174 iter 15 loss=0.04456654191017151\n",
      "epoch 174 iter 16 loss=0.028025494888424873\n",
      "epoch 174 iter 17 loss=0.05321010574698448\n",
      "epoch 174 iter 18 loss=0.03445301577448845\n",
      "epoch 174 iter 19 loss=0.02636490948498249\n",
      "epoch 174 iter 20 loss=0.02233588881790638\n",
      "epoch 174 iter 21 loss=0.024117980152368546\n",
      "epoch 174 iter 22 loss=0.03136270120739937\n",
      "epoch 174 iter 23 loss=0.04763474687933922\n",
      "epoch 174 iter 24 loss=0.03989662975072861\n",
      "epoch 174 iter 25 loss=0.020714055746793747\n",
      "epoch 174 iter 26 loss=0.046932198107242584\n",
      "epoch 174 iter 27 loss=0.02407301776111126\n",
      "epoch 174 iter 28 loss=0.03435429185628891\n",
      "epoch 174 iter 29 loss=0.020720388740301132\n",
      "epoch 174 iter 30 loss=0.03977305069565773\n",
      "epoch 174 iter 31 loss=0.023265749216079712\n",
      "epoch 174 iter 32 loss=0.02671438455581665\n",
      "epoch 174 iter 33 loss=0.04964704439043999\n",
      "epoch 174 iter 34 loss=0.04261127859354019\n",
      "epoch 174 iter 35 loss=0.036812037229537964\n",
      "epoch 174 iter 36 loss=0.030553286895155907\n",
      "epoch 174 iter 37 loss=0.03356146067380905\n",
      "epoch 174 iter 38 loss=0.0339597687125206\n",
      "epoch 174 iter 39 loss=0.04100803658366203\n",
      "epoch 174 iter 40 loss=0.026977192610502243\n",
      "epoch 174 iter 41 loss=0.03361843153834343\n",
      "epoch 174 iter 42 loss=0.04747265204787254\n",
      "epoch 174 iter 43 loss=0.028490597382187843\n",
      "epoch 174 iter 44 loss=0.035219233483076096\n",
      "epoch 174 iter 45 loss=0.040370307862758636\n",
      "epoch 174 iter 46 loss=0.02690536342561245\n",
      "epoch 174 iter 47 loss=0.0617278590798378\n",
      "epoch 174 iter 48 loss=0.031194500625133514\n",
      "epoch 174 iter 49 loss=0.05161413177847862\n",
      "epoch 174 iter 50 loss=0.02885705977678299\n",
      "epoch 174 iter 51 loss=0.03589637950062752\n",
      "epoch 174 iter 52 loss=0.04219012334942818\n",
      "epoch 174 iter 53 loss=0.041008539497852325\n",
      "epoch 174 iter 54 loss=0.0357523187994957\n",
      "epoch 174 iter 55 loss=0.057163771241903305\n",
      "epoch 174 iter 56 loss=0.04691726341843605\n",
      "epoch 174 iter 57 loss=0.02769608050584793\n",
      "epoch 174 iter 58 loss=0.039669621735811234\n",
      "epoch 174 iter 59 loss=0.02809230424463749\n",
      "epoch 174 iter 60 loss=0.03558541089296341\n",
      "epoch 174 iter 61 loss=0.0282969418913126\n",
      "epoch 174 iter 62 loss=0.035326600074768066\n",
      "epoch 174 iter 63 loss=0.022614860907197\n",
      "epoch 174 iter 64 loss=0.023746682330965996\n",
      "epoch 174 iter 65 loss=0.03770926967263222\n",
      "epoch 174 iter 66 loss=0.05270494148135185\n",
      "epoch 174 iter 67 loss=0.04544482380151749\n",
      "epoch 174 iter 68 loss=0.02602965198457241\n",
      "epoch 174 iter 69 loss=0.03345194831490517\n",
      "epoch 174 iter 70 loss=0.018190555274486542\n",
      "epoch 174 iter 71 loss=0.037419307976961136\n",
      "epoch 174 iter 72 loss=0.034772150218486786\n",
      "epoch 174 iter 73 loss=0.0459878072142601\n",
      "epoch 174 iter 74 loss=0.03368452191352844\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3186.\n",
      "epoch 175 iter 0 loss=0.057007260620594025\n",
      "epoch 175 iter 1 loss=0.025121303275227547\n",
      "epoch 175 iter 2 loss=0.04139129817485809\n",
      "epoch 175 iter 3 loss=0.04577204957604408\n",
      "epoch 175 iter 4 loss=0.03311912715435028\n",
      "epoch 175 iter 5 loss=0.03739028424024582\n",
      "epoch 175 iter 6 loss=0.04088733345270157\n",
      "epoch 175 iter 7 loss=0.013681103475391865\n",
      "epoch 175 iter 8 loss=0.028407953679561615\n",
      "epoch 175 iter 9 loss=0.025462469086050987\n",
      "epoch 175 iter 10 loss=0.04991086199879646\n",
      "epoch 175 iter 11 loss=0.03183002024888992\n",
      "epoch 175 iter 12 loss=0.039960794150829315\n",
      "epoch 175 iter 13 loss=0.04701032489538193\n",
      "epoch 175 iter 14 loss=0.036821652203798294\n",
      "epoch 175 iter 15 loss=0.027301549911499023\n",
      "epoch 175 iter 16 loss=0.029000459238886833\n",
      "epoch 175 iter 17 loss=0.029988747090101242\n",
      "epoch 175 iter 18 loss=0.02209273912012577\n",
      "epoch 175 iter 19 loss=0.02725209668278694\n",
      "epoch 175 iter 20 loss=0.02190258353948593\n",
      "epoch 175 iter 21 loss=0.034874387085437775\n",
      "epoch 175 iter 22 loss=0.031083760783076286\n",
      "epoch 175 iter 23 loss=0.0335664376616478\n",
      "epoch 175 iter 24 loss=0.030267681926488876\n",
      "epoch 175 iter 25 loss=0.030067110434174538\n",
      "epoch 175 iter 26 loss=0.024554429575800896\n",
      "epoch 175 iter 27 loss=0.030742283910512924\n",
      "epoch 175 iter 28 loss=0.02311389334499836\n",
      "epoch 175 iter 29 loss=0.048268385231494904\n",
      "epoch 175 iter 30 loss=0.04632176458835602\n",
      "epoch 175 iter 31 loss=0.03467346727848053\n",
      "epoch 175 iter 32 loss=0.028721323236823082\n",
      "epoch 175 iter 33 loss=0.03400193899869919\n",
      "epoch 175 iter 34 loss=0.029737452045083046\n",
      "epoch 175 iter 35 loss=0.04096754267811775\n",
      "epoch 175 iter 36 loss=0.036476150155067444\n",
      "epoch 175 iter 37 loss=0.04737663269042969\n",
      "epoch 175 iter 38 loss=0.05710464343428612\n",
      "epoch 175 iter 39 loss=0.024751044809818268\n",
      "epoch 175 iter 40 loss=0.048867303878068924\n",
      "epoch 175 iter 41 loss=0.050566911697387695\n",
      "epoch 175 iter 42 loss=0.054035935550928116\n",
      "epoch 175 iter 43 loss=0.02612794190645218\n",
      "epoch 175 iter 44 loss=0.031241828575730324\n",
      "epoch 175 iter 45 loss=0.050038985908031464\n",
      "epoch 175 iter 46 loss=0.02055012620985508\n",
      "epoch 175 iter 47 loss=0.04068779945373535\n",
      "epoch 175 iter 48 loss=0.034912120550870895\n",
      "epoch 175 iter 49 loss=0.04772787541151047\n",
      "epoch 175 iter 50 loss=0.04743633419275284\n",
      "epoch 175 iter 51 loss=0.030833475291728973\n",
      "epoch 175 iter 52 loss=0.018833203241229057\n",
      "epoch 175 iter 53 loss=0.026762211695313454\n",
      "epoch 175 iter 54 loss=0.044087085872888565\n",
      "epoch 175 iter 55 loss=0.03650632128119469\n",
      "epoch 175 iter 56 loss=0.03308134526014328\n",
      "epoch 175 iter 57 loss=0.03258267417550087\n",
      "epoch 175 iter 58 loss=0.0380433090031147\n",
      "epoch 175 iter 59 loss=0.03819509595632553\n",
      "epoch 175 iter 60 loss=0.0333268977701664\n",
      "epoch 175 iter 61 loss=0.022040478885173798\n",
      "epoch 175 iter 62 loss=0.04064353555440903\n",
      "epoch 175 iter 63 loss=0.0479712188243866\n",
      "epoch 175 iter 64 loss=0.03650474175810814\n",
      "epoch 175 iter 65 loss=0.031935323029756546\n",
      "epoch 175 iter 66 loss=0.034099940210580826\n",
      "epoch 175 iter 67 loss=0.016880080103874207\n",
      "epoch 175 iter 68 loss=0.032967280596494675\n",
      "epoch 175 iter 69 loss=0.03220312297344208\n",
      "epoch 175 iter 70 loss=0.03575483709573746\n",
      "epoch 175 iter 71 loss=0.025117043405771255\n",
      "epoch 175 iter 72 loss=0.029846036806702614\n",
      "epoch 175 iter 73 loss=0.04179935157299042\n",
      "epoch 175 iter 74 loss=0.03457490727305412\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3185.\n",
      "epoch 176 iter 0 loss=0.03774198889732361\n",
      "epoch 176 iter 1 loss=0.04997550696134567\n",
      "epoch 176 iter 2 loss=0.04286296293139458\n",
      "epoch 176 iter 3 loss=0.03815542161464691\n",
      "epoch 176 iter 4 loss=0.040222350507974625\n",
      "epoch 176 iter 5 loss=0.02951201982796192\n",
      "epoch 176 iter 6 loss=0.025504305958747864\n",
      "epoch 176 iter 7 loss=0.040144503116607666\n",
      "epoch 176 iter 8 loss=0.020268209278583527\n",
      "epoch 176 iter 9 loss=0.034423522651195526\n",
      "epoch 176 iter 10 loss=0.037606656551361084\n",
      "epoch 176 iter 11 loss=0.027781467884778976\n",
      "epoch 176 iter 12 loss=0.02928304485976696\n",
      "epoch 176 iter 13 loss=0.019160877913236618\n",
      "epoch 176 iter 14 loss=0.04066800698637962\n",
      "epoch 176 iter 15 loss=0.040145546197891235\n",
      "epoch 176 iter 16 loss=0.031157663092017174\n",
      "epoch 176 iter 17 loss=0.037280358374118805\n",
      "epoch 176 iter 18 loss=0.03805047646164894\n",
      "epoch 176 iter 19 loss=0.028264498338103294\n",
      "epoch 176 iter 20 loss=0.033383842557668686\n",
      "epoch 176 iter 21 loss=0.05296431854367256\n",
      "epoch 176 iter 22 loss=0.04368501529097557\n",
      "epoch 176 iter 23 loss=0.025688165798783302\n",
      "epoch 176 iter 24 loss=0.029776161536574364\n",
      "epoch 176 iter 25 loss=0.046743325889110565\n",
      "epoch 176 iter 26 loss=0.031321436166763306\n",
      "epoch 176 iter 27 loss=0.03660303354263306\n",
      "epoch 176 iter 28 loss=0.04601215571165085\n",
      "epoch 176 iter 29 loss=0.02131895162165165\n",
      "epoch 176 iter 30 loss=0.02804933302104473\n",
      "epoch 176 iter 31 loss=0.04517379775643349\n",
      "epoch 176 iter 32 loss=0.02428133226931095\n",
      "epoch 176 iter 33 loss=0.05699858441948891\n",
      "epoch 176 iter 34 loss=0.027835238724946976\n",
      "epoch 176 iter 35 loss=0.033798523247241974\n",
      "epoch 176 iter 36 loss=0.03181219473481178\n",
      "epoch 176 iter 37 loss=0.03360416740179062\n",
      "epoch 176 iter 38 loss=0.02949862740933895\n",
      "epoch 176 iter 39 loss=0.044474076479673386\n",
      "epoch 176 iter 40 loss=0.041068971157073975\n",
      "epoch 176 iter 41 loss=0.024144120514392853\n",
      "epoch 176 iter 42 loss=0.030943408608436584\n",
      "epoch 176 iter 43 loss=0.03521856665611267\n",
      "epoch 176 iter 44 loss=0.02899082750082016\n",
      "epoch 176 iter 45 loss=0.02979712001979351\n",
      "epoch 176 iter 46 loss=0.03699702024459839\n",
      "epoch 176 iter 47 loss=0.028526967391371727\n",
      "epoch 176 iter 48 loss=0.032849960029125214\n",
      "epoch 176 iter 49 loss=0.048740386962890625\n",
      "epoch 176 iter 50 loss=0.03428640216588974\n",
      "epoch 176 iter 51 loss=0.03971908614039421\n",
      "epoch 176 iter 52 loss=0.018377434462308884\n",
      "epoch 176 iter 53 loss=0.04718330129981041\n",
      "epoch 176 iter 54 loss=0.01913517527282238\n",
      "epoch 176 iter 55 loss=0.02873297408223152\n",
      "epoch 176 iter 56 loss=0.035100389271974564\n",
      "epoch 176 iter 57 loss=0.04011514037847519\n",
      "epoch 176 iter 58 loss=0.02570224739611149\n",
      "epoch 176 iter 59 loss=0.03165953978896141\n",
      "epoch 176 iter 60 loss=0.05379173904657364\n",
      "epoch 176 iter 61 loss=0.0307543333619833\n",
      "epoch 176 iter 62 loss=0.029438143596053123\n",
      "epoch 176 iter 63 loss=0.06199785694479942\n",
      "epoch 176 iter 64 loss=0.044923409819602966\n",
      "epoch 176 iter 65 loss=0.05273061990737915\n",
      "epoch 176 iter 66 loss=0.035081569105386734\n",
      "epoch 176 iter 67 loss=0.044590335339307785\n",
      "epoch 176 iter 68 loss=0.02793375588953495\n",
      "epoch 176 iter 69 loss=0.03201702982187271\n",
      "epoch 176 iter 70 loss=0.030577972531318665\n",
      "epoch 176 iter 71 loss=0.04789379611611366\n",
      "epoch 176 iter 72 loss=0.030450662598013878\n",
      "epoch 176 iter 73 loss=0.01247952226549387\n",
      "epoch 176 iter 74 loss=0.0388873890042305\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3190.\n",
      "epoch 177 iter 0 loss=0.029340364038944244\n",
      "epoch 177 iter 1 loss=0.03594786673784256\n",
      "epoch 177 iter 2 loss=0.02791830152273178\n",
      "epoch 177 iter 3 loss=0.042626116424798965\n",
      "epoch 177 iter 4 loss=0.02811824157834053\n",
      "epoch 177 iter 5 loss=0.031071841716766357\n",
      "epoch 177 iter 6 loss=0.0632186233997345\n",
      "epoch 177 iter 7 loss=0.03417925536632538\n",
      "epoch 177 iter 8 loss=0.04941520467400551\n",
      "epoch 177 iter 9 loss=0.03270016983151436\n",
      "epoch 177 iter 10 loss=0.04397541284561157\n",
      "epoch 177 iter 11 loss=0.03641878440976143\n",
      "epoch 177 iter 12 loss=0.03774154558777809\n",
      "epoch 177 iter 13 loss=0.02603740431368351\n",
      "epoch 177 iter 14 loss=0.027486199513077736\n",
      "epoch 177 iter 15 loss=0.04595266282558441\n",
      "epoch 177 iter 16 loss=0.03162037953734398\n",
      "epoch 177 iter 17 loss=0.04211769998073578\n",
      "epoch 177 iter 18 loss=0.021882768720388412\n",
      "epoch 177 iter 19 loss=0.03969133645296097\n",
      "epoch 177 iter 20 loss=0.0631270706653595\n",
      "epoch 177 iter 21 loss=0.043639495968818665\n",
      "epoch 177 iter 22 loss=0.03513349965214729\n",
      "epoch 177 iter 23 loss=0.06031963601708412\n",
      "epoch 177 iter 24 loss=0.03103584609925747\n",
      "epoch 177 iter 25 loss=0.02987099066376686\n",
      "epoch 177 iter 26 loss=0.03924126923084259\n",
      "epoch 177 iter 27 loss=0.033767394721508026\n",
      "epoch 177 iter 28 loss=0.03631972149014473\n",
      "epoch 177 iter 29 loss=0.050292208790779114\n",
      "epoch 177 iter 30 loss=0.03144661337137222\n",
      "epoch 177 iter 31 loss=0.04793068766593933\n",
      "epoch 177 iter 32 loss=0.030861644074320793\n",
      "epoch 177 iter 33 loss=0.04464492201805115\n",
      "epoch 177 iter 34 loss=0.021570960059762\n",
      "epoch 177 iter 35 loss=0.037851545959711075\n",
      "epoch 177 iter 36 loss=0.03173830732703209\n",
      "epoch 177 iter 37 loss=0.043765246868133545\n",
      "epoch 177 iter 38 loss=0.022692151367664337\n",
      "epoch 177 iter 39 loss=0.018742894753813744\n",
      "epoch 177 iter 40 loss=0.040174081921577454\n",
      "epoch 177 iter 41 loss=0.024860216304659843\n",
      "epoch 177 iter 42 loss=0.020539211109280586\n",
      "epoch 177 iter 43 loss=0.04572640731930733\n",
      "epoch 177 iter 44 loss=0.05924782529473305\n",
      "epoch 177 iter 45 loss=0.02791484072804451\n",
      "epoch 177 iter 46 loss=0.039286620914936066\n",
      "epoch 177 iter 47 loss=0.02925335057079792\n",
      "epoch 177 iter 48 loss=0.03021879307925701\n",
      "epoch 177 iter 49 loss=0.03731580078601837\n",
      "epoch 177 iter 50 loss=0.04071371257305145\n",
      "epoch 177 iter 51 loss=0.03055732324719429\n",
      "epoch 177 iter 52 loss=0.032350897789001465\n",
      "epoch 177 iter 53 loss=0.03524113819003105\n",
      "epoch 177 iter 54 loss=0.02220779098570347\n",
      "epoch 177 iter 55 loss=0.03267350047826767\n",
      "epoch 177 iter 56 loss=0.03382263705134392\n",
      "epoch 177 iter 57 loss=0.04047147557139397\n",
      "epoch 177 iter 58 loss=0.02296413853764534\n",
      "epoch 177 iter 59 loss=0.02744026854634285\n",
      "epoch 177 iter 60 loss=0.04208667576313019\n",
      "epoch 177 iter 61 loss=0.019423363730311394\n",
      "epoch 177 iter 62 loss=0.03533392772078514\n",
      "epoch 177 iter 63 loss=0.0437970831990242\n",
      "epoch 177 iter 64 loss=0.046781282871961594\n",
      "epoch 177 iter 65 loss=0.02242816612124443\n",
      "epoch 177 iter 66 loss=0.030243752524256706\n",
      "epoch 177 iter 67 loss=0.0258028581738472\n",
      "epoch 177 iter 68 loss=0.029499540105462074\n",
      "epoch 177 iter 69 loss=0.0337761752307415\n",
      "epoch 177 iter 70 loss=0.028111934661865234\n",
      "epoch 177 iter 71 loss=0.024283766746520996\n",
      "epoch 177 iter 72 loss=0.01823461800813675\n",
      "epoch 177 iter 73 loss=0.05492936074733734\n",
      "epoch 177 iter 74 loss=0.04049501195549965\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3214.\n",
      "epoch 178 iter 0 loss=0.03740695118904114\n",
      "epoch 178 iter 1 loss=0.02600967325270176\n",
      "epoch 178 iter 2 loss=0.041328299790620804\n",
      "epoch 178 iter 3 loss=0.027937158942222595\n",
      "epoch 178 iter 4 loss=0.03983234986662865\n",
      "epoch 178 iter 5 loss=0.036080338060855865\n",
      "epoch 178 iter 6 loss=0.021809294819831848\n",
      "epoch 178 iter 7 loss=0.03925938159227371\n",
      "epoch 178 iter 8 loss=0.018108364194631577\n",
      "epoch 178 iter 9 loss=0.0248471200466156\n",
      "epoch 178 iter 10 loss=0.02972794882953167\n",
      "epoch 178 iter 11 loss=0.03590533509850502\n",
      "epoch 178 iter 12 loss=0.05475439131259918\n",
      "epoch 178 iter 13 loss=0.04525323584675789\n",
      "epoch 178 iter 14 loss=0.02232261374592781\n",
      "epoch 178 iter 15 loss=0.01631648652255535\n",
      "epoch 178 iter 16 loss=0.03866845741868019\n",
      "epoch 178 iter 17 loss=0.02218582294881344\n",
      "epoch 178 iter 18 loss=0.027443191036581993\n",
      "epoch 178 iter 19 loss=0.03784782066941261\n",
      "epoch 178 iter 20 loss=0.044576097279787064\n",
      "epoch 178 iter 21 loss=0.03206127882003784\n",
      "epoch 178 iter 22 loss=0.05324399843811989\n",
      "epoch 178 iter 23 loss=0.029106052592396736\n",
      "epoch 178 iter 24 loss=0.030984532088041306\n",
      "epoch 178 iter 25 loss=0.039774563163518906\n",
      "epoch 178 iter 26 loss=0.03681735321879387\n",
      "epoch 178 iter 27 loss=0.03918048366904259\n",
      "epoch 178 iter 28 loss=0.05448402091860771\n",
      "epoch 178 iter 29 loss=0.04759255796670914\n",
      "epoch 178 iter 30 loss=0.0384695827960968\n",
      "epoch 178 iter 31 loss=0.044140130281448364\n",
      "epoch 178 iter 32 loss=0.03746810555458069\n",
      "epoch 178 iter 33 loss=0.03016931749880314\n",
      "epoch 178 iter 34 loss=0.03507549688220024\n",
      "epoch 178 iter 35 loss=0.02598554827272892\n",
      "epoch 178 iter 36 loss=0.023526839911937714\n",
      "epoch 178 iter 37 loss=0.03462568297982216\n",
      "epoch 178 iter 38 loss=0.03812041133642197\n",
      "epoch 178 iter 39 loss=0.02213599719107151\n",
      "epoch 178 iter 40 loss=0.0384540781378746\n",
      "epoch 178 iter 41 loss=0.028736555948853493\n",
      "epoch 178 iter 42 loss=0.027200808748602867\n",
      "epoch 178 iter 43 loss=0.044456589967012405\n",
      "epoch 178 iter 44 loss=0.034800611436367035\n",
      "epoch 178 iter 45 loss=0.025514977052807808\n",
      "epoch 178 iter 46 loss=0.03630112484097481\n",
      "epoch 178 iter 47 loss=0.03743681311607361\n",
      "epoch 178 iter 48 loss=0.022850390523672104\n",
      "epoch 178 iter 49 loss=0.04032379016280174\n",
      "epoch 178 iter 50 loss=0.023443901911377907\n",
      "epoch 178 iter 51 loss=0.027233911678195\n",
      "epoch 178 iter 52 loss=0.045938823372125626\n",
      "epoch 178 iter 53 loss=0.04191809892654419\n",
      "epoch 178 iter 54 loss=0.05257990583777428\n",
      "epoch 178 iter 55 loss=0.043628957122564316\n",
      "epoch 178 iter 56 loss=0.03846170753240585\n",
      "epoch 178 iter 57 loss=0.041067879647016525\n",
      "epoch 178 iter 58 loss=0.04096933454275131\n",
      "epoch 178 iter 59 loss=0.023601124063134193\n",
      "epoch 178 iter 60 loss=0.03586655855178833\n",
      "epoch 178 iter 61 loss=0.03993210569024086\n",
      "epoch 178 iter 62 loss=0.034029021859169006\n",
      "epoch 178 iter 63 loss=0.033726681023836136\n",
      "epoch 178 iter 64 loss=0.029843486845493317\n",
      "epoch 178 iter 65 loss=0.03981715440750122\n",
      "epoch 178 iter 66 loss=0.03009907901287079\n",
      "epoch 178 iter 67 loss=0.032016731798648834\n",
      "epoch 178 iter 68 loss=0.030190037563443184\n",
      "epoch 178 iter 69 loss=0.03829861804842949\n",
      "epoch 178 iter 70 loss=0.031081676483154297\n",
      "epoch 178 iter 71 loss=0.02838192880153656\n",
      "epoch 178 iter 72 loss=0.04313324764370918\n",
      "epoch 178 iter 73 loss=0.042103011161088943\n",
      "epoch 178 iter 74 loss=0.04245172441005707\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3181.\n",
      "epoch 179 iter 0 loss=0.030430294573307037\n",
      "epoch 179 iter 1 loss=0.027902275323867798\n",
      "epoch 179 iter 2 loss=0.05320890620350838\n",
      "epoch 179 iter 3 loss=0.024536507204174995\n",
      "epoch 179 iter 4 loss=0.04848683997988701\n",
      "epoch 179 iter 5 loss=0.02157364971935749\n",
      "epoch 179 iter 6 loss=0.0533280111849308\n",
      "epoch 179 iter 7 loss=0.02600524015724659\n",
      "epoch 179 iter 8 loss=0.03329182416200638\n",
      "epoch 179 iter 9 loss=0.021676044911146164\n",
      "epoch 179 iter 10 loss=0.034187201410532\n",
      "epoch 179 iter 11 loss=0.038926105946302414\n",
      "epoch 179 iter 12 loss=0.02478947676718235\n",
      "epoch 179 iter 13 loss=0.030632231384515762\n",
      "epoch 179 iter 14 loss=0.034513477236032486\n",
      "epoch 179 iter 15 loss=0.043256811797618866\n",
      "epoch 179 iter 16 loss=0.03339347988367081\n",
      "epoch 179 iter 17 loss=0.03605666011571884\n",
      "epoch 179 iter 18 loss=0.03981270641088486\n",
      "epoch 179 iter 19 loss=0.0245915986597538\n",
      "epoch 179 iter 20 loss=0.05089540034532547\n",
      "epoch 179 iter 21 loss=0.033072225749492645\n",
      "epoch 179 iter 22 loss=0.030502622947096825\n",
      "epoch 179 iter 23 loss=0.027863891795277596\n",
      "epoch 179 iter 24 loss=0.03508307784795761\n",
      "epoch 179 iter 25 loss=0.04052551090717316\n",
      "epoch 179 iter 26 loss=0.03526175022125244\n",
      "epoch 179 iter 27 loss=0.017686989158391953\n",
      "epoch 179 iter 28 loss=0.03311528265476227\n",
      "epoch 179 iter 29 loss=0.048863835632801056\n",
      "epoch 179 iter 30 loss=0.031863439828157425\n",
      "epoch 179 iter 31 loss=0.02545376680791378\n",
      "epoch 179 iter 32 loss=0.04469101130962372\n",
      "epoch 179 iter 33 loss=0.016896510496735573\n",
      "epoch 179 iter 34 loss=0.03279164433479309\n",
      "epoch 179 iter 35 loss=0.048287324607372284\n",
      "epoch 179 iter 36 loss=0.03948364406824112\n",
      "epoch 179 iter 37 loss=0.03833632916212082\n",
      "epoch 179 iter 38 loss=0.022516926750540733\n",
      "epoch 179 iter 39 loss=0.03951868414878845\n",
      "epoch 179 iter 40 loss=0.03810107335448265\n",
      "epoch 179 iter 41 loss=0.034769680351018906\n",
      "epoch 179 iter 42 loss=0.039141684770584106\n",
      "epoch 179 iter 43 loss=0.04379303753376007\n",
      "epoch 179 iter 44 loss=0.04070628061890602\n",
      "epoch 179 iter 45 loss=0.05446455255150795\n",
      "epoch 179 iter 46 loss=0.04493112117052078\n",
      "epoch 179 iter 47 loss=0.028041362762451172\n",
      "epoch 179 iter 48 loss=0.03858358412981033\n",
      "epoch 179 iter 49 loss=0.030495062470436096\n",
      "epoch 179 iter 50 loss=0.03439042717218399\n",
      "epoch 179 iter 51 loss=0.03761505335569382\n",
      "epoch 179 iter 52 loss=0.03402088209986687\n",
      "epoch 179 iter 53 loss=0.04300408065319061\n",
      "epoch 179 iter 54 loss=0.035539235919713974\n",
      "epoch 179 iter 55 loss=0.04345739632844925\n",
      "epoch 179 iter 56 loss=0.03033897466957569\n",
      "epoch 179 iter 57 loss=0.029141005128622055\n",
      "epoch 179 iter 58 loss=0.03546566143631935\n",
      "epoch 179 iter 59 loss=0.04566486179828644\n",
      "epoch 179 iter 60 loss=0.042083363980054855\n",
      "epoch 179 iter 61 loss=0.02745772898197174\n",
      "epoch 179 iter 62 loss=0.04452011361718178\n",
      "epoch 179 iter 63 loss=0.015943119302392006\n",
      "epoch 179 iter 64 loss=0.03330891206860542\n",
      "epoch 179 iter 65 loss=0.045428596436977386\n",
      "epoch 179 iter 66 loss=0.030240869149565697\n",
      "epoch 179 iter 67 loss=0.02564311772584915\n",
      "epoch 179 iter 68 loss=0.033758193254470825\n",
      "epoch 179 iter 69 loss=0.026510274037718773\n",
      "epoch 179 iter 70 loss=0.04913767799735069\n",
      "epoch 179 iter 71 loss=0.04063211753964424\n",
      "epoch 179 iter 72 loss=0.023602236062288284\n",
      "epoch 179 iter 73 loss=0.03889547660946846\n",
      "epoch 179 iter 74 loss=0.031602080911397934\n",
      "Begin testing\n",
      "Eval result: mIoU 0.3185.\n",
      "The final mIoU is : 0.3421.\n"
     ]
    }
   ],
   "source": [
    "# #The training will take ~1 h.\n",
    "# mIoU = 0.0\n",
    "# IoU_list = []\n",
    "# train_loss = []\n",
    "# evl_each = True #Perform evaluation after each epoch. You can define the false case to save training time\n",
    "# patience = 10  # Number of epochs to wait for improvement\n",
    "# best_mIoU = 0.0\n",
    "# epochs_no_improve = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for iter, (imgs, labels) in enumerate(train_loader):\n",
    "#         pred = segNet(imgs.to(device))\n",
    "#         segNet.zero_grad()\n",
    "#         loss = criterion(pred, labels.long().to(device)) #calculate the loss\n",
    "#         train_loss.append(loss.detach().cpu().numpy().item())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print('epoch {} iter {} loss={}'.format(epoch, iter, loss.data.cpu().numpy()))\n",
    "\n",
    "# #-----Evaluation------\n",
    "#     if evl_each and epoch > 100:\n",
    "#         segNet.eval()\n",
    "#         gray_folder, color_folder = get_predictions(segNet, dataFolder, device)\n",
    "#         segNet.train()\n",
    "#         temp_mIoU = cal_acc(gray_folder, os.path.join(dataFolder, 'testing/label'))\n",
    "#         IoU_list.append(temp_mIoU)\n",
    "#         if temp_mIoU > best_mIoU:\n",
    "#             best_mIoU = temp_mIoU\n",
    "#             epochs_no_improve = 0\n",
    "#             torch.save(segNet.state_dict(), './model.pth')\n",
    "#             move_folders(gray_folder, color_folder,\n",
    "#                          gray_folder.replace('temp_', ''),\n",
    "#                          color_folder.replace('temp_', ''))\n",
    "#         else:\n",
    "#             epochs_no_improve += 1\n",
    "#         if epochs_no_improve >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch}. Best mIoU: {best_mIoU:.4f}\")\n",
    "#             break\n",
    "#     scheduler.step() #Update the learning rate\n",
    "    \n",
    "# print('The final mIoU is : {:.4f}.'.format(best_mIoU)) #The final mIoU is ~0.28\n",
    "# #Remember to download the results before closing the tab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If using MobileNetV3 backbone to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training will take ~1 h.\n",
    "mIoU = 0.0\n",
    "IoU_list = []\n",
    "train_loss = []\n",
    "evl_each = True #Perform evaluation after each epoch. You can define the false case to save training time\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "best_mIoU = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter, (imgs, labels) in enumerate(train_loader):\n",
    "        output = segNet(imgs.to(device))\n",
    "        pred = output['out']  # Get the segmentation output\n",
    "        segNet.zero_grad()\n",
    "        loss = criterion(pred, labels.long().to(device)) #calculate the loss\n",
    "        train_loss.append(loss.detach().cpu().numpy().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch {} iter {} loss={}'.format(epoch, iter, loss.data.cpu().numpy()))\n",
    "\n",
    "#-----Evaluation------\n",
    "    if evl_each and epoch > 100:\n",
    "        segNet.eval()\n",
    "        gray_folder, color_folder = get_predictions(segNet, dataFolder, device)\n",
    "        segNet.train()\n",
    "        temp_mIoU = cal_acc(gray_folder, os.path.join(dataFolder, 'testing/label'))\n",
    "        IoU_list.append(temp_mIoU)\n",
    "        if temp_mIoU > best_mIoU:\n",
    "            best_mIoU = temp_mIoU\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(segNet.state_dict(), './model.pth')\n",
    "            move_folders(gray_folder, color_folder,\n",
    "                         gray_folder.replace('temp_', ''),\n",
    "                         color_folder.replace('temp_', ''))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best mIoU: {best_mIoU:.4f}\")\n",
    "            break\n",
    "    scheduler.step() #Update the learning rate\n",
    "    \n",
    "print('The final mIoU is : {:.4f}.'.format(best_mIoU)) #The final mIoU is ~0.28\n",
    "#Remember to download the results before closing the tab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T07:41:00.683697Z",
     "iopub.status.busy": "2025-06-08T07:41:00.683363Z",
     "iopub.status.idle": "2025-06-08T07:41:00.687666Z",
     "shell.execute_reply": "2025-06-08T07:41:00.686959Z",
     "shell.execute_reply.started": "2025-06-08T07:41:00.683663Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747835847101,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "1n19sQrd_62W",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T07:41:00.688551Z",
     "iopub.status.busy": "2025-06-08T07:41:00.688371Z",
     "iopub.status.idle": "2025-06-08T07:41:00.897164Z",
     "shell.execute_reply": "2025-06-08T07:41:00.896445Z",
     "shell.execute_reply.started": "2025-06-08T07:41:00.688537Z"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1747835847459,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "sM4LDbAO3OzL",
    "outputId": "b2f561f1-95e7-46c1-b983-23452360cab5",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73610a9fd0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss,label=\"train_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T07:41:00.899449Z",
     "iopub.status.busy": "2025-06-08T07:41:00.899225Z",
     "iopub.status.idle": "2025-06-08T07:41:01.046220Z",
     "shell.execute_reply": "2025-06-08T07:41:01.045614Z",
     "shell.execute_reply.started": "2025-06-08T07:41:00.899432Z"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1747835847518,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "1jBwoTvq3r7d",
    "outputId": "71632fe5-ac7e-46e8-df5b-d98ee21c3337",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f736c6a3310>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(IoU_list,label=\"IoU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJBjS-Hli40"
   },
   "source": [
    "\n",
    "A prediction example by using the baseline:\n",
    "\n",
    "![test2.jpg](data:image/jpeg;base64,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)![test1.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FlshC7yryJZ"
   },
   "source": [
    "# 8. FLOPs\n",
    "In deep learning, FLOPs (Floating Point Operations) quantify the total number of arithmetic operations—such as additions, multiplications, and divisions—that a model performs during a single forward pass (i.e., when making a prediction). This metric serves as an indicator of a model’s computational complexity. When discussing large-scale models, FLOPs are often expressed in GFLOPs (Giga Floating Point Operations), where 1 GFLOP equals one billion operations. This unit helps in comparing the computational demands of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T07:41:01.047078Z",
     "iopub.status.busy": "2025-06-08T07:41:01.046856Z",
     "iopub.status.idle": "2025-06-08T07:41:08.730396Z",
     "shell.execute_reply": "2025-06-08T07:41:08.729643Z",
     "shell.execute_reply.started": "2025-06-08T07:41:01.047052Z"
    },
    "executionInfo": {
     "elapsed": 4944,
     "status": "ok",
     "timestamp": 1747835852463,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "C_D3Xsvu467x",
    "outputId": "df0bf371-a6e5-4b59-a25e-b2adf30e016f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
      "Collecting yacs>=0.1.6 (from fvcore)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.0.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
      "Collecting iopath>=0.1.7 (from fvcore)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
      "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fvcore) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fvcore) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fvcore) (2024.2.0)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: fvcore, iopath\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=3d7a7cf81213f8a81109e86c86597f500d7c79b8f884f3173011da0c2de403de\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=6fe9d535b09cc5c69b369eb34f0bd10728de4eff86c94270430614383b737141\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
      "Successfully built fvcore iopath\n",
      "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
      "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# we use fvcore to calculate the FLOPs\n",
    "!pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-08T07:42:22.471278Z",
     "iopub.status.busy": "2025-06-08T07:42:22.470762Z",
     "iopub.status.idle": "2025-06-08T07:42:26.734894Z",
     "shell.execute_reply": "2025-06-08T07:42:26.734082Z",
     "shell.execute_reply.started": "2025-06-08T07:42:22.471254Z"
    },
    "executionInfo": {
     "elapsed": 3669,
     "status": "ok",
     "timestamp": 1747835856134,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "J5fUBat44-hT",
    "outputId": "3d21678e-e92b-4028-ba64-4177526ee771",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 204.83 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# from fvcore.nn import FlopCountAnalysis\n",
    "# input = torch.randn(1, 3, 375, 1242) # Modifying the size (3, 375, 1242) is ***NOT*** allowed.\n",
    "\n",
    "# # Get the network and its FLOPs\n",
    "# model = SegNetwork(n_class=19, use_batch_norm=True)\n",
    "# flops = FlopCountAnalysis(model, input)\n",
    "# print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFLOPs for MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "import torch\n",
    "\n",
    "input = torch.randn(1, 3, 375, 1242).to(device)  # Do NOT change the size\n",
    "\n",
    "# Pass the model directly, not a lambda\n",
    "segNet.eval()  # Set to eval mode for FLOPs calculation\n",
    "with torch.no_grad():\n",
    "    flops = FlopCountAnalysis(segNet, input)\n",
    "    print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study Table\n",
    "| Data Augmentation         | Training Params                     | Architecture                     | Loss Function | mIoU (%) | GFLOPs |\n",
    "|--------------------------|------------------------------------|----------------------------------|---------------|----------|--------|\n",
    "| None                     | Adam, lr=3e-4, BS=4 | Baseline                         | CrossEntropy  |     0.286     |       66.97     |\n",
    "| Crop+Rot+Blur+Jitter      | AdamW, lr=3e-4, BS=2, CosineAnneal | Baseline                         | CrossEntropy  |          |          |\n",
    "| Crop+Rot+Blur+Jitter      | AdamW, lr=3e-4, BS=2, CosineAnneal | BatchNorm+Residual+Skip+Dilated          | CrossEntropy  |     0.3421     |      204.83     |\n",
    "| Crop+Rot+Blur+Jitter      | AdamW, lr=3e-4, BS=2, CosineAnneal | BatchNorm+Residual+Skip+Dilated  | Hybrid        |     0.2125     |          |\n",
    "| Crop+Rot+Blur+Jitter      | AdamW, lr=3e-4, BS=2, CosineAnneal | MobileNetV3+BatchNorm+Skip       | CrossEntropy        |     0.3377     |     18.10      |\n",
    "| Crop+Rot+Blur+Jitter      | AdamW, lr=3e-4, BS=2, CosineAnneal | MobileNetV3+BatchNorm+Skip       | Hybrid        |          |           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747835856137,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "x43K0wi4S8cy",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7608721,
     "sourceId": 12086865,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
